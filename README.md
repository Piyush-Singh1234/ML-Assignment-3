# Text Generation and Machine Learning Models with MLP

This repository contains implementations and experiments for generating text using a next-word prediction model based on a Multi-Layer Perceptron (MLP) and several other machine learning tasks. The main tasks in this project involve implementing text prediction, training MLP models on custom datasets, and visualizing embeddings. Detailed explanations of the experiments and results are provided within the notebooks.

## Project Overview

1. *Next-Word Prediction with MLP*:
   - A text generator based on next-word prediction using an MLP model.
   - Preprocessing includes cleaning the text and creating a vocabulary of unique words.
   - The model is trained to predict the next word based on a given sequence.
   - Includes an interactive Streamlit app where users can:
     - Input custom text and generate the next k words.
     - Modify parameters such as context length, embedding dimension, activation function, random seed, etc.
   - Embeddings are visualized using t-SNE to capture relationships between words.

2. *XOR Dataset Classification*:
   - Trains various models on the XOR dataset with 200 training instances and 200 test instances:
     - MLP with and without L1/L2 regularization.
     - Logistic regression with additional features (e.g., \(x_1 \times x_2\), \(x_1^2\)).
   - Visualizes decision boundaries for each model and compares their performance.

3. *MNIST Digit Classification*:
   - Trains an MLP on the MNIST dataset with the following architecture: 30 neurons in the first layer, 20 in the second, and 10 in the output layer (for 10 classes).
   - Compares MLP performance against Random Forest and Logistic Regression models using F1-score and confusion matrix.
   - Visualizes t-SNE embeddings of the second layer (20 neurons) for the 10 digits, comparing trained and untrained models.

4. *Fashion-MNIST Classification*:
   - Uses the trained MNIST model to predict on Fashion-MNIST data.
   - Visualizes and contrasts t-SNE embeddings for MNIST and Fashion-MNIST, observing the differences in the learned representations.

## Notebooks

Each task is organized into separate Jupyter notebooks. The naming convention is as follows:
- question1.ipynb: Implementation of the next-word prediction model and Streamlit app setup.
- question2.ipynb: Experiments on the XOR dataset.
- question3.ipynb: MNIST classification, Random Forest and Logistic Regression comparisons, and t-SNE visualizations.

## Streamlit App

A Streamlit app is provided to interact with the next-word prediction model. The app allows users to:
- Enter an input text to predict the next k words.
- Configure model parameters such as:
  - Context length
  - Embedding dimension
  - Activation function
  - Random seed
- View predictions generated by pretrained model variants.

*[Streamlit App Link](your-streamlit-app-link)*

## Dataset

Datasets used in this project:
- *Text Generation*: Choices include Paul Graham essays, Wikipedia (English), and other text-based datasets.
- *XOR Dataset*: Generated custom dataset containing XOR patterns.
- *MNIST and Fashion-MNIST*: Classic benchmark datasets for handwritten digit and clothing item classification.
   

## Results and Observations

1. *Text Prediction Model*:
   - Observations on word embedding patterns via t-SNE, focusing on relationships like synonyms and antonyms.
2. *XOR Classification*:
   - Analysis of decision boundaries for various MLP models, discussing the effects of regularization.
3. *MNIST and Fashion-MNIST*:
   - Comparison of t-SNE embeddings before and after training.
   - Insights into classification patterns and commonly confused digits.

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T34MKAIcwggk",
        "outputId": "664d2662-6087-4d59-ca13-27ca5515d42d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.253108024597168\n",
            "Epoch: 1, Loss: 7.5538458824157715\n",
            "Epoch: 2, Loss: 6.9404826164245605\n",
            "Epoch: 3, Loss: 6.270946979522705\n",
            "Epoch: 4, Loss: 5.444431304931641\n",
            "Epoch: 5, Loss: 4.463929176330566\n",
            "Epoch: 6, Loss: 3.630117893218994\n",
            "Epoch: 7, Loss: 3.132969379425049\n",
            "Epoch: 8, Loss: 2.8181889057159424\n",
            "Epoch: 9, Loss: 2.5814990997314453\n",
            "Epoch: 10, Loss: 2.3891701698303223\n",
            "Epoch: 11, Loss: 2.224639654159546\n",
            "Epoch: 12, Loss: 2.082214593887329\n",
            "Epoch: 13, Loss: 1.9582651853561401\n",
            "Epoch: 14, Loss: 1.8509658575057983\n",
            "Epoch: 15, Loss: 1.754125714302063\n",
            "Epoch: 16, Loss: 1.663224697113037\n",
            "Epoch: 17, Loss: 1.5824570655822754\n",
            "Epoch: 18, Loss: 1.5068684816360474\n",
            "Epoch: 19, Loss: 1.4355195760726929\n",
            "Epoch: 20, Loss: 1.366905689239502\n",
            "Epoch: 21, Loss: 1.3023841381072998\n",
            "Epoch: 22, Loss: 1.239965558052063\n",
            "Epoch: 23, Loss: 1.1783822774887085\n",
            "Epoch: 24, Loss: 1.1164973974227905\n",
            "Epoch: 25, Loss: 1.0564357042312622\n",
            "Epoch: 26, Loss: 0.9991075396537781\n",
            "Epoch: 27, Loss: 0.9448041319847107\n",
            "Epoch: 28, Loss: 0.889735221862793\n",
            "Epoch: 29, Loss: 0.8389010429382324\n",
            "Epoch: 30, Loss: 0.7875626683235168\n",
            "Epoch: 31, Loss: 0.7394120693206787\n",
            "Epoch: 32, Loss: 0.6910073161125183\n",
            "Epoch: 33, Loss: 0.6445260047912598\n",
            "Epoch: 34, Loss: 0.6012612581253052\n",
            "Epoch: 35, Loss: 0.5588460564613342\n",
            "Epoch: 36, Loss: 0.5165626406669617\n",
            "Epoch: 37, Loss: 0.47627294063568115\n",
            "Epoch: 38, Loss: 0.4381791353225708\n",
            "Epoch: 39, Loss: 0.40446263551712036\n",
            "Epoch: 40, Loss: 0.3738705515861511\n",
            "Epoch: 41, Loss: 0.3450919985771179\n",
            "Epoch: 42, Loss: 0.3184589445590973\n",
            "Epoch: 43, Loss: 0.29397860169410706\n",
            "Epoch: 44, Loss: 0.2712787389755249\n",
            "Epoch: 45, Loss: 0.24969491362571716\n",
            "Epoch: 46, Loss: 0.2300100177526474\n",
            "Epoch: 47, Loss: 0.21184208989143372\n",
            "Epoch: 48, Loss: 0.19408035278320312\n",
            "Epoch: 49, Loss: 0.17889092862606049\n",
            "Epoch: 50, Loss: 0.16256128251552582\n",
            "Epoch: 51, Loss: 0.14880958199501038\n",
            "Epoch: 52, Loss: 0.13523581624031067\n",
            "Epoch: 53, Loss: 0.12371977418661118\n",
            "Epoch: 54, Loss: 0.11202393472194672\n",
            "Epoch: 55, Loss: 0.1023125872015953\n",
            "Epoch: 56, Loss: 0.09254675358533859\n",
            "Epoch: 57, Loss: 0.08369775861501694\n",
            "Epoch: 58, Loss: 0.07537664473056793\n",
            "Epoch: 59, Loss: 0.06862975656986237\n",
            "Epoch: 60, Loss: 0.06210235506296158\n",
            "Epoch: 61, Loss: 0.056160639971494675\n",
            "Epoch: 62, Loss: 0.05120318382978439\n",
            "Epoch: 63, Loss: 0.046342913061380386\n",
            "Epoch: 64, Loss: 0.04212605953216553\n",
            "Epoch: 65, Loss: 0.03855717182159424\n",
            "Epoch: 66, Loss: 0.03484449163079262\n",
            "Epoch: 67, Loss: 0.0321512334048748\n",
            "Epoch: 68, Loss: 0.02921716310083866\n",
            "Epoch: 69, Loss: 0.02667577564716339\n",
            "Epoch: 70, Loss: 0.02419029176235199\n",
            "Epoch: 71, Loss: 0.022655529901385307\n",
            "Epoch: 72, Loss: 0.020488278940320015\n",
            "Epoch: 73, Loss: 0.018557364121079445\n",
            "Epoch: 74, Loss: 0.01700470596551895\n",
            "Epoch: 75, Loss: 0.015355038456618786\n",
            "Epoch: 76, Loss: 0.014173898845911026\n",
            "Epoch: 77, Loss: 0.012850929982960224\n",
            "Epoch: 78, Loss: 0.011583891697227955\n",
            "Epoch: 79, Loss: 0.010626513510942459\n",
            "Epoch: 80, Loss: 0.009662183001637459\n",
            "Epoch: 81, Loss: 0.00843451265245676\n",
            "Epoch: 82, Loss: 0.007612844929099083\n",
            "Epoch: 83, Loss: 0.006699143443256617\n",
            "Epoch: 84, Loss: 0.0061169820837676525\n",
            "Epoch: 85, Loss: 0.005325999576598406\n",
            "Epoch: 86, Loss: 0.004729681648313999\n",
            "Epoch: 87, Loss: 0.004267182666808367\n",
            "Epoch: 88, Loss: 0.0036859516985714436\n",
            "Epoch: 89, Loss: 0.0032731941901147366\n",
            "Epoch: 90, Loss: 0.0028890278190374374\n",
            "Epoch: 91, Loss: 0.002606708789244294\n",
            "Epoch: 92, Loss: 0.002342220162972808\n",
            "Epoch: 93, Loss: 0.0020738954190164804\n",
            "Epoch: 94, Loss: 0.0018684761598706245\n",
            "Epoch: 95, Loss: 0.0016584971453994513\n",
            "Epoch: 96, Loss: 0.001488841138780117\n",
            "Epoch: 97, Loss: 0.0013485823292285204\n",
            "Epoch: 98, Loss: 0.0011948667233809829\n",
            "Epoch: 99, Loss: 0.0010946569964289665\n",
            "Epoch: 100, Loss: 0.000971938599832356\n",
            "Epoch: 101, Loss: 0.0008831362938508391\n",
            "Epoch: 102, Loss: 0.0007749035721644759\n",
            "Epoch: 103, Loss: 0.000716875831130892\n",
            "Epoch: 104, Loss: 0.0006339928368106484\n",
            "Epoch: 105, Loss: 0.0005897877854295075\n",
            "Epoch: 106, Loss: 0.0005194044788368046\n",
            "Epoch: 107, Loss: 0.00047964323312044144\n",
            "Epoch: 108, Loss: 0.0004254213999956846\n",
            "Epoch: 109, Loss: 0.00039480580016970634\n",
            "Epoch: 110, Loss: 0.0003477612335700542\n",
            "Epoch: 111, Loss: 0.0003317847731523216\n",
            "Epoch: 112, Loss: 0.00028692453633993864\n",
            "Epoch: 113, Loss: 0.0002737799659371376\n",
            "Epoch: 114, Loss: 0.00023994885850697756\n",
            "Epoch: 115, Loss: 0.00023520436661783606\n",
            "Epoch: 116, Loss: 0.00021676764299627393\n",
            "Epoch: 117, Loss: 0.00032273566466756165\n",
            "Epoch: 118, Loss: 0.05329633876681328\n",
            "Epoch: 119, Loss: 0.008845276199281216\n",
            "Epoch: 120, Loss: 0.000883743807207793\n",
            "Epoch: 121, Loss: 0.0005749068222939968\n",
            "Epoch: 122, Loss: 0.0004766747006215155\n",
            "Epoch: 123, Loss: 0.00042023544665426016\n",
            "Epoch: 124, Loss: 0.00037633441388607025\n",
            "Epoch: 125, Loss: 0.00036077023833058774\n",
            "Epoch: 126, Loss: 0.0003296723880339414\n",
            "Epoch: 127, Loss: 0.00032141953124664724\n",
            "Epoch: 128, Loss: 0.00029561202973127365\n",
            "Epoch: 129, Loss: 0.00029201351571828127\n",
            "Epoch: 130, Loss: 0.00026910207816399634\n",
            "Epoch: 131, Loss: 0.00026808076654560864\n",
            "Epoch: 132, Loss: 0.0002470940526109189\n",
            "Epoch: 133, Loss: 0.0002472437045071274\n",
            "Epoch: 134, Loss: 0.0002276049490319565\n",
            "Epoch: 135, Loss: 0.00022955778695177287\n",
            "Epoch: 136, Loss: 0.00021040970750618726\n",
            "Epoch: 137, Loss: 0.00021154216665308923\n",
            "Epoch: 138, Loss: 0.00019490420527290553\n",
            "Epoch: 139, Loss: 0.00019533868180587888\n",
            "Epoch: 140, Loss: 0.0001790199166862294\n",
            "Epoch: 141, Loss: 0.0001813861890695989\n",
            "Epoch: 142, Loss: 0.00016692640201654285\n",
            "Epoch: 143, Loss: 0.00016879882605280727\n",
            "Epoch: 144, Loss: 0.00015345301653724164\n",
            "Epoch: 145, Loss: 0.00015418525435961783\n",
            "Epoch: 146, Loss: 0.0001399900356773287\n",
            "Epoch: 147, Loss: 0.00014015831402502954\n",
            "Epoch: 148, Loss: 0.00012946744391229004\n",
            "Epoch: 149, Loss: 0.0001283801393583417\n",
            "Epoch: 150, Loss: 0.00011679958697641268\n",
            "Epoch: 151, Loss: 0.00011282152991043404\n",
            "Epoch: 152, Loss: 0.00010388879309175536\n",
            "Epoch: 153, Loss: 0.00010559149814071134\n",
            "Epoch: 154, Loss: 0.0001002329372568056\n",
            "Epoch: 155, Loss: 9.066886559594423e-05\n",
            "Epoch: 156, Loss: 8.348861592821777e-05\n",
            "Epoch: 157, Loss: 8.310843986691907e-05\n",
            "Epoch: 158, Loss: 7.613267371198162e-05\n",
            "Epoch: 159, Loss: 7.488308619940653e-05\n",
            "Epoch: 160, Loss: 6.705089617753401e-05\n",
            "Epoch: 161, Loss: 6.590029079234228e-05\n",
            "Epoch: 162, Loss: 5.833521572640166e-05\n",
            "Epoch: 163, Loss: 5.781954314443283e-05\n",
            "Epoch: 164, Loss: 5.270069232210517e-05\n",
            "Epoch: 165, Loss: 5.1017548685194924e-05\n",
            "Epoch: 166, Loss: 4.587130024447106e-05\n",
            "Epoch: 167, Loss: 4.408382301335223e-05\n",
            "Epoch: 168, Loss: 4.4966101995669305e-05\n",
            "Epoch: 169, Loss: 4.410633118823171e-05\n",
            "Epoch: 170, Loss: 3.432082667131908e-05\n",
            "Epoch: 171, Loss: 3.479605584288947e-05\n",
            "Epoch: 172, Loss: 3.0871640774421394e-05\n",
            "Epoch: 173, Loss: 2.7968984795734286e-05\n",
            "Epoch: 174, Loss: 2.4824970751069486e-05\n",
            "Epoch: 175, Loss: 2.4277605916722678e-05\n",
            "Epoch: 176, Loss: 2.093520015478134e-05\n",
            "Epoch: 177, Loss: 2.1230636775726452e-05\n",
            "Epoch: 178, Loss: 1.878691000456456e-05\n",
            "Epoch: 179, Loss: 1.8451397409080528e-05\n",
            "Epoch: 180, Loss: 1.597100344952196e-05\n",
            "Epoch: 181, Loss: 0.017061015591025352\n",
            "Epoch: 182, Loss: 0.009257528930902481\n",
            "Epoch: 183, Loss: 0.0005376402987167239\n",
            "Epoch: 184, Loss: 0.00023316917940974236\n",
            "Epoch: 185, Loss: 0.00020302030316088349\n",
            "Epoch: 186, Loss: 0.0001832147827371955\n",
            "Epoch: 187, Loss: 0.00016805158520583063\n",
            "Epoch: 188, Loss: 0.00015575732686556876\n",
            "Epoch: 189, Loss: 0.00014556050882674754\n",
            "Epoch: 190, Loss: 0.00013641179248224944\n",
            "Epoch: 191, Loss: 0.00012924674956593663\n",
            "Epoch: 192, Loss: 0.00012161451013525948\n",
            "Epoch: 193, Loss: 0.00011669210653053597\n",
            "Epoch: 194, Loss: 0.00011028383596567437\n",
            "Epoch: 195, Loss: 0.00010590565943857655\n",
            "Epoch: 196, Loss: 0.0001007739920169115\n",
            "Epoch: 197, Loss: 9.713944018585607e-05\n",
            "Epoch: 198, Loss: 9.238292841473594e-05\n",
            "Epoch: 199, Loss: 8.978326513897628e-05\n",
            "Epoch: 200, Loss: 8.488371531711891e-05\n",
            "Epoch: 201, Loss: 8.310628618346527e-05\n",
            "Epoch: 202, Loss: 7.825680950190872e-05\n",
            "Epoch: 203, Loss: 7.677508983761072e-05\n",
            "Epoch: 204, Loss: 7.206087320810184e-05\n",
            "Epoch: 205, Loss: 7.145751442294568e-05\n",
            "Epoch: 206, Loss: 6.703299732180312e-05\n",
            "Epoch: 207, Loss: 6.606424722122028e-05\n",
            "Epoch: 208, Loss: 6.215228495420888e-05\n",
            "Epoch: 209, Loss: 6.145346560515463e-05\n",
            "Epoch: 210, Loss: 5.760717613156885e-05\n",
            "Epoch: 211, Loss: 5.688418968929909e-05\n",
            "Epoch: 212, Loss: 5.3584873967338353e-05\n",
            "Epoch: 213, Loss: 5.28866148670204e-05\n",
            "Epoch: 214, Loss: 4.9637117626843974e-05\n",
            "Epoch: 215, Loss: 4.8908892495092005e-05\n",
            "Epoch: 216, Loss: 4.5770819269819185e-05\n",
            "Epoch: 217, Loss: 4.50789048045408e-05\n",
            "Epoch: 218, Loss: 4.2289073462598026e-05\n",
            "Epoch: 219, Loss: 4.129772059968673e-05\n",
            "Epoch: 220, Loss: 3.9531136280857027e-05\n",
            "Epoch: 221, Loss: 3.873978857882321e-05\n",
            "Epoch: 222, Loss: 3.667754936031997e-05\n",
            "Epoch: 223, Loss: 3.7043260817881674e-05\n",
            "Epoch: 224, Loss: 0.00035042534000240266\n",
            "Epoch: 225, Loss: 0.023216553032398224\n",
            "Epoch: 226, Loss: 0.0021489670034497976\n",
            "Epoch: 227, Loss: 0.0003458786814007908\n",
            "Epoch: 228, Loss: 0.00025846844073385\n",
            "Epoch: 229, Loss: 0.00021965776977594942\n",
            "Epoch: 230, Loss: 0.00019589161092881113\n",
            "Epoch: 231, Loss: 0.00017857327475212514\n",
            "Epoch: 232, Loss: 0.00016526780382264405\n",
            "Epoch: 233, Loss: 0.00015496958803851157\n",
            "Epoch: 234, Loss: 0.00014554578228853643\n",
            "Epoch: 235, Loss: 0.00013861815386917442\n",
            "Epoch: 236, Loss: 0.00013134621258359402\n",
            "Epoch: 237, Loss: 0.00012667159899137914\n",
            "Epoch: 238, Loss: 0.00012014422827633098\n",
            "Epoch: 239, Loss: 0.00011660654126899317\n",
            "Epoch: 240, Loss: 0.00011055458162445575\n",
            "Epoch: 241, Loss: 0.00010832940461114049\n",
            "Epoch: 242, Loss: 0.00010275159729644656\n",
            "Epoch: 243, Loss: 0.00010111346637131646\n",
            "Epoch: 244, Loss: 9.562839841237292e-05\n",
            "Epoch: 245, Loss: 9.405628952663392e-05\n",
            "Epoch: 246, Loss: 8.974525553639978e-05\n",
            "Epoch: 247, Loss: 8.868299482855946e-05\n",
            "Epoch: 248, Loss: 8.335526217706501e-05\n",
            "Epoch: 249, Loss: 8.18271073512733e-05\n",
            "Epoch: 250, Loss: 7.802239997545257e-05\n",
            "Epoch: 251, Loss: 7.665419252589345e-05\n",
            "Epoch: 252, Loss: 7.244129665195942e-05\n",
            "Epoch: 253, Loss: 7.110343722160906e-05\n",
            "Epoch: 254, Loss: 6.754881906090304e-05\n",
            "Epoch: 255, Loss: 6.618548650294542e-05\n",
            "Epoch: 256, Loss: 6.130443944130093e-05\n",
            "Epoch: 257, Loss: 6.048132854630239e-05\n",
            "Epoch: 258, Loss: 5.6538872740929946e-05\n",
            "Epoch: 259, Loss: 5.546892498387024e-05\n",
            "Epoch: 260, Loss: 5.187062924960628e-05\n",
            "Epoch: 261, Loss: 5.079123729956336e-05\n",
            "Epoch: 262, Loss: 4.704938328359276e-05\n",
            "Epoch: 263, Loss: 4.63325013697613e-05\n",
            "Epoch: 264, Loss: 4.189350875094533e-05\n",
            "Epoch: 265, Loss: 4.062177686137147e-05\n",
            "Epoch: 266, Loss: 3.710837700054981e-05\n",
            "Epoch: 267, Loss: 3.725650458363816e-05\n",
            "Epoch: 268, Loss: 3.382105569471605e-05\n",
            "Epoch: 269, Loss: 3.3421438274672255e-05\n",
            "Epoch: 270, Loss: 2.9425713364616968e-05\n",
            "Epoch: 271, Loss: 2.927143759734463e-05\n",
            "Epoch: 272, Loss: 0.00035781238693743944\n",
            "Epoch: 273, Loss: 0.013126633130013943\n",
            "Epoch: 274, Loss: 0.0008712306735105813\n",
            "Epoch: 275, Loss: 0.0003027956117875874\n",
            "Epoch: 276, Loss: 0.00019908617832697928\n",
            "Epoch: 277, Loss: 0.00017236353596672416\n",
            "Epoch: 278, Loss: 0.00015407834143843502\n",
            "Epoch: 279, Loss: 0.00013997529458720237\n",
            "Epoch: 280, Loss: 0.00012904926552437246\n",
            "Epoch: 281, Loss: 0.00011983871809206903\n",
            "Epoch: 282, Loss: 0.00011209418880753219\n",
            "Epoch: 283, Loss: 0.00010533938620937988\n",
            "Epoch: 284, Loss: 9.959376620827243e-05\n",
            "Epoch: 285, Loss: 9.443606541026384e-05\n",
            "Epoch: 286, Loss: 8.985328895505518e-05\n",
            "Epoch: 287, Loss: 8.551607606932521e-05\n",
            "Epoch: 288, Loss: 8.1632737419568e-05\n",
            "Epoch: 289, Loss: 7.816596917109564e-05\n",
            "Epoch: 290, Loss: 7.499898492824286e-05\n",
            "Epoch: 291, Loss: 7.213615026557818e-05\n",
            "Epoch: 292, Loss: 6.929053051862866e-05\n",
            "Epoch: 293, Loss: 6.667028355877846e-05\n",
            "Epoch: 294, Loss: 6.46032058284618e-05\n",
            "Epoch: 295, Loss: 6.147824751678854e-05\n",
            "Epoch: 296, Loss: 6.091469185776077e-05\n",
            "Epoch: 297, Loss: 5.703632268705405e-05\n",
            "Epoch: 298, Loss: 5.700022302335128e-05\n",
            "Epoch: 299, Loss: 5.3245839808369055e-05\n",
            "Epoch: 300, Loss: 5.255328869679943e-05\n",
            "Epoch: 301, Loss: 4.9960348405875266e-05\n",
            "Epoch: 302, Loss: 4.903187073068693e-05\n",
            "Epoch: 303, Loss: 4.641656778403558e-05\n",
            "Epoch: 304, Loss: 4.566726784105413e-05\n",
            "Epoch: 305, Loss: 4.303970854380168e-05\n",
            "Epoch: 306, Loss: 4.236363020027056e-05\n",
            "Epoch: 307, Loss: 3.963769995607436e-05\n",
            "Epoch: 308, Loss: 3.900715819327161e-05\n",
            "Epoch: 309, Loss: 3.631589061114937e-05\n",
            "Epoch: 310, Loss: 3.569789623725228e-05\n",
            "Epoch: 311, Loss: 3.318185918033123e-05\n",
            "Epoch: 312, Loss: 3.2062409445643425e-05\n",
            "Epoch: 313, Loss: 3.088924495386891e-05\n",
            "Epoch: 314, Loss: 2.887035225285217e-05\n",
            "Epoch: 315, Loss: 2.838688305928372e-05\n",
            "Epoch: 316, Loss: 2.9504655685741454e-05\n",
            "Epoch: 317, Loss: 0.005620916374027729\n",
            "Epoch: 318, Loss: 0.002915419638156891\n",
            "Epoch: 319, Loss: 0.0003463488246779889\n",
            "Epoch: 320, Loss: 0.00019010191317647696\n",
            "Epoch: 321, Loss: 0.00015214159793686122\n",
            "Epoch: 322, Loss: 0.0001379283203277737\n",
            "Epoch: 323, Loss: 0.00012710852024611086\n",
            "Epoch: 324, Loss: 0.00011873067705892026\n",
            "Epoch: 325, Loss: 0.00011165047180838883\n",
            "Epoch: 326, Loss: 0.00010607147123664618\n",
            "Epoch: 327, Loss: 0.00010058734915219247\n",
            "Epoch: 328, Loss: 9.724559640744701e-05\n",
            "Epoch: 329, Loss: 9.199711348628625e-05\n",
            "Epoch: 330, Loss: 8.990654168883339e-05\n",
            "Epoch: 331, Loss: 8.545831951778382e-05\n",
            "Epoch: 332, Loss: 8.380444342037663e-05\n",
            "Epoch: 333, Loss: 7.936318434076384e-05\n",
            "Epoch: 334, Loss: 7.806402572896332e-05\n",
            "Epoch: 335, Loss: 7.415526488330215e-05\n",
            "Epoch: 336, Loss: 7.29166204109788e-05\n",
            "Epoch: 337, Loss: 6.93571928422898e-05\n",
            "Epoch: 338, Loss: 6.762649718439206e-05\n",
            "Epoch: 339, Loss: 6.401631253538653e-05\n",
            "Epoch: 340, Loss: 6.295257480815053e-05\n",
            "Epoch: 341, Loss: 5.918213719269261e-05\n",
            "Epoch: 342, Loss: 5.8381177950650454e-05\n",
            "Epoch: 343, Loss: 5.546768443309702e-05\n",
            "Epoch: 344, Loss: 5.490291732712649e-05\n",
            "Epoch: 345, Loss: 5.13489794684574e-05\n",
            "Epoch: 346, Loss: 4.988661748939194e-05\n",
            "Epoch: 347, Loss: 4.672767681768164e-05\n",
            "Epoch: 348, Loss: 4.548884680843912e-05\n",
            "Epoch: 349, Loss: 4.2783583921846e-05\n",
            "Epoch: 350, Loss: 4.150619133724831e-05\n",
            "Epoch: 351, Loss: 3.883582758135162e-05\n",
            "Epoch: 352, Loss: 3.7623503885697573e-05\n",
            "Epoch: 353, Loss: 3.495422060950659e-05\n",
            "Epoch: 354, Loss: 3.39969337801449e-05\n",
            "Epoch: 355, Loss: 3.131170888082124e-05\n",
            "Epoch: 356, Loss: 2.9830091079929844e-05\n",
            "Epoch: 357, Loss: 2.73432979156496e-05\n",
            "Epoch: 358, Loss: 2.72510114882607e-05\n",
            "Epoch: 359, Loss: 3.091985490755178e-05\n",
            "Epoch: 360, Loss: 2.358537858526688e-05\n",
            "Epoch: 361, Loss: 2.1643147192662582e-05\n",
            "Epoch: 362, Loss: 2.0887739083264023e-05\n",
            "Epoch: 363, Loss: 0.027334602549672127\n",
            "Epoch: 364, Loss: 0.0015001774299889803\n",
            "Epoch: 365, Loss: 0.0002557258994784206\n",
            "Epoch: 366, Loss: 0.0001911004219437018\n",
            "Epoch: 367, Loss: 0.00016293447697535157\n",
            "Epoch: 368, Loss: 0.0001427477109245956\n",
            "Epoch: 369, Loss: 0.00012954279372934252\n",
            "Epoch: 370, Loss: 0.00011902293772436678\n",
            "Epoch: 371, Loss: 0.00011029676534235477\n",
            "Epoch: 372, Loss: 0.00010333543468732387\n",
            "Epoch: 373, Loss: 9.720581147121266e-05\n",
            "Epoch: 374, Loss: 9.195331949740648e-05\n",
            "Epoch: 375, Loss: 8.71526644914411e-05\n",
            "Epoch: 376, Loss: 8.295513543998823e-05\n",
            "Epoch: 377, Loss: 7.905750680947676e-05\n",
            "Epoch: 378, Loss: 7.575286872452125e-05\n",
            "Epoch: 379, Loss: 7.248143811011687e-05\n",
            "Epoch: 380, Loss: 6.958799349376932e-05\n",
            "Epoch: 381, Loss: 6.674803444184363e-05\n",
            "Epoch: 382, Loss: 6.461903831223026e-05\n",
            "Epoch: 383, Loss: 6.186809332575649e-05\n",
            "Epoch: 384, Loss: 6.016789120621979e-05\n",
            "Epoch: 385, Loss: 5.739232437917963e-05\n",
            "Epoch: 386, Loss: 5.5837706895545125e-05\n",
            "Epoch: 387, Loss: 5.3336269047576934e-05\n",
            "Epoch: 388, Loss: 5.200899613555521e-05\n",
            "Epoch: 389, Loss: 4.9562524509383366e-05\n",
            "Epoch: 390, Loss: 4.806989090866409e-05\n",
            "Epoch: 391, Loss: 4.577615254675038e-05\n",
            "Epoch: 392, Loss: 4.435905066202395e-05\n",
            "Epoch: 393, Loss: 4.280023495084606e-05\n",
            "Epoch: 394, Loss: 4.110197551199235e-05\n",
            "Epoch: 395, Loss: 3.8783564377808943e-05\n",
            "Epoch: 396, Loss: 3.7693440390285105e-05\n",
            "Epoch: 397, Loss: 3.6156794521957636e-05\n",
            "Epoch: 398, Loss: 3.50073205481749e-05\n",
            "Epoch: 399, Loss: 3.218611527699977e-05\n",
            "Epoch: 400, Loss: 3.0433537176577374e-05\n",
            "Epoch: 401, Loss: 2.8359407224343158e-05\n",
            "Epoch: 402, Loss: 2.7015110390493646e-05\n",
            "Epoch: 403, Loss: 2.4668363039381802e-05\n",
            "Epoch: 404, Loss: 2.367624983889982e-05\n",
            "Epoch: 405, Loss: 2.2030319087207317e-05\n",
            "Epoch: 406, Loss: 2.1264568204060197e-05\n",
            "Epoch: 407, Loss: 1.9585504560382105e-05\n",
            "Epoch: 408, Loss: 1.8962688045576215e-05\n",
            "Epoch: 409, Loss: 1.8069866200676188e-05\n",
            "Epoch: 410, Loss: 1.6447000234620646e-05\n",
            "Epoch: 411, Loss: 1.7012842363328673e-05\n",
            "Epoch: 412, Loss: 3.358601315994747e-05\n",
            "Epoch: 413, Loss: 0.014540513977408409\n",
            "Epoch: 414, Loss: 0.001736916252411902\n",
            "Epoch: 415, Loss: 0.00020266785577405244\n",
            "Epoch: 416, Loss: 0.00016049962141551077\n",
            "Epoch: 417, Loss: 0.0001412447018083185\n",
            "Epoch: 418, Loss: 0.0001277293631574139\n",
            "Epoch: 419, Loss: 0.00011726604861905798\n",
            "Epoch: 420, Loss: 0.00010883657523663715\n",
            "Epoch: 421, Loss: 0.0001017355898511596\n",
            "Epoch: 422, Loss: 9.550037793815136e-05\n",
            "Epoch: 423, Loss: 9.026558109326288e-05\n",
            "Epoch: 424, Loss: 8.509930921718478e-05\n",
            "Epoch: 425, Loss: 8.073116623563692e-05\n",
            "Epoch: 426, Loss: 7.682245632167906e-05\n",
            "Epoch: 427, Loss: 7.312363595701754e-05\n",
            "Epoch: 428, Loss: 6.982775084907189e-05\n",
            "Epoch: 429, Loss: 6.670265429420397e-05\n",
            "Epoch: 430, Loss: 6.374080112436786e-05\n",
            "Epoch: 431, Loss: 6.097272125771269e-05\n",
            "Epoch: 432, Loss: 5.8417241234565154e-05\n",
            "Epoch: 433, Loss: 5.590840373770334e-05\n",
            "Epoch: 434, Loss: 5.355405664886348e-05\n",
            "Epoch: 435, Loss: 5.124012750457041e-05\n",
            "Epoch: 436, Loss: 4.916337275062688e-05\n",
            "Epoch: 437, Loss: 4.702184014604427e-05\n",
            "Epoch: 438, Loss: 4.554229599307291e-05\n",
            "Epoch: 439, Loss: 4.288724812795408e-05\n",
            "Epoch: 440, Loss: 4.2429212044226006e-05\n",
            "Epoch: 441, Loss: 3.97844378312584e-05\n",
            "Epoch: 442, Loss: 3.869738065986894e-05\n",
            "Epoch: 443, Loss: 3.64881670975592e-05\n",
            "Epoch: 444, Loss: 3.520014797686599e-05\n",
            "Epoch: 445, Loss: 3.319443203508854e-05\n",
            "Epoch: 446, Loss: 3.1976509490050375e-05\n",
            "Epoch: 447, Loss: 3.0343508115038276e-05\n",
            "Epoch: 448, Loss: 2.9471997549990192e-05\n",
            "Epoch: 449, Loss: 2.746001337072812e-05\n",
            "Epoch: 450, Loss: 2.6427407647133805e-05\n",
            "Epoch: 451, Loss: 2.462126394675579e-05\n",
            "Epoch: 452, Loss: 2.3430082364939153e-05\n",
            "Epoch: 453, Loss: 2.2254089344642125e-05\n",
            "Epoch: 454, Loss: 2.1060932340333238e-05\n",
            "Epoch: 455, Loss: 1.9345126929692924e-05\n",
            "Epoch: 456, Loss: 1.8650152924237773e-05\n",
            "Epoch: 457, Loss: 1.748725844663568e-05\n",
            "Epoch: 458, Loss: 1.6379904991481453e-05\n",
            "Epoch: 459, Loss: 1.4829460269538686e-05\n",
            "Epoch: 460, Loss: 1.4240268683352042e-05\n",
            "Epoch: 461, Loss: 1.2925066585012246e-05\n",
            "Epoch: 462, Loss: 1.2339002751105e-05\n",
            "Epoch: 463, Loss: 1.1289016583759803e-05\n",
            "Epoch: 464, Loss: 0.017898904159665108\n",
            "Epoch: 465, Loss: 0.006508155260235071\n",
            "Epoch: 466, Loss: 0.00015589024405926466\n",
            "Epoch: 467, Loss: 9.950298408512026e-05\n",
            "Epoch: 468, Loss: 8.590969810029492e-05\n",
            "Epoch: 469, Loss: 7.737161649856716e-05\n",
            "Epoch: 470, Loss: 7.123092655092478e-05\n",
            "Epoch: 471, Loss: 6.650241994066164e-05\n",
            "Epoch: 472, Loss: 6.262627721298486e-05\n",
            "Epoch: 473, Loss: 5.935154331382364e-05\n",
            "Epoch: 474, Loss: 5.654184860759415e-05\n",
            "Epoch: 475, Loss: 5.410710218711756e-05\n",
            "Epoch: 476, Loss: 5.174818215891719e-05\n",
            "Epoch: 477, Loss: 5.003679689252749e-05\n",
            "Epoch: 478, Loss: 4.8057729145511985e-05\n",
            "Epoch: 479, Loss: 4.615302532329224e-05\n",
            "Epoch: 480, Loss: 4.462797369342297e-05\n",
            "Epoch: 481, Loss: 4.3146268581040204e-05\n",
            "Epoch: 482, Loss: 4.1659408452687785e-05\n",
            "Epoch: 483, Loss: 4.021300992462784e-05\n",
            "Epoch: 484, Loss: 3.8797672459622845e-05\n",
            "Epoch: 485, Loss: 3.7473819247679785e-05\n",
            "Epoch: 486, Loss: 3.61131715180818e-05\n",
            "Epoch: 487, Loss: 3.4980312193511054e-05\n",
            "Epoch: 488, Loss: 3.366953751537949e-05\n",
            "Epoch: 489, Loss: 3.269289663876407e-05\n",
            "Epoch: 490, Loss: 3.109957106062211e-05\n",
            "Epoch: 491, Loss: 3.012381785083562e-05\n",
            "Epoch: 492, Loss: 2.8769904020009562e-05\n",
            "Epoch: 493, Loss: 2.823550494213123e-05\n",
            "Epoch: 494, Loss: 2.672620576049667e-05\n",
            "Epoch: 495, Loss: 2.5764666133909486e-05\n",
            "Epoch: 496, Loss: 2.4618613679194823e-05\n",
            "Epoch: 497, Loss: 2.3622527805855498e-05\n",
            "Epoch: 498, Loss: 2.2628186343354173e-05\n",
            "Epoch: 499, Loss: 2.1820898837177083e-05\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 1]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size = 10  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 64\n",
        "activation = 'relu'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(500):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_2': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'trained_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  import torch\n",
        "  import torch.nn.functional as F\n",
        "  from torch import nn\n",
        "  import re\n",
        "  from urllib import request\n",
        "\n",
        "  # Load the text data\n",
        "  url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "  response = request.urlopen(url)\n",
        "  text = response.read().decode('utf-8')\n",
        "\n",
        "  # Preprocess the text\n",
        "  text = text.lower().replace('\\n', ' ')\n",
        "  words = text.split()\n",
        "  words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 1]  # Remove punctuation and short words\n",
        "\n",
        "  # Create vocabulary\n",
        "  vocab = sorted(set(words))\n",
        "  stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "  itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "  # Prepare the dataset\n",
        "  block_size = 5  # Number of previous words to predict the next one\n",
        "  X, Y = [], []\n",
        "\n",
        "  for i in range(len(words) - block_size):\n",
        "      X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "      Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Move data to GPU\n",
        "  X = torch.tensor(X).to(device)\n",
        "  Y = torch.tensor(Y).to(device)\n",
        "\n",
        "  # Define the MLP model\n",
        "  class NextWordMLP(nn.Module):\n",
        "      def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "          super().__init__()\n",
        "          self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "          self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "          self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "          # Set the activation function\n",
        "          if activation == 'relu':\n",
        "              self.activation = nn.ReLU()\n",
        "          elif activation == 'tanh':\n",
        "              self.activation = nn.Tanh()\n",
        "          elif activation == 'sigmoid':\n",
        "              self.activation = nn.Sigmoid()\n",
        "          else:\n",
        "              raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "          x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "          x = self.fc1(x)\n",
        "          x = self.activation(x)\n",
        "          x = self.fc2(x)\n",
        "          return x\n",
        "\n",
        "  # Initialize and train the model\n",
        "  emb_dim = 128\n",
        "  activation = 'tanh'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "  model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "  # Training loop\n",
        "  batch_size = 512\n",
        "  for epoch in range(500):  # Reduce epoch for quicker training\n",
        "      for i in range(0, X.shape[0], batch_size):\n",
        "          x = X[i:i + batch_size]\n",
        "          y = Y[i:i + batch_size]\n",
        "          y_pred = model(x)\n",
        "          loss = loss_fn(y_pred, y)\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "          opt.step()\n",
        "          opt.zero_grad()\n",
        "      print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "  # Save the trained model and vocabulary\n",
        "  torch.save({\n",
        "      'model_state_dict_4': model.state_dict(),\n",
        "      'stoi': stoi,\n",
        "      'itos': itos,\n",
        "      'block_size': block_size,\n",
        "      'emb_dim': emb_dim,\n",
        "      'activation': activation,\n",
        "  }, 'trained_model_7.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_XnLxn81j1X",
        "outputId": "3f7d158d-8408-4e8a-9e91-e44c002c21f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.673714637756348\n",
            "Epoch: 1, Loss: 7.718428134918213\n",
            "Epoch: 2, Loss: 7.027989387512207\n",
            "Epoch: 3, Loss: 6.4449334144592285\n",
            "Epoch: 4, Loss: 5.946618556976318\n",
            "Epoch: 5, Loss: 5.499574661254883\n",
            "Epoch: 6, Loss: 5.079165458679199\n",
            "Epoch: 7, Loss: 4.6753692626953125\n",
            "Epoch: 8, Loss: 4.297863960266113\n",
            "Epoch: 9, Loss: 3.9464595317840576\n",
            "Epoch: 10, Loss: 3.6218655109405518\n",
            "Epoch: 11, Loss: 3.3198394775390625\n",
            "Epoch: 12, Loss: 3.040300130844116\n",
            "Epoch: 13, Loss: 2.7814507484436035\n",
            "Epoch: 14, Loss: 2.5447943210601807\n",
            "Epoch: 15, Loss: 2.3272502422332764\n",
            "Epoch: 16, Loss: 2.1278440952301025\n",
            "Epoch: 17, Loss: 1.9429484605789185\n",
            "Epoch: 18, Loss: 1.7751718759536743\n",
            "Epoch: 19, Loss: 1.6167782545089722\n",
            "Epoch: 20, Loss: 1.474145531654358\n",
            "Epoch: 21, Loss: 1.3392846584320068\n",
            "Epoch: 22, Loss: 1.2180719375610352\n",
            "Epoch: 23, Loss: 1.1096935272216797\n",
            "Epoch: 24, Loss: 1.0102531909942627\n",
            "Epoch: 25, Loss: 0.9214450120925903\n",
            "Epoch: 26, Loss: 0.8397054672241211\n",
            "Epoch: 27, Loss: 0.7666102647781372\n",
            "Epoch: 28, Loss: 0.6991007924079895\n",
            "Epoch: 29, Loss: 0.6391618251800537\n",
            "Epoch: 30, Loss: 0.5842709541320801\n",
            "Epoch: 31, Loss: 0.5360462665557861\n",
            "Epoch: 32, Loss: 0.4918252229690552\n",
            "Epoch: 33, Loss: 0.4524984061717987\n",
            "Epoch: 34, Loss: 0.4159533381462097\n",
            "Epoch: 35, Loss: 0.3837331533432007\n",
            "Epoch: 36, Loss: 0.35371437668800354\n",
            "Epoch: 37, Loss: 0.32746925950050354\n",
            "Epoch: 38, Loss: 0.3030610978603363\n",
            "Epoch: 39, Loss: 0.28165239095687866\n",
            "Epoch: 40, Loss: 0.26155558228492737\n",
            "Epoch: 41, Loss: 0.24390028417110443\n",
            "Epoch: 42, Loss: 0.22716863453388214\n",
            "Epoch: 43, Loss: 0.21241123974323273\n",
            "Epoch: 44, Loss: 0.19840353727340698\n",
            "Epoch: 45, Loss: 0.18563982844352722\n",
            "Epoch: 46, Loss: 0.1739933043718338\n",
            "Epoch: 47, Loss: 0.1626059114933014\n",
            "Epoch: 48, Loss: 0.1528562605381012\n",
            "Epoch: 49, Loss: 0.1427392214536667\n",
            "Epoch: 50, Loss: 0.13458462059497833\n",
            "Epoch: 51, Loss: 0.12578795850276947\n",
            "Epoch: 52, Loss: 0.11907132714986801\n",
            "Epoch: 53, Loss: 0.11132325232028961\n",
            "Epoch: 54, Loss: 0.10566141456365585\n",
            "Epoch: 55, Loss: 0.09839963912963867\n",
            "Epoch: 56, Loss: 0.09282923489809036\n",
            "Epoch: 57, Loss: 0.08622575551271439\n",
            "Epoch: 58, Loss: 0.08268678188323975\n",
            "Epoch: 59, Loss: 0.07742755115032196\n",
            "Epoch: 60, Loss: 0.07474233955144882\n",
            "Epoch: 61, Loss: 0.07000499218702316\n",
            "Epoch: 62, Loss: 0.06779751181602478\n",
            "Epoch: 63, Loss: 0.06344526261091232\n",
            "Epoch: 64, Loss: 0.06168718263506889\n",
            "Epoch: 65, Loss: 0.05762072652578354\n",
            "Epoch: 66, Loss: 0.05647442489862442\n",
            "Epoch: 67, Loss: 0.052454039454460144\n",
            "Epoch: 68, Loss: 0.0517716109752655\n",
            "Epoch: 69, Loss: 0.04818626493215561\n",
            "Epoch: 70, Loss: 0.046630069613456726\n",
            "Epoch: 71, Loss: 0.04421619698405266\n",
            "Epoch: 72, Loss: 0.04314970597624779\n",
            "Epoch: 73, Loss: 0.04177958890795708\n",
            "Epoch: 74, Loss: 0.03978216275572777\n",
            "Epoch: 75, Loss: 0.03716077283024788\n",
            "Epoch: 76, Loss: 0.03679633513092995\n",
            "Epoch: 77, Loss: 0.03450381010770798\n",
            "Epoch: 78, Loss: 0.03396555036306381\n",
            "Epoch: 79, Loss: 0.031865060329437256\n",
            "Epoch: 80, Loss: 0.031357452273368835\n",
            "Epoch: 81, Loss: 0.02931063063442707\n",
            "Epoch: 82, Loss: 0.029523063451051712\n",
            "Epoch: 83, Loss: 0.026994256302714348\n",
            "Epoch: 84, Loss: 0.026783829554915428\n",
            "Epoch: 85, Loss: 0.025137457996606827\n",
            "Epoch: 86, Loss: 0.024948131293058395\n",
            "Epoch: 87, Loss: 0.023569172248244286\n",
            "Epoch: 88, Loss: 0.02356588840484619\n",
            "Epoch: 89, Loss: 0.022381898015737534\n",
            "Epoch: 90, Loss: 0.022447383031249046\n",
            "Epoch: 91, Loss: 0.021395904943346977\n",
            "Epoch: 92, Loss: 0.021394923329353333\n",
            "Epoch: 93, Loss: 0.02020346373319626\n",
            "Epoch: 94, Loss: 0.020383747294545174\n",
            "Epoch: 95, Loss: 0.01936945877969265\n",
            "Epoch: 96, Loss: 0.019719615578651428\n",
            "Epoch: 97, Loss: 0.018271466717123985\n",
            "Epoch: 98, Loss: 0.018786409869790077\n",
            "Epoch: 99, Loss: 0.01784054934978485\n",
            "Epoch: 100, Loss: 0.018007565289735794\n",
            "Epoch: 101, Loss: 0.017138125374913216\n",
            "Epoch: 102, Loss: 0.017428921535611153\n",
            "Epoch: 103, Loss: 0.016684040427207947\n",
            "Epoch: 104, Loss: 0.017217285931110382\n",
            "Epoch: 105, Loss: 0.015910573303699493\n",
            "Epoch: 106, Loss: 0.016454458236694336\n",
            "Epoch: 107, Loss: 0.015773434191942215\n",
            "Epoch: 108, Loss: 0.016147755086421967\n",
            "Epoch: 109, Loss: 0.015206788666546345\n",
            "Epoch: 110, Loss: 0.01573445461690426\n",
            "Epoch: 111, Loss: 0.014971797354519367\n",
            "Epoch: 112, Loss: 0.015570747666060925\n",
            "Epoch: 113, Loss: 0.014463755302131176\n",
            "Epoch: 114, Loss: 0.015004611574113369\n",
            "Epoch: 115, Loss: 0.014458142220973969\n",
            "Epoch: 116, Loss: 0.014827755279839039\n",
            "Epoch: 117, Loss: 0.014038038440048695\n",
            "Epoch: 118, Loss: 0.014526103623211384\n",
            "Epoch: 119, Loss: 0.013936770148575306\n",
            "Epoch: 120, Loss: 0.014417290687561035\n",
            "Epoch: 121, Loss: 0.013647552579641342\n",
            "Epoch: 122, Loss: 0.014105857349932194\n",
            "Epoch: 123, Loss: 0.013676322996616364\n",
            "Epoch: 124, Loss: 0.01415562815964222\n",
            "Epoch: 125, Loss: 0.0133646409958601\n",
            "Epoch: 126, Loss: 0.013977373018860817\n",
            "Epoch: 127, Loss: 0.013325262814760208\n",
            "Epoch: 128, Loss: 0.013882474973797798\n",
            "Epoch: 129, Loss: 0.013140168972313404\n",
            "Epoch: 130, Loss: 0.013626999221742153\n",
            "Epoch: 131, Loss: 0.013180090114474297\n",
            "Epoch: 132, Loss: 0.013625355437397957\n",
            "Epoch: 133, Loss: 0.01294769998639822\n",
            "Epoch: 134, Loss: 0.013458318077027798\n",
            "Epoch: 135, Loss: 0.01306142844259739\n",
            "Epoch: 136, Loss: 0.01353849284350872\n",
            "Epoch: 137, Loss: 0.012807674705982208\n",
            "Epoch: 138, Loss: 0.013316147029399872\n",
            "Epoch: 139, Loss: 0.012880591675639153\n",
            "Epoch: 140, Loss: 0.013335580006241798\n",
            "Epoch: 141, Loss: 0.012681017629802227\n",
            "Epoch: 142, Loss: 0.013149254955351353\n",
            "Epoch: 143, Loss: 0.012792041525244713\n",
            "Epoch: 144, Loss: 0.01320671197026968\n",
            "Epoch: 145, Loss: 0.012560383416712284\n",
            "Epoch: 146, Loss: 0.013101099990308285\n",
            "Epoch: 147, Loss: 0.012643137015402317\n",
            "Epoch: 148, Loss: 0.013116275891661644\n",
            "Epoch: 149, Loss: 0.012491635046899319\n",
            "Epoch: 150, Loss: 0.012985961511731148\n",
            "Epoch: 151, Loss: 0.012597442604601383\n",
            "Epoch: 152, Loss: 0.013041774742305279\n",
            "Epoch: 153, Loss: 0.012402771040797234\n",
            "Epoch: 154, Loss: 0.012923512607812881\n",
            "Epoch: 155, Loss: 0.012505360879004002\n",
            "Epoch: 156, Loss: 0.01313086785376072\n",
            "Epoch: 157, Loss: 0.012826798483729362\n",
            "Epoch: 158, Loss: 0.014829671941697598\n",
            "Epoch: 159, Loss: 0.013096361421048641\n",
            "Epoch: 160, Loss: 0.01263686828315258\n",
            "Epoch: 161, Loss: 0.012431534938514233\n",
            "Epoch: 162, Loss: 0.01266875583678484\n",
            "Epoch: 163, Loss: 0.012463602237403393\n",
            "Epoch: 164, Loss: 0.012812386266887188\n",
            "Epoch: 165, Loss: 0.012245445512235165\n",
            "Epoch: 166, Loss: 0.012767105363309383\n",
            "Epoch: 167, Loss: 0.01230170950293541\n",
            "Epoch: 168, Loss: 0.012821445241570473\n",
            "Epoch: 169, Loss: 0.012223849073052406\n",
            "Epoch: 170, Loss: 0.012721853330731392\n",
            "Epoch: 171, Loss: 0.01234774012118578\n",
            "Epoch: 172, Loss: 0.012790235690772533\n",
            "Epoch: 173, Loss: 0.01221739873290062\n",
            "Epoch: 174, Loss: 0.012678097002208233\n",
            "Epoch: 175, Loss: 0.012372622266411781\n",
            "Epoch: 176, Loss: 0.01275006402283907\n",
            "Epoch: 177, Loss: 0.012254960834980011\n",
            "Epoch: 178, Loss: 0.012699889950454235\n",
            "Epoch: 179, Loss: 0.012352628633379936\n",
            "Epoch: 180, Loss: 0.013705242425203323\n",
            "Epoch: 181, Loss: 0.013585486449301243\n",
            "Epoch: 182, Loss: 0.013812677003443241\n",
            "Epoch: 183, Loss: 0.012354586273431778\n",
            "Epoch: 184, Loss: 0.012815818190574646\n",
            "Epoch: 185, Loss: 0.012186226435005665\n",
            "Epoch: 186, Loss: 0.01264901738613844\n",
            "Epoch: 187, Loss: 0.01233020331710577\n",
            "Epoch: 188, Loss: 0.012724576517939568\n",
            "Epoch: 189, Loss: 0.012163981795310974\n",
            "Epoch: 190, Loss: 0.012624312192201614\n",
            "Epoch: 191, Loss: 0.01228793803602457\n",
            "Epoch: 192, Loss: 0.012708649039268494\n",
            "Epoch: 193, Loss: 0.012155497446656227\n",
            "Epoch: 194, Loss: 0.012602224014699459\n",
            "Epoch: 195, Loss: 0.012298363260924816\n",
            "Epoch: 196, Loss: 0.012707700952887535\n",
            "Epoch: 197, Loss: 0.012146933004260063\n",
            "Epoch: 198, Loss: 0.012627176940441132\n",
            "Epoch: 199, Loss: 0.012262256816029549\n",
            "Epoch: 200, Loss: 0.01270866859704256\n",
            "Epoch: 201, Loss: 0.012145791202783585\n",
            "Epoch: 202, Loss: 0.012603288516402245\n",
            "Epoch: 203, Loss: 0.012272004969418049\n",
            "Epoch: 204, Loss: 0.01269783265888691\n",
            "Epoch: 205, Loss: 0.012130100280046463\n",
            "Epoch: 206, Loss: 0.01262608915567398\n",
            "Epoch: 207, Loss: 0.012244229204952717\n",
            "Epoch: 208, Loss: 0.012902761809527874\n",
            "Epoch: 209, Loss: 0.015480329282581806\n",
            "Epoch: 210, Loss: 0.013333014212548733\n",
            "Epoch: 211, Loss: 0.012368653900921345\n",
            "Epoch: 212, Loss: 0.012816368602216244\n",
            "Epoch: 213, Loss: 0.012184175662696362\n",
            "Epoch: 214, Loss: 0.012635238468647003\n",
            "Epoch: 215, Loss: 0.012311223894357681\n",
            "Epoch: 216, Loss: 0.012711982242763042\n",
            "Epoch: 217, Loss: 0.012129830196499825\n",
            "Epoch: 218, Loss: 0.012649696320295334\n",
            "Epoch: 219, Loss: 0.01218237355351448\n",
            "Epoch: 220, Loss: 0.012699631974101067\n",
            "Epoch: 221, Loss: 0.012083880603313446\n",
            "Epoch: 222, Loss: 0.012600652873516083\n",
            "Epoch: 223, Loss: 0.012204840779304504\n",
            "Epoch: 224, Loss: 0.012700032442808151\n",
            "Epoch: 225, Loss: 0.012061185203492641\n",
            "Epoch: 226, Loss: 0.012612460181117058\n",
            "Epoch: 227, Loss: 0.01217521633952856\n",
            "Epoch: 228, Loss: 0.012698625214397907\n",
            "Epoch: 229, Loss: 0.012051683850586414\n",
            "Epoch: 230, Loss: 0.012597288005053997\n",
            "Epoch: 231, Loss: 0.012183154001832008\n",
            "Epoch: 232, Loss: 0.01268333476036787\n",
            "Epoch: 233, Loss: 0.012041615322232246\n",
            "Epoch: 234, Loss: 0.012605362571775913\n",
            "Epoch: 235, Loss: 0.012160991318523884\n",
            "Epoch: 236, Loss: 0.01269292738288641\n",
            "Epoch: 237, Loss: 0.012041609734296799\n",
            "Epoch: 238, Loss: 0.012594037689268589\n",
            "Epoch: 239, Loss: 0.01214525755494833\n",
            "Epoch: 240, Loss: 0.012693249620497227\n",
            "Epoch: 241, Loss: 0.012019636109471321\n",
            "Epoch: 242, Loss: 0.012555297464132309\n",
            "Epoch: 243, Loss: 0.01217161025851965\n",
            "Epoch: 244, Loss: 0.012666095048189163\n",
            "Epoch: 245, Loss: 0.012074915692210197\n",
            "Epoch: 246, Loss: 0.012668910436332226\n",
            "Epoch: 247, Loss: 0.020309433341026306\n",
            "Epoch: 248, Loss: 0.013459589332342148\n",
            "Epoch: 249, Loss: 0.01195372175425291\n",
            "Epoch: 250, Loss: 0.012718657962977886\n",
            "Epoch: 251, Loss: 0.01213949266821146\n",
            "Epoch: 252, Loss: 0.01270944532006979\n",
            "Epoch: 253, Loss: 0.012033211998641491\n",
            "Epoch: 254, Loss: 0.012546143494546413\n",
            "Epoch: 255, Loss: 0.01218772865831852\n",
            "Epoch: 256, Loss: 0.012623620219528675\n",
            "Epoch: 257, Loss: 0.012029561214148998\n",
            "Epoch: 258, Loss: 0.01254741195589304\n",
            "Epoch: 259, Loss: 0.012111402116715908\n",
            "Epoch: 260, Loss: 0.012618256732821465\n",
            "Epoch: 261, Loss: 0.012007943354547024\n",
            "Epoch: 262, Loss: 0.012518230825662613\n",
            "Epoch: 263, Loss: 0.012143590487539768\n",
            "Epoch: 264, Loss: 0.012616554275155067\n",
            "Epoch: 265, Loss: 0.012000523507595062\n",
            "Epoch: 266, Loss: 0.01253691129386425\n",
            "Epoch: 267, Loss: 0.012117636390030384\n",
            "Epoch: 268, Loss: 0.012610982172191143\n",
            "Epoch: 269, Loss: 0.011990996077656746\n",
            "Epoch: 270, Loss: 0.012530182488262653\n",
            "Epoch: 271, Loss: 0.012125914916396141\n",
            "Epoch: 272, Loss: 0.012627670541405678\n",
            "Epoch: 273, Loss: 0.011994699947535992\n",
            "Epoch: 274, Loss: 0.012540624476969242\n",
            "Epoch: 275, Loss: 0.01210402324795723\n",
            "Epoch: 276, Loss: 0.012636320665478706\n",
            "Epoch: 277, Loss: 0.011982211843132973\n",
            "Epoch: 278, Loss: 0.012531358748674393\n",
            "Epoch: 279, Loss: 0.012127934023737907\n",
            "Epoch: 280, Loss: 0.012621348723769188\n",
            "Epoch: 281, Loss: 0.011986764147877693\n",
            "Epoch: 282, Loss: 0.012524466030299664\n",
            "Epoch: 283, Loss: 0.012113457545638084\n",
            "Epoch: 284, Loss: 0.012612891383469105\n",
            "Epoch: 285, Loss: 0.012040381319820881\n",
            "Epoch: 286, Loss: 0.014415501616895199\n",
            "Epoch: 287, Loss: 0.012775326147675514\n",
            "Epoch: 288, Loss: 0.012937447987496853\n",
            "Epoch: 289, Loss: 0.01204606518149376\n",
            "Epoch: 290, Loss: 0.012652168981730938\n",
            "Epoch: 291, Loss: 0.012168889865279198\n",
            "Epoch: 292, Loss: 0.01267771702259779\n",
            "Epoch: 293, Loss: 0.01204129122197628\n",
            "Epoch: 294, Loss: 0.01254263799637556\n",
            "Epoch: 295, Loss: 0.012172752059996128\n",
            "Epoch: 296, Loss: 0.012620284222066402\n",
            "Epoch: 297, Loss: 0.012024082243442535\n",
            "Epoch: 298, Loss: 0.01253319438546896\n",
            "Epoch: 299, Loss: 0.012125887908041477\n",
            "Epoch: 300, Loss: 0.012600582093000412\n",
            "Epoch: 301, Loss: 0.01200798712670803\n",
            "Epoch: 302, Loss: 0.012493758462369442\n",
            "Epoch: 303, Loss: 0.012151159346103668\n",
            "Epoch: 304, Loss: 0.012592890299856663\n",
            "Epoch: 305, Loss: 0.012016125954687595\n",
            "Epoch: 306, Loss: 0.012476897798478603\n",
            "Epoch: 307, Loss: 0.012157976627349854\n",
            "Epoch: 308, Loss: 0.01258142665028572\n",
            "Epoch: 309, Loss: 0.012017110362648964\n",
            "Epoch: 310, Loss: 0.012511013075709343\n",
            "Epoch: 311, Loss: 0.012130293063819408\n",
            "Epoch: 312, Loss: 0.012602490372955799\n",
            "Epoch: 313, Loss: 0.012008416466414928\n",
            "Epoch: 314, Loss: 0.012507964856922626\n",
            "Epoch: 315, Loss: 0.012137298472225666\n",
            "Epoch: 316, Loss: 0.012584952637553215\n",
            "Epoch: 317, Loss: 0.012019881047308445\n",
            "Epoch: 318, Loss: 0.012466924265027046\n",
            "Epoch: 319, Loss: 0.012155727483332157\n",
            "Epoch: 320, Loss: 0.01258652750402689\n",
            "Epoch: 321, Loss: 0.011995656415820122\n",
            "Epoch: 322, Loss: 0.012495631352066994\n",
            "Epoch: 323, Loss: 0.012105684727430344\n",
            "Epoch: 324, Loss: 0.012565935961902142\n",
            "Epoch: 325, Loss: 0.011983556672930717\n",
            "Epoch: 326, Loss: 0.012492122128605843\n",
            "Epoch: 327, Loss: 0.012109261937439442\n",
            "Epoch: 328, Loss: 0.012583974748849869\n",
            "Epoch: 329, Loss: 0.011943052522838116\n",
            "Epoch: 330, Loss: 0.012469318695366383\n",
            "Epoch: 331, Loss: 0.012130098417401314\n",
            "Epoch: 332, Loss: 0.012578722089529037\n",
            "Epoch: 333, Loss: 0.01198879536241293\n",
            "Epoch: 334, Loss: 0.01869792304933071\n",
            "Epoch: 335, Loss: 0.01195553969591856\n",
            "Epoch: 336, Loss: 0.013071268796920776\n",
            "Epoch: 337, Loss: 0.011914119124412537\n",
            "Epoch: 338, Loss: 0.01262933574616909\n",
            "Epoch: 339, Loss: 0.012154439464211464\n",
            "Epoch: 340, Loss: 0.012637130916118622\n",
            "Epoch: 341, Loss: 0.012009515427052975\n",
            "Epoch: 342, Loss: 0.01255008950829506\n",
            "Epoch: 343, Loss: 0.01207827404141426\n",
            "Epoch: 344, Loss: 0.012578331865370274\n",
            "Epoch: 345, Loss: 0.012000230140984058\n",
            "Epoch: 346, Loss: 0.012491393834352493\n",
            "Epoch: 347, Loss: 0.012094241566956043\n",
            "Epoch: 348, Loss: 0.012564562261104584\n",
            "Epoch: 349, Loss: 0.0119851715862751\n",
            "Epoch: 350, Loss: 0.012479608878493309\n",
            "Epoch: 351, Loss: 0.012086396105587482\n",
            "Epoch: 352, Loss: 0.012559894472360611\n",
            "Epoch: 353, Loss: 0.011977518908679485\n",
            "Epoch: 354, Loss: 0.012464061379432678\n",
            "Epoch: 355, Loss: 0.012126905843615532\n",
            "Epoch: 356, Loss: 0.012555581517517567\n",
            "Epoch: 357, Loss: 0.011997377499938011\n",
            "Epoch: 358, Loss: 0.01247013732790947\n",
            "Epoch: 359, Loss: 0.01211232878267765\n",
            "Epoch: 360, Loss: 0.012563150376081467\n",
            "Epoch: 361, Loss: 0.01198481023311615\n",
            "Epoch: 362, Loss: 0.012466146610677242\n",
            "Epoch: 363, Loss: 0.012129631824791431\n",
            "Epoch: 364, Loss: 0.012587725184857845\n",
            "Epoch: 365, Loss: 0.01198881771415472\n",
            "Epoch: 366, Loss: 0.012482562102377415\n",
            "Epoch: 367, Loss: 0.012091616168618202\n",
            "Epoch: 368, Loss: 0.012569493614137173\n",
            "Epoch: 369, Loss: 0.01200092677026987\n",
            "Epoch: 370, Loss: 0.012475763447582722\n",
            "Epoch: 371, Loss: 0.01209836546331644\n",
            "Epoch: 372, Loss: 0.012579314410686493\n",
            "Epoch: 373, Loss: 0.01200401596724987\n",
            "Epoch: 374, Loss: 0.012460113503038883\n",
            "Epoch: 375, Loss: 0.012125903740525246\n",
            "Epoch: 376, Loss: 0.012556282803416252\n",
            "Epoch: 377, Loss: 0.011975475586950779\n",
            "Epoch: 378, Loss: 0.012467760592699051\n",
            "Epoch: 379, Loss: 0.012087464332580566\n",
            "Epoch: 380, Loss: 0.0125762103125453\n",
            "Epoch: 381, Loss: 0.011966973543167114\n",
            "Epoch: 382, Loss: 0.012430833652615547\n",
            "Epoch: 383, Loss: 0.012123542837798595\n",
            "Epoch: 384, Loss: 0.012533425353467464\n",
            "Epoch: 385, Loss: 0.011971352621912956\n",
            "Epoch: 386, Loss: 0.01241492759436369\n",
            "Epoch: 387, Loss: 0.012141333892941475\n",
            "Epoch: 388, Loss: 0.012525523081421852\n",
            "Epoch: 389, Loss: 0.011969557031989098\n",
            "Epoch: 390, Loss: 0.012494037859141827\n",
            "Epoch: 391, Loss: 0.012024126015603542\n",
            "Epoch: 392, Loss: 0.012521552853286266\n",
            "Epoch: 393, Loss: 0.011995747685432434\n",
            "Epoch: 394, Loss: 0.01243276335299015\n",
            "Epoch: 395, Loss: 0.01206304132938385\n",
            "Epoch: 396, Loss: 0.012530682608485222\n",
            "Epoch: 397, Loss: 0.011952388100326061\n",
            "Epoch: 398, Loss: 0.012420815415680408\n",
            "Epoch: 399, Loss: 0.01208663173019886\n",
            "Epoch: 400, Loss: 0.012518571689724922\n",
            "Epoch: 401, Loss: 0.011940344236791134\n",
            "Epoch: 402, Loss: 0.012411048635840416\n",
            "Epoch: 403, Loss: 0.012089588679373264\n",
            "Epoch: 404, Loss: 0.012515773996710777\n",
            "Epoch: 405, Loss: 0.011952073313295841\n",
            "Epoch: 406, Loss: 0.012475515715777874\n",
            "Epoch: 407, Loss: 0.012042495422065258\n",
            "Epoch: 408, Loss: 0.013110365718603134\n",
            "Epoch: 409, Loss: 0.016362804919481277\n",
            "Epoch: 410, Loss: 0.012699502520263195\n",
            "Epoch: 411, Loss: 0.012431983835995197\n",
            "Epoch: 412, Loss: 0.01257662195712328\n",
            "Epoch: 413, Loss: 0.01211029663681984\n",
            "Epoch: 414, Loss: 0.01254737377166748\n",
            "Epoch: 415, Loss: 0.012129382230341434\n",
            "Epoch: 416, Loss: 0.01260000467300415\n",
            "Epoch: 417, Loss: 0.012022295035421848\n",
            "Epoch: 418, Loss: 0.012518699280917645\n",
            "Epoch: 419, Loss: 0.012103968299925327\n",
            "Epoch: 420, Loss: 0.0125725781545043\n",
            "Epoch: 421, Loss: 0.011994672939181328\n",
            "Epoch: 422, Loss: 0.012496341951191425\n",
            "Epoch: 423, Loss: 0.012083746492862701\n",
            "Epoch: 424, Loss: 0.012555079534649849\n",
            "Epoch: 425, Loss: 0.011979647912085056\n",
            "Epoch: 426, Loss: 0.012469847686588764\n",
            "Epoch: 427, Loss: 0.012074635364115238\n",
            "Epoch: 428, Loss: 0.012547507882118225\n",
            "Epoch: 429, Loss: 0.01197371818125248\n",
            "Epoch: 430, Loss: 0.012467310763895512\n",
            "Epoch: 431, Loss: 0.012074747122824192\n",
            "Epoch: 432, Loss: 0.012554371729493141\n",
            "Epoch: 433, Loss: 0.011977285146713257\n",
            "Epoch: 434, Loss: 0.012462932616472244\n",
            "Epoch: 435, Loss: 0.01207063626497984\n",
            "Epoch: 436, Loss: 0.012538810260593891\n",
            "Epoch: 437, Loss: 0.01197919249534607\n",
            "Epoch: 438, Loss: 0.0124604981392622\n",
            "Epoch: 439, Loss: 0.012078885920345783\n",
            "Epoch: 440, Loss: 0.012556992471218109\n",
            "Epoch: 441, Loss: 0.011965380981564522\n",
            "Epoch: 442, Loss: 0.012442703358829021\n",
            "Epoch: 443, Loss: 0.012108409777283669\n",
            "Epoch: 444, Loss: 0.012561099603772163\n",
            "Epoch: 445, Loss: 0.011956675909459591\n",
            "Epoch: 446, Loss: 0.012452410534024239\n",
            "Epoch: 447, Loss: 0.012105411849915981\n",
            "Epoch: 448, Loss: 0.012551061809062958\n",
            "Epoch: 449, Loss: 0.01198292151093483\n",
            "Epoch: 450, Loss: 0.012449425645172596\n",
            "Epoch: 451, Loss: 0.012097757309675217\n",
            "Epoch: 452, Loss: 0.012541387230157852\n",
            "Epoch: 453, Loss: 0.011978017166256905\n",
            "Epoch: 454, Loss: 0.012457530945539474\n",
            "Epoch: 455, Loss: 0.012086412869393826\n",
            "Epoch: 456, Loss: 0.012516042217612267\n",
            "Epoch: 457, Loss: 0.011976604349911213\n",
            "Epoch: 458, Loss: 0.01243139710277319\n",
            "Epoch: 459, Loss: 0.012111700139939785\n",
            "Epoch: 460, Loss: 0.012523937039077282\n",
            "Epoch: 461, Loss: 0.011976526118814945\n",
            "Epoch: 462, Loss: 0.012464752420783043\n",
            "Epoch: 463, Loss: 0.012060332112014294\n",
            "Epoch: 464, Loss: 0.01251982618123293\n",
            "Epoch: 465, Loss: 0.011980220675468445\n",
            "Epoch: 466, Loss: 0.012442337349057198\n",
            "Epoch: 467, Loss: 0.012081054039299488\n",
            "Epoch: 468, Loss: 0.012563277035951614\n",
            "Epoch: 469, Loss: 0.015257423743605614\n",
            "Epoch: 470, Loss: 0.012768333777785301\n",
            "Epoch: 471, Loss: 0.012201685458421707\n",
            "Epoch: 472, Loss: 0.01277327723801136\n",
            "Epoch: 473, Loss: 0.012063770554959774\n",
            "Epoch: 474, Loss: 0.012559260241687298\n",
            "Epoch: 475, Loss: 0.01219505351036787\n",
            "Epoch: 476, Loss: 0.012621830217540264\n",
            "Epoch: 477, Loss: 0.012032092548906803\n",
            "Epoch: 478, Loss: 0.012560321018099785\n",
            "Epoch: 479, Loss: 0.012086998671293259\n",
            "Epoch: 480, Loss: 0.012578245252370834\n",
            "Epoch: 481, Loss: 0.012021204456686974\n",
            "Epoch: 482, Loss: 0.012518879026174545\n",
            "Epoch: 483, Loss: 0.012077385559678078\n",
            "Epoch: 484, Loss: 0.012561081908643246\n",
            "Epoch: 485, Loss: 0.01200883835554123\n",
            "Epoch: 486, Loss: 0.012502425350248814\n",
            "Epoch: 487, Loss: 0.01207641325891018\n",
            "Epoch: 488, Loss: 0.01257102470844984\n",
            "Epoch: 489, Loss: 0.011991376057267189\n",
            "Epoch: 490, Loss: 0.012485052458941936\n",
            "Epoch: 491, Loss: 0.012099228799343109\n",
            "Epoch: 492, Loss: 0.012559966184198856\n",
            "Epoch: 493, Loss: 0.011996570974588394\n",
            "Epoch: 494, Loss: 0.012501666322350502\n",
            "Epoch: 495, Loss: 0.012090911157429218\n",
            "Epoch: 496, Loss: 0.012584760785102844\n",
            "Epoch: 497, Loss: 0.011984541080892086\n",
            "Epoch: 498, Loss: 0.01249039638787508\n",
            "Epoch: 499, Loss: 0.0120873237028718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbDU9zALm3dv",
        "outputId": "65409040-e3c6-4300-8820-48ba83ff7292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.33010196685791\n",
            "Epoch: 1, Loss: 7.648670196533203\n",
            "Epoch: 2, Loss: 7.0389862060546875\n",
            "Epoch: 3, Loss: 6.382065773010254\n",
            "Epoch: 4, Loss: 5.5773234367370605\n",
            "Epoch: 5, Loss: 4.6474432945251465\n",
            "Epoch: 6, Loss: 3.8237690925598145\n",
            "Epoch: 7, Loss: 3.307467460632324\n",
            "Epoch: 8, Loss: 2.994114637374878\n",
            "Epoch: 9, Loss: 2.774240016937256\n",
            "Epoch: 10, Loss: 2.5978147983551025\n",
            "Epoch: 11, Loss: 2.4483330249786377\n",
            "Epoch: 12, Loss: 2.31909441947937\n",
            "Epoch: 13, Loss: 2.2066762447357178\n",
            "Epoch: 14, Loss: 2.1045875549316406\n",
            "Epoch: 15, Loss: 2.011168956756592\n",
            "Epoch: 16, Loss: 1.9244948625564575\n",
            "Epoch: 17, Loss: 1.8458614349365234\n",
            "Epoch: 18, Loss: 1.7756061553955078\n",
            "Epoch: 19, Loss: 1.7051494121551514\n",
            "Epoch: 20, Loss: 1.628291130065918\n",
            "Epoch: 21, Loss: 1.5640881061553955\n",
            "Epoch: 22, Loss: 1.4976122379302979\n",
            "Epoch: 23, Loss: 1.4354373216629028\n",
            "Epoch: 24, Loss: 1.3744069337844849\n",
            "Epoch: 25, Loss: 1.3176707029342651\n",
            "Epoch: 26, Loss: 1.2611401081085205\n",
            "Epoch: 27, Loss: 1.2087738513946533\n",
            "Epoch: 28, Loss: 1.1569607257843018\n",
            "Epoch: 29, Loss: 1.1064231395721436\n",
            "Epoch: 30, Loss: 1.0576854944229126\n",
            "Epoch: 31, Loss: 1.0110067129135132\n",
            "Epoch: 32, Loss: 0.9650568962097168\n",
            "Epoch: 33, Loss: 0.9209648370742798\n",
            "Epoch: 34, Loss: 0.8778860569000244\n",
            "Epoch: 35, Loss: 0.8363565802574158\n",
            "Epoch: 36, Loss: 0.7963799834251404\n",
            "Epoch: 37, Loss: 0.7628870010375977\n",
            "Epoch: 38, Loss: 0.7343594431877136\n",
            "Epoch: 39, Loss: 0.6907874345779419\n",
            "Epoch: 40, Loss: 0.650229811668396\n",
            "Epoch: 41, Loss: 0.6161499619483948\n",
            "Epoch: 42, Loss: 0.584172248840332\n",
            "Epoch: 43, Loss: 0.5548942685127258\n",
            "Epoch: 44, Loss: 0.5276581048965454\n",
            "Epoch: 45, Loss: 0.5017353296279907\n",
            "Epoch: 46, Loss: 0.47760504484176636\n",
            "Epoch: 47, Loss: 0.4544084370136261\n",
            "Epoch: 48, Loss: 0.433681845664978\n",
            "Epoch: 49, Loss: 0.41271811723709106\n",
            "Epoch: 50, Loss: 0.39294224977493286\n",
            "Epoch: 51, Loss: 0.3744852840900421\n",
            "Epoch: 52, Loss: 0.3565271496772766\n",
            "Epoch: 53, Loss: 0.3405746817588806\n",
            "Epoch: 54, Loss: 0.32327568531036377\n",
            "Epoch: 55, Loss: 0.30826660990715027\n",
            "Epoch: 56, Loss: 0.2933405935764313\n",
            "Epoch: 57, Loss: 0.2795799970626831\n",
            "Epoch: 58, Loss: 0.26579269766807556\n",
            "Epoch: 59, Loss: 0.25360748171806335\n",
            "Epoch: 60, Loss: 0.2402794361114502\n",
            "Epoch: 61, Loss: 0.23068572580814362\n",
            "Epoch: 62, Loss: 0.21831098198890686\n",
            "Epoch: 63, Loss: 0.20912279188632965\n",
            "Epoch: 64, Loss: 0.19803506135940552\n",
            "Epoch: 65, Loss: 0.19102364778518677\n",
            "Epoch: 66, Loss: 0.1793949156999588\n",
            "Epoch: 67, Loss: 0.17509573698043823\n",
            "Epoch: 68, Loss: 0.1633705496788025\n",
            "Epoch: 69, Loss: 0.17852193117141724\n",
            "Epoch: 70, Loss: 0.1596248298883438\n",
            "Epoch: 71, Loss: 0.15438400208950043\n",
            "Epoch: 72, Loss: 0.13479825854301453\n",
            "Epoch: 73, Loss: 0.1336534321308136\n",
            "Epoch: 74, Loss: 0.12294012308120728\n",
            "Epoch: 75, Loss: 0.12126672267913818\n",
            "Epoch: 76, Loss: 0.11164475977420807\n",
            "Epoch: 77, Loss: 0.1114947646856308\n",
            "Epoch: 78, Loss: 0.10165177285671234\n",
            "Epoch: 79, Loss: 0.10275115072727203\n",
            "Epoch: 80, Loss: 0.0931839793920517\n",
            "Epoch: 81, Loss: 0.09304869920015335\n",
            "Epoch: 82, Loss: 0.08553431183099747\n",
            "Epoch: 83, Loss: 0.08876585960388184\n",
            "Epoch: 84, Loss: 0.07765685766935349\n",
            "Epoch: 85, Loss: 0.07812714576721191\n",
            "Epoch: 86, Loss: 0.07156737893819809\n",
            "Epoch: 87, Loss: 0.07577043771743774\n",
            "Epoch: 88, Loss: 0.06524108350276947\n",
            "Epoch: 89, Loss: 0.06681401282548904\n",
            "Epoch: 90, Loss: 0.060742445290088654\n",
            "Epoch: 91, Loss: 0.06749145686626434\n",
            "Epoch: 92, Loss: 0.056614674627780914\n",
            "Epoch: 93, Loss: 0.05618343874812126\n",
            "Epoch: 94, Loss: 0.0521945059299469\n",
            "Epoch: 95, Loss: 0.057603321969509125\n",
            "Epoch: 96, Loss: 0.049132850021123886\n",
            "Epoch: 97, Loss: 0.052434228360652924\n",
            "Epoch: 98, Loss: 0.046226292848587036\n",
            "Epoch: 99, Loss: 0.05476173385977745\n",
            "Epoch: 100, Loss: 0.044020358473062515\n",
            "Epoch: 101, Loss: 0.04404677450656891\n",
            "Epoch: 102, Loss: 0.041272107511758804\n",
            "Epoch: 103, Loss: 0.05430902913212776\n",
            "Epoch: 104, Loss: 0.040027398616075516\n",
            "Epoch: 105, Loss: 0.03788404166698456\n",
            "Epoch: 106, Loss: 0.036357469856739044\n",
            "Epoch: 107, Loss: 0.03619854897260666\n",
            "Epoch: 108, Loss: 0.03428425267338753\n",
            "Epoch: 109, Loss: 0.03439311310648918\n",
            "Epoch: 110, Loss: 0.03223996236920357\n",
            "Epoch: 111, Loss: 0.03297068923711777\n",
            "Epoch: 112, Loss: 0.030898716300725937\n",
            "Epoch: 113, Loss: 0.04509644955396652\n",
            "Epoch: 114, Loss: 0.036269087344408035\n",
            "Epoch: 115, Loss: 0.05940627306699753\n",
            "Epoch: 116, Loss: 0.029531128704547882\n",
            "Epoch: 117, Loss: 0.029086992144584656\n",
            "Epoch: 118, Loss: 0.027843279764056206\n",
            "Epoch: 119, Loss: 0.04608151689171791\n",
            "Epoch: 120, Loss: 0.029001977294683456\n",
            "Epoch: 121, Loss: 0.028107186779379845\n",
            "Epoch: 122, Loss: 0.02672988548874855\n",
            "Epoch: 123, Loss: 0.027955153957009315\n",
            "Epoch: 124, Loss: 0.024271804839372635\n",
            "Epoch: 125, Loss: 0.03986122086644173\n",
            "Epoch: 126, Loss: 0.024019217118620872\n",
            "Epoch: 127, Loss: 0.026507098227739334\n",
            "Epoch: 128, Loss: 0.023187587037682533\n",
            "Epoch: 129, Loss: 0.04279421642422676\n",
            "Epoch: 130, Loss: 0.02364426665008068\n",
            "Epoch: 131, Loss: 0.0253605917096138\n",
            "Epoch: 132, Loss: 0.02188071422278881\n",
            "Epoch: 133, Loss: 0.03727947548031807\n",
            "Epoch: 134, Loss: 0.021300559863448143\n",
            "Epoch: 135, Loss: 0.022928792983293533\n",
            "Epoch: 136, Loss: 0.020368043333292007\n",
            "Epoch: 137, Loss: 0.04457605257630348\n",
            "Epoch: 138, Loss: 0.026406191289424896\n",
            "Epoch: 139, Loss: 0.02227693982422352\n",
            "Epoch: 140, Loss: 0.02041342854499817\n",
            "Epoch: 141, Loss: 0.021301593631505966\n",
            "Epoch: 142, Loss: 0.017838051542639732\n",
            "Epoch: 143, Loss: 0.03818197175860405\n",
            "Epoch: 144, Loss: 0.019572356715798378\n",
            "Epoch: 145, Loss: 0.027107873931527138\n",
            "Epoch: 146, Loss: 0.020480327308177948\n",
            "Epoch: 147, Loss: 0.03880888968706131\n",
            "Epoch: 148, Loss: 0.02916533872485161\n",
            "Epoch: 149, Loss: 0.01594666764140129\n",
            "Epoch: 150, Loss: 0.027759699150919914\n",
            "Epoch: 151, Loss: 0.020778056234121323\n",
            "Epoch: 152, Loss: 0.02485862374305725\n",
            "Epoch: 153, Loss: 0.04446658864617348\n",
            "Epoch: 154, Loss: 0.02565012313425541\n",
            "Epoch: 155, Loss: 0.02225915901362896\n",
            "Epoch: 156, Loss: 0.023472240194678307\n",
            "Epoch: 157, Loss: 0.03769446909427643\n",
            "Epoch: 158, Loss: 0.01959201693534851\n",
            "Epoch: 159, Loss: 0.01886957883834839\n",
            "Epoch: 160, Loss: 0.0319281630218029\n",
            "Epoch: 161, Loss: 0.028477398678660393\n",
            "Epoch: 162, Loss: 0.012204216793179512\n",
            "Epoch: 163, Loss: 0.018206486478447914\n",
            "Epoch: 164, Loss: 0.014126896858215332\n",
            "Epoch: 165, Loss: 0.06969113647937775\n",
            "Epoch: 166, Loss: 0.02839789167046547\n",
            "Epoch: 167, Loss: 0.02305402234196663\n",
            "Epoch: 168, Loss: 0.02514123171567917\n",
            "Epoch: 169, Loss: 0.040981680154800415\n",
            "Epoch: 170, Loss: 0.025378454476594925\n",
            "Epoch: 171, Loss: 0.018263543024659157\n",
            "Epoch: 172, Loss: 0.029718520119786263\n",
            "Epoch: 173, Loss: 0.04054344445466995\n",
            "Epoch: 174, Loss: 0.027655549347400665\n",
            "Epoch: 175, Loss: 0.02942590042948723\n",
            "Epoch: 176, Loss: 0.01545059122145176\n",
            "Epoch: 177, Loss: 0.04417158663272858\n",
            "Epoch: 178, Loss: 0.01955023594200611\n",
            "Epoch: 179, Loss: 0.0268274936825037\n",
            "Epoch: 180, Loss: 0.009945264086127281\n",
            "Epoch: 181, Loss: 0.035764604806900024\n",
            "Epoch: 182, Loss: 0.017334699630737305\n",
            "Epoch: 183, Loss: 0.023579882457852364\n",
            "Epoch: 184, Loss: 0.010167117230594158\n",
            "Epoch: 185, Loss: 0.032139867544174194\n",
            "Epoch: 186, Loss: 0.01866893842816353\n",
            "Epoch: 187, Loss: 0.02247529849410057\n",
            "Epoch: 188, Loss: 0.026942962780594826\n",
            "Epoch: 189, Loss: 0.04474738612771034\n",
            "Epoch: 190, Loss: 0.024222372099757195\n",
            "Epoch: 191, Loss: 0.012072097510099411\n",
            "Epoch: 192, Loss: 0.008433072827756405\n",
            "Epoch: 193, Loss: 0.03514253720641136\n",
            "Epoch: 194, Loss: 0.018155304715037346\n",
            "Epoch: 195, Loss: 0.02832099422812462\n",
            "Epoch: 196, Loss: 0.012799981981515884\n",
            "Epoch: 197, Loss: 0.04282664507627487\n",
            "Epoch: 198, Loss: 0.031415607780218124\n",
            "Epoch: 199, Loss: 0.01482565701007843\n",
            "Epoch: 200, Loss: 0.016718586906790733\n",
            "Epoch: 201, Loss: 0.030120735988020897\n",
            "Epoch: 202, Loss: 0.013613106682896614\n",
            "Epoch: 203, Loss: 0.0371338427066803\n",
            "Epoch: 204, Loss: 0.025620276108384132\n",
            "Epoch: 205, Loss: 0.008847096003592014\n",
            "Epoch: 206, Loss: 0.0476691909134388\n",
            "Epoch: 207, Loss: 0.034224770963191986\n",
            "Epoch: 208, Loss: 0.03399067372083664\n",
            "Epoch: 209, Loss: 0.027218956500291824\n",
            "Epoch: 210, Loss: 0.01211727224290371\n",
            "Epoch: 211, Loss: 0.02018824592232704\n",
            "Epoch: 212, Loss: 0.03138931095600128\n",
            "Epoch: 213, Loss: 0.01839323528110981\n",
            "Epoch: 214, Loss: 0.02252299152314663\n",
            "Epoch: 215, Loss: 0.01155002135783434\n",
            "Epoch: 216, Loss: 0.02164502814412117\n",
            "Epoch: 217, Loss: 0.009333400055766106\n",
            "Epoch: 218, Loss: 0.022642379626631737\n",
            "Epoch: 219, Loss: 0.019613297656178474\n",
            "Epoch: 220, Loss: 0.02800801768898964\n",
            "Epoch: 221, Loss: 0.016963543370366096\n",
            "Epoch: 222, Loss: 0.017649555578827858\n",
            "Epoch: 223, Loss: 0.010267535224556923\n",
            "Epoch: 224, Loss: 0.038436293601989746\n",
            "Epoch: 225, Loss: 0.020602138713002205\n",
            "Epoch: 226, Loss: 0.031231524422764778\n",
            "Epoch: 227, Loss: 0.015074999071657658\n",
            "Epoch: 228, Loss: 0.02332943119108677\n",
            "Epoch: 229, Loss: 0.01452164351940155\n",
            "Epoch: 230, Loss: 0.01780427247285843\n",
            "Epoch: 231, Loss: 0.03941158577799797\n",
            "Epoch: 232, Loss: 0.03480774536728859\n",
            "Epoch: 233, Loss: 0.01577822118997574\n",
            "Epoch: 234, Loss: 0.021462468430399895\n",
            "Epoch: 235, Loss: 0.01750596985220909\n",
            "Epoch: 236, Loss: 0.010009653866291046\n",
            "Epoch: 237, Loss: 0.03625458851456642\n",
            "Epoch: 238, Loss: 0.016466500237584114\n",
            "Epoch: 239, Loss: 0.025994986295700073\n",
            "Epoch: 240, Loss: 0.012334153056144714\n",
            "Epoch: 241, Loss: 0.018039731308817863\n",
            "Epoch: 242, Loss: 0.029560118913650513\n",
            "Epoch: 243, Loss: 0.02686874382197857\n",
            "Epoch: 244, Loss: 0.01910986378788948\n",
            "Epoch: 245, Loss: 0.017034245654940605\n",
            "Epoch: 246, Loss: 0.010925565846264362\n",
            "Epoch: 247, Loss: 0.0156400203704834\n",
            "Epoch: 248, Loss: 0.010861437767744064\n",
            "Epoch: 249, Loss: 0.023256508633494377\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 1]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size = 5  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 64\n",
        "activation = 'relu'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(250):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_1': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'trained_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 1]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size = 10  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 128\n",
        "activation = 'tanh'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(500):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_8': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'trained_model_8.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmrZUxGn2A2z",
        "outputId": "ff7637aa-f152-482f-f354-659b0ca765fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.473753929138184\n",
            "Epoch: 1, Loss: 7.503234386444092\n",
            "Epoch: 2, Loss: 6.790429592132568\n",
            "Epoch: 3, Loss: 6.200444221496582\n",
            "Epoch: 4, Loss: 5.6689348220825195\n",
            "Epoch: 5, Loss: 5.184218883514404\n",
            "Epoch: 6, Loss: 4.733572006225586\n",
            "Epoch: 7, Loss: 4.307787895202637\n",
            "Epoch: 8, Loss: 3.907140016555786\n",
            "Epoch: 9, Loss: 3.52960467338562\n",
            "Epoch: 10, Loss: 3.1754047870635986\n",
            "Epoch: 11, Loss: 2.8446905612945557\n",
            "Epoch: 12, Loss: 2.537773370742798\n",
            "Epoch: 13, Loss: 2.256553888320923\n",
            "Epoch: 14, Loss: 2.008148193359375\n",
            "Epoch: 15, Loss: 1.7891106605529785\n",
            "Epoch: 16, Loss: 1.5976574420928955\n",
            "Epoch: 17, Loss: 1.4286812543869019\n",
            "Epoch: 18, Loss: 1.2766282558441162\n",
            "Epoch: 19, Loss: 1.1400991678237915\n",
            "Epoch: 20, Loss: 1.0181150436401367\n",
            "Epoch: 21, Loss: 0.9084488749504089\n",
            "Epoch: 22, Loss: 0.8096156716346741\n",
            "Epoch: 23, Loss: 0.7212905287742615\n",
            "Epoch: 24, Loss: 0.6422297358512878\n",
            "Epoch: 25, Loss: 0.5720602869987488\n",
            "Epoch: 26, Loss: 0.5097741484642029\n",
            "Epoch: 27, Loss: 0.45409098267555237\n",
            "Epoch: 28, Loss: 0.40376725792884827\n",
            "Epoch: 29, Loss: 0.3580939471721649\n",
            "Epoch: 30, Loss: 0.31816360354423523\n",
            "Epoch: 31, Loss: 0.2842663824558258\n",
            "Epoch: 32, Loss: 0.25490322709083557\n",
            "Epoch: 33, Loss: 0.22918392717838287\n",
            "Epoch: 34, Loss: 0.20670315623283386\n",
            "Epoch: 35, Loss: 0.18634717166423798\n",
            "Epoch: 36, Loss: 0.1681540459394455\n",
            "Epoch: 37, Loss: 0.1513533741235733\n",
            "Epoch: 38, Loss: 0.13664531707763672\n",
            "Epoch: 39, Loss: 0.12257414311170578\n",
            "Epoch: 40, Loss: 0.11103324592113495\n",
            "Epoch: 41, Loss: 0.0997534841299057\n",
            "Epoch: 42, Loss: 0.09082143753767014\n",
            "Epoch: 43, Loss: 0.08222052454948425\n",
            "Epoch: 44, Loss: 0.07486102730035782\n",
            "Epoch: 45, Loss: 0.06804905086755753\n",
            "Epoch: 46, Loss: 0.06151152402162552\n",
            "Epoch: 47, Loss: 0.055255863815546036\n",
            "Epoch: 48, Loss: 0.04990086331963539\n",
            "Epoch: 49, Loss: 0.04514891654253006\n",
            "Epoch: 50, Loss: 0.040890172123909\n",
            "Epoch: 51, Loss: 0.03670741245150566\n",
            "Epoch: 52, Loss: 0.03243013843894005\n",
            "Epoch: 53, Loss: 0.02932511642575264\n",
            "Epoch: 54, Loss: 0.02668868377804756\n",
            "Epoch: 55, Loss: 0.024205874651670456\n",
            "Epoch: 56, Loss: 0.02182558737695217\n",
            "Epoch: 57, Loss: 0.019906066358089447\n",
            "Epoch: 58, Loss: 0.018207300454378128\n",
            "Epoch: 59, Loss: 0.016623584553599358\n",
            "Epoch: 60, Loss: 0.015149914659559727\n",
            "Epoch: 61, Loss: 0.013737334869801998\n",
            "Epoch: 62, Loss: 0.01238507404923439\n",
            "Epoch: 63, Loss: 0.011146827600896358\n",
            "Epoch: 64, Loss: 0.009961307048797607\n",
            "Epoch: 65, Loss: 0.00898458156734705\n",
            "Epoch: 66, Loss: 0.00821478758007288\n",
            "Epoch: 67, Loss: 0.007533610798418522\n",
            "Epoch: 68, Loss: 0.006891596131026745\n",
            "Epoch: 69, Loss: 0.006318607833236456\n",
            "Epoch: 70, Loss: 0.0057835765182971954\n",
            "Epoch: 71, Loss: 0.0052533866837620735\n",
            "Epoch: 72, Loss: 0.004749462008476257\n",
            "Epoch: 73, Loss: 0.004273881670087576\n",
            "Epoch: 74, Loss: 0.003895797301083803\n",
            "Epoch: 75, Loss: 0.003555428236722946\n",
            "Epoch: 76, Loss: 0.003242349484935403\n",
            "Epoch: 77, Loss: 0.0029597789980471134\n",
            "Epoch: 78, Loss: 0.002702624537050724\n",
            "Epoch: 79, Loss: 0.002474879613146186\n",
            "Epoch: 80, Loss: 0.0022703525610268116\n",
            "Epoch: 81, Loss: 0.0020749301183968782\n",
            "Epoch: 82, Loss: 0.0019039757316932082\n",
            "Epoch: 83, Loss: 0.0017402158118784428\n",
            "Epoch: 84, Loss: 0.0015986344078555703\n",
            "Epoch: 85, Loss: 0.0014602813171222806\n",
            "Epoch: 86, Loss: 0.0013415927533060312\n",
            "Epoch: 87, Loss: 0.0012262348318472505\n",
            "Epoch: 88, Loss: 0.0011266525834798813\n",
            "Epoch: 89, Loss: 0.0010318782879039645\n",
            "Epoch: 90, Loss: 0.000948539178352803\n",
            "Epoch: 91, Loss: 0.0008690636022947729\n",
            "Epoch: 92, Loss: 0.0007990319281816483\n",
            "Epoch: 93, Loss: 0.0007321307784877717\n",
            "Epoch: 94, Loss: 0.0006714450428262353\n",
            "Epoch: 95, Loss: 0.0006142673082649708\n",
            "Epoch: 96, Loss: 0.000563772046007216\n",
            "Epoch: 97, Loss: 0.0005176676204428077\n",
            "Epoch: 98, Loss: 0.0004756089474540204\n",
            "Epoch: 99, Loss: 0.00043575430754572153\n",
            "Epoch: 100, Loss: 0.00039971445221453905\n",
            "Epoch: 101, Loss: 0.00036742869997397065\n",
            "Epoch: 102, Loss: 0.00033849640749394894\n",
            "Epoch: 103, Loss: 0.00031168138957582414\n",
            "Epoch: 104, Loss: 0.00028786074835807085\n",
            "Epoch: 105, Loss: 0.00026495580095797777\n",
            "Epoch: 106, Loss: 0.00024490326177328825\n",
            "Epoch: 107, Loss: 0.000225702635361813\n",
            "Epoch: 108, Loss: 0.00020881652017123997\n",
            "Epoch: 109, Loss: 0.0001928939309436828\n",
            "Epoch: 110, Loss: 0.00017792618018575013\n",
            "Epoch: 111, Loss: 0.00016448178212158382\n",
            "Epoch: 112, Loss: 0.00015160892507992685\n",
            "Epoch: 113, Loss: 0.00014025250857230276\n",
            "Epoch: 114, Loss: 0.00012934816186316311\n",
            "Epoch: 115, Loss: 0.00011989469930995256\n",
            "Epoch: 116, Loss: 0.00011076116061303765\n",
            "Epoch: 117, Loss: 0.00010271307110087946\n",
            "Epoch: 118, Loss: 9.48811648413539e-05\n",
            "Epoch: 119, Loss: 8.782725490164012e-05\n",
            "Epoch: 120, Loss: 8.172606612788513e-05\n",
            "Epoch: 121, Loss: 7.528282003477216e-05\n",
            "Epoch: 122, Loss: 6.971060065552592e-05\n",
            "Epoch: 123, Loss: 6.456785195041448e-05\n",
            "Epoch: 124, Loss: 5.9940462961094454e-05\n",
            "Epoch: 125, Loss: 5.590144792222418e-05\n",
            "Epoch: 126, Loss: 5.162576417205855e-05\n",
            "Epoch: 127, Loss: 4.8068512114696205e-05\n",
            "Epoch: 128, Loss: 4.489692219067365e-05\n",
            "Epoch: 129, Loss: 4.1892752051353455e-05\n",
            "Epoch: 130, Loss: 3.901543823303655e-05\n",
            "Epoch: 131, Loss: 3.6429817555472255e-05\n",
            "Epoch: 132, Loss: 3.379589907126501e-05\n",
            "Epoch: 133, Loss: 3.1307754397857934e-05\n",
            "Epoch: 134, Loss: 2.9053977414150722e-05\n",
            "Epoch: 135, Loss: 2.6676705601857975e-05\n",
            "Epoch: 136, Loss: 2.4959475922514684e-05\n",
            "Epoch: 137, Loss: 2.338995545869693e-05\n",
            "Epoch: 138, Loss: 2.2692716811434366e-05\n",
            "Epoch: 139, Loss: 2.0940382455592044e-05\n",
            "Epoch: 140, Loss: 1.941132541105617e-05\n",
            "Epoch: 141, Loss: 1.793497722246684e-05\n",
            "Epoch: 142, Loss: 1.689554483164102e-05\n",
            "Epoch: 143, Loss: 1.5904272004263476e-05\n",
            "Epoch: 144, Loss: 1.5088690815900918e-05\n",
            "Epoch: 145, Loss: 1.4149038179311901e-05\n",
            "Epoch: 146, Loss: 1.3493715414369944e-05\n",
            "Epoch: 147, Loss: 1.2629631783056539e-05\n",
            "Epoch: 148, Loss: 1.1955473382840864e-05\n",
            "Epoch: 149, Loss: 1.136800528911408e-05\n",
            "Epoch: 150, Loss: 1.0545268196437974e-05\n",
            "Epoch: 151, Loss: 1.0013980499934405e-05\n",
            "Epoch: 152, Loss: 9.491243872616906e-06\n",
            "Epoch: 153, Loss: 8.992752555059269e-06\n",
            "Epoch: 154, Loss: 8.598917702329345e-06\n",
            "Epoch: 155, Loss: 8.104416338028386e-06\n",
            "Epoch: 156, Loss: 8.514226465194952e-06\n",
            "Epoch: 157, Loss: 0.050304170697927475\n",
            "Epoch: 158, Loss: 0.002162887714803219\n",
            "Epoch: 159, Loss: 0.00033085429458878934\n",
            "Epoch: 160, Loss: 0.00026415762840770185\n",
            "Epoch: 161, Loss: 0.000227031487156637\n",
            "Epoch: 162, Loss: 0.0002015010395552963\n",
            "Epoch: 163, Loss: 0.00018224946688860655\n",
            "Epoch: 164, Loss: 0.00016689793847035617\n",
            "Epoch: 165, Loss: 0.0001541896053822711\n",
            "Epoch: 166, Loss: 0.00014336933963932097\n",
            "Epoch: 167, Loss: 0.00013397418661043048\n",
            "Epoch: 168, Loss: 0.00012568036618176848\n",
            "Epoch: 169, Loss: 0.00011826748959720135\n",
            "Epoch: 170, Loss: 0.00011156666005263105\n",
            "Epoch: 171, Loss: 0.00010547019337536767\n",
            "Epoch: 172, Loss: 9.986528311856091e-05\n",
            "Epoch: 173, Loss: 9.471381781622767e-05\n",
            "Epoch: 174, Loss: 8.995903772301972e-05\n",
            "Epoch: 175, Loss: 8.552691724617034e-05\n",
            "Epoch: 176, Loss: 8.140801946865395e-05\n",
            "Epoch: 177, Loss: 7.756101695122197e-05\n",
            "Epoch: 178, Loss: 7.394464046228677e-05\n",
            "Epoch: 179, Loss: 7.054546586005017e-05\n",
            "Epoch: 180, Loss: 6.72588575980626e-05\n",
            "Epoch: 181, Loss: 6.43400417175144e-05\n",
            "Epoch: 182, Loss: 6.13364900345914e-05\n",
            "Epoch: 183, Loss: 5.859238444827497e-05\n",
            "Epoch: 184, Loss: 5.603072349913418e-05\n",
            "Epoch: 185, Loss: 5.359679562388919e-05\n",
            "Epoch: 186, Loss: 5.128430711920373e-05\n",
            "Epoch: 187, Loss: 4.9066766223404557e-05\n",
            "Epoch: 188, Loss: 4.696182077168487e-05\n",
            "Epoch: 189, Loss: 4.4949265429750085e-05\n",
            "Epoch: 190, Loss: 4.3010244553443044e-05\n",
            "Epoch: 191, Loss: 4.1175597289111465e-05\n",
            "Epoch: 192, Loss: 3.93999507650733e-05\n",
            "Epoch: 193, Loss: 3.771326009882614e-05\n",
            "Epoch: 194, Loss: 3.610041312640533e-05\n",
            "Epoch: 195, Loss: 3.456883496255614e-05\n",
            "Epoch: 196, Loss: 3.307944643893279e-05\n",
            "Epoch: 197, Loss: 3.168158946209587e-05\n",
            "Epoch: 198, Loss: 3.0313092793221585e-05\n",
            "Epoch: 199, Loss: 2.9032424208708107e-05\n",
            "Epoch: 200, Loss: 2.777941153908614e-05\n",
            "Epoch: 201, Loss: 2.6600819182931446e-05\n",
            "Epoch: 202, Loss: 2.545786992413923e-05\n",
            "Epoch: 203, Loss: 2.4375940483878367e-05\n",
            "Epoch: 204, Loss: 2.3325947040575556e-05\n",
            "Epoch: 205, Loss: 2.2339825591188855e-05\n",
            "Epoch: 206, Loss: 2.139648495358415e-05\n",
            "Epoch: 207, Loss: 2.050075818260666e-05\n",
            "Epoch: 208, Loss: 1.9624423657660373e-05\n",
            "Epoch: 209, Loss: 1.8796279618982226e-05\n",
            "Epoch: 210, Loss: 1.7958729586098343e-05\n",
            "Epoch: 211, Loss: 1.7238373402506113e-05\n",
            "Epoch: 212, Loss: 1.6467263776576146e-05\n",
            "Epoch: 213, Loss: 1.578340925334487e-05\n",
            "Epoch: 214, Loss: 1.508530112914741e-05\n",
            "Epoch: 215, Loss: 1.4453634321398567e-05\n",
            "Epoch: 216, Loss: 1.3824250345351174e-05\n",
            "Epoch: 217, Loss: 1.3240206499176566e-05\n",
            "Epoch: 218, Loss: 1.2674702702497598e-05\n",
            "Epoch: 219, Loss: 1.2132294614275452e-05\n",
            "Epoch: 220, Loss: 1.1618971257121302e-05\n",
            "Epoch: 221, Loss: 1.1139872185594868e-05\n",
            "Epoch: 222, Loss: 1.066305412678048e-05\n",
            "Epoch: 223, Loss: 1.0195357390330173e-05\n",
            "Epoch: 224, Loss: 9.754758139024489e-06\n",
            "Epoch: 225, Loss: 9.354648682347033e-06\n",
            "Epoch: 226, Loss: 8.974220691015944e-06\n",
            "Epoch: 227, Loss: 8.58665953273885e-06\n",
            "Epoch: 228, Loss: 8.363649612874724e-06\n",
            "Epoch: 229, Loss: 7.954982720548287e-06\n",
            "Epoch: 230, Loss: 7.956678018672392e-06\n",
            "Epoch: 231, Loss: 7.34098648536019e-06\n",
            "Epoch: 232, Loss: 7.016732070042053e-06\n",
            "Epoch: 233, Loss: 6.7540781856223475e-06\n",
            "Epoch: 234, Loss: 6.446935913118068e-06\n",
            "Epoch: 235, Loss: 6.2224926296039484e-06\n",
            "Epoch: 236, Loss: 5.990638783259783e-06\n",
            "Epoch: 237, Loss: 5.9048024922958575e-06\n",
            "Epoch: 238, Loss: 1.1437026842031628e-05\n",
            "Epoch: 239, Loss: 0.035898357629776\n",
            "Epoch: 240, Loss: 0.0003967668453697115\n",
            "Epoch: 241, Loss: 0.0003493837721180171\n",
            "Epoch: 242, Loss: 0.00021381043188739568\n",
            "Epoch: 243, Loss: 0.0001718232233542949\n",
            "Epoch: 244, Loss: 0.0001501047081546858\n",
            "Epoch: 245, Loss: 0.00013402642798610032\n",
            "Epoch: 246, Loss: 0.00012128066009609029\n",
            "Epoch: 247, Loss: 0.00011078239185735583\n",
            "Epoch: 248, Loss: 0.00010192899935645983\n",
            "Epoch: 249, Loss: 9.432251681573689e-05\n",
            "Epoch: 250, Loss: 8.769625128479674e-05\n",
            "Epoch: 251, Loss: 8.184573380276561e-05\n",
            "Epoch: 252, Loss: 7.664997974643484e-05\n",
            "Epoch: 253, Loss: 7.198791718110442e-05\n",
            "Epoch: 254, Loss: 6.778239912819117e-05\n",
            "Epoch: 255, Loss: 6.396188837243244e-05\n",
            "Epoch: 256, Loss: 6.0487094742711633e-05\n",
            "Epoch: 257, Loss: 5.729418626287952e-05\n",
            "Epoch: 258, Loss: 5.43629503226839e-05\n",
            "Epoch: 259, Loss: 5.164948379388079e-05\n",
            "Epoch: 260, Loss: 4.913327575195581e-05\n",
            "Epoch: 261, Loss: 4.683745282818563e-05\n",
            "Epoch: 262, Loss: 4.4615138904191554e-05\n",
            "Epoch: 263, Loss: 4.2611216485966e-05\n",
            "Epoch: 264, Loss: 4.0699942474020645e-05\n",
            "Epoch: 265, Loss: 3.8954036426730454e-05\n",
            "Epoch: 266, Loss: 3.7269983295118436e-05\n",
            "Epoch: 267, Loss: 3.571851266315207e-05\n",
            "Epoch: 268, Loss: 3.42326020472683e-05\n",
            "Epoch: 269, Loss: 3.2854481105459854e-05\n",
            "Epoch: 270, Loss: 3.15362231049221e-05\n",
            "Epoch: 271, Loss: 3.0304352549137548e-05\n",
            "Epoch: 272, Loss: 2.911724914156366e-05\n",
            "Epoch: 273, Loss: 2.801996197376866e-05\n",
            "Epoch: 274, Loss: 2.6946052457788028e-05\n",
            "Epoch: 275, Loss: 2.594313809822779e-05\n",
            "Epoch: 276, Loss: 2.4968167053884827e-05\n",
            "Epoch: 277, Loss: 2.4046805265243165e-05\n",
            "Epoch: 278, Loss: 2.315223537152633e-05\n",
            "Epoch: 279, Loss: 2.2318125047604553e-05\n",
            "Epoch: 280, Loss: 2.1484866010723636e-05\n",
            "Epoch: 281, Loss: 2.0719757230835967e-05\n",
            "Epoch: 282, Loss: 1.9954646631958894e-05\n",
            "Epoch: 283, Loss: 1.9274228179710917e-05\n",
            "Epoch: 284, Loss: 1.8570426618680358e-05\n",
            "Epoch: 285, Loss: 1.789541965990793e-05\n",
            "Epoch: 286, Loss: 1.7246084098587744e-05\n",
            "Epoch: 287, Loss: 1.66306781466119e-05\n",
            "Epoch: 288, Loss: 1.603010059625376e-05\n",
            "Epoch: 289, Loss: 1.5453475498361513e-05\n",
            "Epoch: 290, Loss: 1.4886831195326522e-05\n",
            "Epoch: 291, Loss: 1.4355550774780568e-05\n",
            "Epoch: 292, Loss: 1.383196831739042e-05\n",
            "Epoch: 293, Loss: 1.3326633052201942e-05\n",
            "Epoch: 294, Loss: 1.284525296796346e-05\n",
            "Epoch: 295, Loss: 1.2348757991276216e-05\n",
            "Epoch: 296, Loss: 1.1897894182766322e-05\n",
            "Epoch: 297, Loss: 1.1434764019213617e-05\n",
            "Epoch: 298, Loss: 1.1073157111241017e-05\n",
            "Epoch: 299, Loss: 1.0635690159688238e-05\n",
            "Epoch: 300, Loss: 1.0204788850387558e-05\n",
            "Epoch: 301, Loss: 9.873980161501095e-06\n",
            "Epoch: 302, Loss: 9.478148058406077e-06\n",
            "Epoch: 303, Loss: 9.123099516727962e-06\n",
            "Epoch: 304, Loss: 8.728698048798833e-06\n",
            "Epoch: 305, Loss: 8.338284715136979e-06\n",
            "Epoch: 306, Loss: 8.015173989406321e-06\n",
            "Epoch: 307, Loss: 7.700047717662528e-06\n",
            "Epoch: 308, Loss: 7.362107680819463e-06\n",
            "Epoch: 309, Loss: 7.038138846837683e-06\n",
            "Epoch: 310, Loss: 6.754950391041348e-06\n",
            "Epoch: 311, Loss: 6.4999967435142025e-06\n",
            "Epoch: 312, Loss: 6.2766962400928605e-06\n",
            "Epoch: 313, Loss: 0.021435998380184174\n",
            "Epoch: 314, Loss: 0.00041659382986836135\n",
            "Epoch: 315, Loss: 0.00018982603796757758\n",
            "Epoch: 316, Loss: 0.00015523022739216685\n",
            "Epoch: 317, Loss: 0.00013530933938454837\n",
            "Epoch: 318, Loss: 0.00012115066783735529\n",
            "Epoch: 319, Loss: 0.00011022571561625227\n",
            "Epoch: 320, Loss: 0.00010139207006432116\n",
            "Epoch: 321, Loss: 9.400112321600318e-05\n",
            "Epoch: 322, Loss: 8.767594408709556e-05\n",
            "Epoch: 323, Loss: 8.217577851610258e-05\n",
            "Epoch: 324, Loss: 7.730403012828901e-05\n",
            "Epoch: 325, Loss: 7.294385432032868e-05\n",
            "Epoch: 326, Loss: 6.901264714542776e-05\n",
            "Epoch: 327, Loss: 6.542916526086628e-05\n",
            "Epoch: 328, Loss: 6.214782479219139e-05\n",
            "Epoch: 329, Loss: 5.912900451221503e-05\n",
            "Epoch: 330, Loss: 5.633138425764628e-05\n",
            "Epoch: 331, Loss: 5.372532541514374e-05\n",
            "Epoch: 332, Loss: 5.130170757183805e-05\n",
            "Epoch: 333, Loss: 4.903089939034544e-05\n",
            "Epoch: 334, Loss: 4.690033529186621e-05\n",
            "Epoch: 335, Loss: 4.489922139327973e-05\n",
            "Epoch: 336, Loss: 4.30058607889805e-05\n",
            "Epoch: 337, Loss: 4.1250776121160015e-05\n",
            "Epoch: 338, Loss: 3.9492537325713784e-05\n",
            "Epoch: 339, Loss: 3.792535062530078e-05\n",
            "Epoch: 340, Loss: 3.6389214074006304e-05\n",
            "Epoch: 341, Loss: 3.496457793517038e-05\n",
            "Epoch: 342, Loss: 3.357528112246655e-05\n",
            "Epoch: 343, Loss: 3.228778950870037e-05\n",
            "Epoch: 344, Loss: 3.1023388146422803e-05\n",
            "Epoch: 345, Loss: 2.9853939849999733e-05\n",
            "Epoch: 346, Loss: 2.8692751584458165e-05\n",
            "Epoch: 347, Loss: 2.7614840291789733e-05\n",
            "Epoch: 348, Loss: 2.654603849805426e-05\n",
            "Epoch: 349, Loss: 2.5567629563738592e-05\n",
            "Epoch: 350, Loss: 2.458095332258381e-05\n",
            "Epoch: 351, Loss: 2.3662723833695054e-05\n",
            "Epoch: 352, Loss: 2.275104088766966e-05\n",
            "Epoch: 353, Loss: 2.1913503587711602e-05\n",
            "Epoch: 354, Loss: 2.1075105905765668e-05\n",
            "Epoch: 355, Loss: 2.0305715224822052e-05\n",
            "Epoch: 356, Loss: 1.9523780792951584e-05\n",
            "Epoch: 357, Loss: 1.8796305084833875e-05\n",
            "Epoch: 358, Loss: 1.8095923223881982e-05\n",
            "Epoch: 359, Loss: 1.742092536005657e-05\n",
            "Epoch: 360, Loss: 1.673879160080105e-05\n",
            "Epoch: 361, Loss: 1.611967309145257e-05\n",
            "Epoch: 362, Loss: 1.5488301869481802e-05\n",
            "Epoch: 363, Loss: 1.492678711656481e-05\n",
            "Epoch: 364, Loss: 1.4313946849142667e-05\n",
            "Epoch: 365, Loss: 1.376697764499113e-05\n",
            "Epoch: 366, Loss: 1.3229705473349895e-05\n",
            "Epoch: 367, Loss: 1.2696419616986532e-05\n",
            "Epoch: 368, Loss: 1.2169127330707852e-05\n",
            "Epoch: 369, Loss: 1.1690595783875324e-05\n",
            "Epoch: 370, Loss: 1.1235449164814781e-05\n",
            "Epoch: 371, Loss: 1.076205080607906e-05\n",
            "Epoch: 372, Loss: 1.037791389535414e-05\n",
            "Epoch: 373, Loss: 9.902802048600279e-06\n",
            "Epoch: 374, Loss: 9.497272912994958e-06\n",
            "Epoch: 375, Loss: 9.08347556105582e-06\n",
            "Epoch: 376, Loss: 8.697338671481702e-06\n",
            "Epoch: 377, Loss: 8.351978976861574e-06\n",
            "Epoch: 378, Loss: 7.977250788826495e-06\n",
            "Epoch: 379, Loss: 7.658700269530527e-06\n",
            "Epoch: 380, Loss: 7.330168500629952e-06\n",
            "Epoch: 381, Loss: 7.036145234451396e-06\n",
            "Epoch: 382, Loss: 6.730996119586052e-06\n",
            "Epoch: 383, Loss: 6.459215910581406e-06\n",
            "Epoch: 384, Loss: 6.1589162214659154e-06\n",
            "Epoch: 385, Loss: 5.933619377174182e-06\n",
            "Epoch: 386, Loss: 5.640448307531187e-06\n",
            "Epoch: 387, Loss: 5.4282700148178264e-06\n",
            "Epoch: 388, Loss: 5.190139290789375e-06\n",
            "Epoch: 389, Loss: 4.980812718713423e-06\n",
            "Epoch: 390, Loss: 4.813979103346355e-06\n",
            "Epoch: 391, Loss: 4.5767033043375704e-06\n",
            "Epoch: 392, Loss: 4.368232566775987e-06\n",
            "Epoch: 393, Loss: 4.190275376458885e-06\n",
            "Epoch: 394, Loss: 3.992355686932569e-06\n",
            "Epoch: 395, Loss: 3.84576924261637e-06\n",
            "Epoch: 396, Loss: 3.7245640669425484e-06\n",
            "Epoch: 397, Loss: 3.5383368413022254e-06\n",
            "Epoch: 398, Loss: 3.4165616398240672e-06\n",
            "Epoch: 399, Loss: 3.2882271625567228e-06\n",
            "Epoch: 400, Loss: 3.1958261388354003e-06\n",
            "Epoch: 401, Loss: 3.1048514301801333e-06\n",
            "Epoch: 402, Loss: 3.011310127476463e-06\n",
            "Epoch: 403, Loss: 0.01802336797118187\n",
            "Epoch: 404, Loss: 0.00045015220530331135\n",
            "Epoch: 405, Loss: 0.00014581071445718408\n",
            "Epoch: 406, Loss: 0.0001204280779347755\n",
            "Epoch: 407, Loss: 0.00010561962699284777\n",
            "Epoch: 408, Loss: 9.516481077298522e-05\n",
            "Epoch: 409, Loss: 8.712103590369225e-05\n",
            "Epoch: 410, Loss: 8.058980893110856e-05\n",
            "Epoch: 411, Loss: 7.509440183639526e-05\n",
            "Epoch: 412, Loss: 7.035406451905146e-05\n",
            "Epoch: 413, Loss: 6.61690064589493e-05\n",
            "Epoch: 414, Loss: 6.24292079010047e-05\n",
            "Epoch: 415, Loss: 5.903891724301502e-05\n",
            "Epoch: 416, Loss: 5.594423782895319e-05\n",
            "Epoch: 417, Loss: 5.308703475748189e-05\n",
            "Epoch: 418, Loss: 5.04387789987959e-05\n",
            "Epoch: 419, Loss: 4.796614666702226e-05\n",
            "Epoch: 420, Loss: 4.5647178922081366e-05\n",
            "Epoch: 421, Loss: 4.347473804955371e-05\n",
            "Epoch: 422, Loss: 4.142403486184776e-05\n",
            "Epoch: 423, Loss: 3.94694106944371e-05\n",
            "Epoch: 424, Loss: 3.764022039831616e-05\n",
            "Epoch: 425, Loss: 3.589057450881228e-05\n",
            "Epoch: 426, Loss: 3.424784154049121e-05\n",
            "Epoch: 427, Loss: 3.2682957680663094e-05\n",
            "Epoch: 428, Loss: 3.120358451269567e-05\n",
            "Epoch: 429, Loss: 2.978723932756111e-05\n",
            "Epoch: 430, Loss: 2.845299241016619e-05\n",
            "Epoch: 431, Loss: 2.7171223337063566e-05\n",
            "Epoch: 432, Loss: 2.5959863705793396e-05\n",
            "Epoch: 433, Loss: 2.4799834136501886e-05\n",
            "Epoch: 434, Loss: 2.3699685698375106e-05\n",
            "Epoch: 435, Loss: 2.2662548872176558e-05\n",
            "Epoch: 436, Loss: 2.1664181986125186e-05\n",
            "Epoch: 437, Loss: 2.0723136913147755e-05\n",
            "Epoch: 438, Loss: 1.98202978936024e-05\n",
            "Epoch: 439, Loss: 1.8963371985591948e-05\n",
            "Epoch: 440, Loss: 1.8153488781535998e-05\n",
            "Epoch: 441, Loss: 1.738410173857119e-05\n",
            "Epoch: 442, Loss: 1.664208321017213e-05\n",
            "Epoch: 443, Loss: 1.5941130186547525e-05\n",
            "Epoch: 444, Loss: 1.5274965335265733e-05\n",
            "Epoch: 445, Loss: 1.4633039427280892e-05\n",
            "Epoch: 446, Loss: 1.4035881577001419e-05\n",
            "Epoch: 447, Loss: 1.3453265637508593e-05\n",
            "Epoch: 448, Loss: 1.289888587052701e-05\n",
            "Epoch: 449, Loss: 1.2362182133074384e-05\n",
            "Epoch: 450, Loss: 1.1878802069986705e-05\n",
            "Epoch: 451, Loss: 1.1420809642004315e-05\n",
            "Epoch: 452, Loss: 1.096309642889537e-05\n",
            "Epoch: 453, Loss: 1.052990774041973e-05\n",
            "Epoch: 454, Loss: 1.0138069228560198e-05\n",
            "Epoch: 455, Loss: 9.773037163540721e-06\n",
            "Epoch: 456, Loss: 9.369507097289898e-06\n",
            "Epoch: 457, Loss: 9.062938261195086e-06\n",
            "Epoch: 458, Loss: 8.704463652975392e-06\n",
            "Epoch: 459, Loss: 8.392475137952715e-06\n",
            "Epoch: 460, Loss: 8.09645644039847e-06\n",
            "Epoch: 461, Loss: 7.83037921792129e-06\n",
            "Epoch: 462, Loss: 7.5682955866795965e-06\n",
            "Epoch: 463, Loss: 7.279688816197449e-06\n",
            "Epoch: 464, Loss: 6.980816579016391e-06\n",
            "Epoch: 465, Loss: 6.753240086254664e-06\n",
            "Epoch: 466, Loss: 6.498285074485466e-06\n",
            "Epoch: 467, Loss: 6.30749582342105e-06\n",
            "Epoch: 468, Loss: 6.070793006074382e-06\n",
            "Epoch: 469, Loss: 5.883996436750749e-06\n",
            "Epoch: 470, Loss: 5.673814939655131e-06\n",
            "Epoch: 471, Loss: 5.475610123539809e-06\n",
            "Epoch: 472, Loss: 5.2731284085894e-06\n",
            "Epoch: 473, Loss: 5.118842636875343e-06\n",
            "Epoch: 474, Loss: 4.904667548544239e-06\n",
            "Epoch: 475, Loss: 4.738403731607832e-06\n",
            "Epoch: 476, Loss: 4.563299626170192e-06\n",
            "Epoch: 477, Loss: 4.4286907723289914e-06\n",
            "Epoch: 478, Loss: 4.24445988755906e-06\n",
            "Epoch: 479, Loss: 4.120118319406174e-06\n",
            "Epoch: 480, Loss: 3.928758360416396e-06\n",
            "Epoch: 481, Loss: 3.8015648442524252e-06\n",
            "Epoch: 482, Loss: 3.720000904650078e-06\n",
            "Epoch: 483, Loss: 3.577121788111981e-06\n",
            "Epoch: 484, Loss: 3.406579708098434e-06\n",
            "Epoch: 485, Loss: 3.3338574212393723e-06\n",
            "Epoch: 486, Loss: 0.036887701600790024\n",
            "Epoch: 487, Loss: 0.0017366912215948105\n",
            "Epoch: 488, Loss: 0.00016829556261654943\n",
            "Epoch: 489, Loss: 0.0001234557421412319\n",
            "Epoch: 490, Loss: 0.00010411917901365086\n",
            "Epoch: 491, Loss: 9.150114783551544e-05\n",
            "Epoch: 492, Loss: 8.226372301578522e-05\n",
            "Epoch: 493, Loss: 7.504398672608659e-05\n",
            "Epoch: 494, Loss: 6.915667472640052e-05\n",
            "Epoch: 495, Loss: 6.420428690034896e-05\n",
            "Epoch: 496, Loss: 5.994733874103986e-05\n",
            "Epoch: 497, Loss: 5.620466617983766e-05\n",
            "Epoch: 498, Loss: 5.2925908676115796e-05\n",
            "Epoch: 499, Loss: 4.992124377167784e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhEM45qCySFr",
        "outputId": "1ed59a3d-204c-4fe4-87e3-4994fed99dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.337628364562988\n",
            "Epoch: 1, Loss: 7.573127269744873\n",
            "Epoch: 2, Loss: 6.921727180480957\n",
            "Epoch: 3, Loss: 6.23065185546875\n",
            "Epoch: 4, Loss: 5.377331256866455\n",
            "Epoch: 5, Loss: 4.347499370574951\n",
            "Epoch: 6, Loss: 3.4780073165893555\n",
            "Epoch: 7, Loss: 2.964050054550171\n",
            "Epoch: 8, Loss: 2.6289753913879395\n",
            "Epoch: 9, Loss: 2.376471996307373\n",
            "Epoch: 10, Loss: 2.168919801712036\n",
            "Epoch: 11, Loss: 1.9952117204666138\n",
            "Epoch: 12, Loss: 1.8466012477874756\n",
            "Epoch: 13, Loss: 1.7159780263900757\n",
            "Epoch: 14, Loss: 1.6021184921264648\n",
            "Epoch: 15, Loss: 1.497164011001587\n",
            "Epoch: 16, Loss: 1.4013597965240479\n",
            "Epoch: 17, Loss: 1.3115330934524536\n",
            "Epoch: 18, Loss: 1.2277109622955322\n",
            "Epoch: 19, Loss: 1.146233320236206\n",
            "Epoch: 20, Loss: 1.0710577964782715\n",
            "Epoch: 21, Loss: 0.996680736541748\n",
            "Epoch: 22, Loss: 0.9283207058906555\n",
            "Epoch: 23, Loss: 0.8596868515014648\n",
            "Epoch: 24, Loss: 0.7956984043121338\n",
            "Epoch: 25, Loss: 0.7315620183944702\n",
            "Epoch: 26, Loss: 0.6733278036117554\n",
            "Epoch: 27, Loss: 0.6125651597976685\n",
            "Epoch: 28, Loss: 0.55580735206604\n",
            "Epoch: 29, Loss: 0.4990778863430023\n",
            "Epoch: 30, Loss: 0.44546744227409363\n",
            "Epoch: 31, Loss: 0.3971831500530243\n",
            "Epoch: 32, Loss: 0.3546367287635803\n",
            "Epoch: 33, Loss: 0.3195173740386963\n",
            "Epoch: 34, Loss: 0.28666093945503235\n",
            "Epoch: 35, Loss: 0.2573905885219574\n",
            "Epoch: 36, Loss: 0.23089390993118286\n",
            "Epoch: 37, Loss: 0.20794588327407837\n",
            "Epoch: 38, Loss: 0.18479202687740326\n",
            "Epoch: 39, Loss: 0.16700828075408936\n",
            "Epoch: 40, Loss: 0.14812760055065155\n",
            "Epoch: 41, Loss: 0.13266490399837494\n",
            "Epoch: 42, Loss: 0.1186613067984581\n",
            "Epoch: 43, Loss: 0.10622365027666092\n",
            "Epoch: 44, Loss: 0.09383539855480194\n",
            "Epoch: 45, Loss: 0.08299096673727036\n",
            "Epoch: 46, Loss: 0.07304263859987259\n",
            "Epoch: 47, Loss: 0.06527510285377502\n",
            "Epoch: 48, Loss: 0.057334013283252716\n",
            "Epoch: 49, Loss: 0.051041726022958755\n",
            "Epoch: 50, Loss: 0.04572411626577377\n",
            "Epoch: 51, Loss: 0.04051719233393669\n",
            "Epoch: 52, Loss: 0.03670958802103996\n",
            "Epoch: 53, Loss: 0.032391369342803955\n",
            "Epoch: 54, Loss: 0.029146194458007812\n",
            "Epoch: 55, Loss: 0.025824759155511856\n",
            "Epoch: 56, Loss: 0.02318112552165985\n",
            "Epoch: 57, Loss: 0.02060845121741295\n",
            "Epoch: 58, Loss: 0.01853206381201744\n",
            "Epoch: 59, Loss: 0.01629568077623844\n",
            "Epoch: 60, Loss: 0.014407915063202381\n",
            "Epoch: 61, Loss: 0.01264513935893774\n",
            "Epoch: 62, Loss: 0.010981935076415539\n",
            "Epoch: 63, Loss: 0.00926833227276802\n",
            "Epoch: 64, Loss: 0.007975541055202484\n",
            "Epoch: 65, Loss: 0.006901463493704796\n",
            "Epoch: 66, Loss: 0.005995596293359995\n",
            "Epoch: 67, Loss: 0.005204389337450266\n",
            "Epoch: 68, Loss: 0.004387040622532368\n",
            "Epoch: 69, Loss: 0.0038165850564837456\n",
            "Epoch: 70, Loss: 0.00334152695722878\n",
            "Epoch: 71, Loss: 0.0029303987976163626\n",
            "Epoch: 72, Loss: 0.0026162120047956705\n",
            "Epoch: 73, Loss: 0.0023537210654467344\n",
            "Epoch: 74, Loss: 0.0021185793448239565\n",
            "Epoch: 75, Loss: 0.00186574412509799\n",
            "Epoch: 76, Loss: 0.0016375178238376975\n",
            "Epoch: 77, Loss: 0.001468030852265656\n",
            "Epoch: 78, Loss: 0.0013078988995403051\n",
            "Epoch: 79, Loss: 0.001165793975815177\n",
            "Epoch: 80, Loss: 0.0010482682846486568\n",
            "Epoch: 81, Loss: 0.0009418356348760426\n",
            "Epoch: 82, Loss: 0.0008524450822733343\n",
            "Epoch: 83, Loss: 0.0007638102979399264\n",
            "Epoch: 84, Loss: 0.0006907280767336488\n",
            "Epoch: 85, Loss: 0.0006214574095793068\n",
            "Epoch: 86, Loss: 0.0005624446785077453\n",
            "Epoch: 87, Loss: 0.0005083843716420233\n",
            "Epoch: 88, Loss: 0.0004602383414749056\n",
            "Epoch: 89, Loss: 0.0004143604601267725\n",
            "Epoch: 90, Loss: 0.00037492072442546487\n",
            "Epoch: 91, Loss: 0.0003382053691893816\n",
            "Epoch: 92, Loss: 0.00030557168065570295\n",
            "Epoch: 93, Loss: 0.000274951133178547\n",
            "Epoch: 94, Loss: 0.00025019593886099756\n",
            "Epoch: 95, Loss: 0.00022405709023587406\n",
            "Epoch: 96, Loss: 0.00020290153042878956\n",
            "Epoch: 97, Loss: 0.00018199159239884466\n",
            "Epoch: 98, Loss: 0.00016725406749174\n",
            "Epoch: 99, Loss: 0.00014978524995967746\n",
            "Epoch: 100, Loss: 0.0001353788102278486\n",
            "Epoch: 101, Loss: 0.00012179094483144581\n",
            "Epoch: 102, Loss: 0.0001109079021262005\n",
            "Epoch: 103, Loss: 9.948851948138326e-05\n",
            "Epoch: 104, Loss: 9.059700096258894e-05\n",
            "Epoch: 105, Loss: 8.143854211084545e-05\n",
            "Epoch: 106, Loss: 7.409409590763971e-05\n",
            "Epoch: 107, Loss: 6.647286500083283e-05\n",
            "Epoch: 108, Loss: 6.0314217989798635e-05\n",
            "Epoch: 109, Loss: 5.453131961985491e-05\n",
            "Epoch: 110, Loss: 4.9580314225750044e-05\n",
            "Epoch: 111, Loss: 4.455136877368204e-05\n",
            "Epoch: 112, Loss: 4.051546784467064e-05\n",
            "Epoch: 113, Loss: 3.663330426206812e-05\n",
            "Epoch: 114, Loss: 3.296586874057539e-05\n",
            "Epoch: 115, Loss: 2.998675154231023e-05\n",
            "Epoch: 116, Loss: 2.724572186707519e-05\n",
            "Epoch: 117, Loss: 2.4579998353146948e-05\n",
            "Epoch: 118, Loss: 2.237607623101212e-05\n",
            "Epoch: 119, Loss: 2.0163455701549537e-05\n",
            "Epoch: 120, Loss: 1.8381759218755178e-05\n",
            "Epoch: 121, Loss: 1.6662972484482452e-05\n",
            "Epoch: 122, Loss: 1.5280142179108225e-05\n",
            "Epoch: 123, Loss: 1.3938004485680722e-05\n",
            "Epoch: 124, Loss: 1.2551415238704067e-05\n",
            "Epoch: 125, Loss: 1.1443638868513517e-05\n",
            "Epoch: 126, Loss: 1.0338458196201827e-05\n",
            "Epoch: 127, Loss: 9.504879926680587e-06\n",
            "Epoch: 128, Loss: 8.682554835104384e-06\n",
            "Epoch: 129, Loss: 8.168494787241798e-06\n",
            "Epoch: 130, Loss: 0.14933469891548157\n",
            "Epoch: 131, Loss: 0.06312664598226547\n",
            "Epoch: 132, Loss: 0.0021374619100242853\n",
            "Epoch: 133, Loss: 0.00033557749702595174\n",
            "Epoch: 134, Loss: 0.00020276049326639622\n",
            "Epoch: 135, Loss: 0.00017535235383547843\n",
            "Epoch: 136, Loss: 0.0001591934124007821\n",
            "Epoch: 137, Loss: 0.00014733170974068344\n",
            "Epoch: 138, Loss: 0.00013846898218616843\n",
            "Epoch: 139, Loss: 0.00013103066885378212\n",
            "Epoch: 140, Loss: 0.0001248830376425758\n",
            "Epoch: 141, Loss: 0.00011939017713302746\n",
            "Epoch: 142, Loss: 0.00011451402679085732\n",
            "Epoch: 143, Loss: 0.00010995831689797342\n",
            "Epoch: 144, Loss: 0.00010578634828561917\n",
            "Epoch: 145, Loss: 0.00010170946916332468\n",
            "Epoch: 146, Loss: 9.785270231077448e-05\n",
            "Epoch: 147, Loss: 9.405886521562934e-05\n",
            "Epoch: 148, Loss: 9.046183549799025e-05\n",
            "Epoch: 149, Loss: 8.706285734660923e-05\n",
            "Epoch: 150, Loss: 8.371476724278182e-05\n",
            "Epoch: 151, Loss: 8.047299343161285e-05\n",
            "Epoch: 152, Loss: 7.745556649751961e-05\n",
            "Epoch: 153, Loss: 7.464981899829581e-05\n",
            "Epoch: 154, Loss: 7.182954141171649e-05\n",
            "Epoch: 155, Loss: 6.917480641277507e-05\n",
            "Epoch: 156, Loss: 6.667438719887286e-05\n",
            "Epoch: 157, Loss: 6.421573198167607e-05\n",
            "Epoch: 158, Loss: 6.180607306305319e-05\n",
            "Epoch: 159, Loss: 5.9585941926343367e-05\n",
            "Epoch: 160, Loss: 5.733719081035815e-05\n",
            "Epoch: 161, Loss: 5.511120616574772e-05\n",
            "Epoch: 162, Loss: 5.298184623825364e-05\n",
            "Epoch: 163, Loss: 5.0919690693262964e-05\n",
            "Epoch: 164, Loss: 4.884508598479442e-05\n",
            "Epoch: 165, Loss: 4.684607847593725e-05\n",
            "Epoch: 166, Loss: 4.495641405810602e-05\n",
            "Epoch: 167, Loss: 4.293225720175542e-05\n",
            "Epoch: 168, Loss: 4.101540253031999e-05\n",
            "Epoch: 169, Loss: 3.914760236511938e-05\n",
            "Epoch: 170, Loss: 3.736952203325927e-05\n",
            "Epoch: 171, Loss: 3.555421426426619e-05\n",
            "Epoch: 172, Loss: 3.379803820280358e-05\n",
            "Epoch: 173, Loss: 3.2170009944820777e-05\n",
            "Epoch: 174, Loss: 3.052261672564782e-05\n",
            "Epoch: 175, Loss: 2.8951104468433186e-05\n",
            "Epoch: 176, Loss: 2.7480602511786856e-05\n",
            "Epoch: 177, Loss: 2.593851968413219e-05\n",
            "Epoch: 178, Loss: 2.4567863874835894e-05\n",
            "Epoch: 179, Loss: 2.3223748939926736e-05\n",
            "Epoch: 180, Loss: 2.1928986825514585e-05\n",
            "Epoch: 181, Loss: 2.0649511498049833e-05\n",
            "Epoch: 182, Loss: 1.937435990839731e-05\n",
            "Epoch: 183, Loss: 1.8204264051746577e-05\n",
            "Epoch: 184, Loss: 1.745122244756203e-05\n",
            "Epoch: 185, Loss: 1.6211562979151495e-05\n",
            "Epoch: 186, Loss: 1.5126597645576112e-05\n",
            "Epoch: 187, Loss: 1.4090411241340917e-05\n",
            "Epoch: 188, Loss: 1.314081146119861e-05\n",
            "Epoch: 189, Loss: 1.2346783478278667e-05\n",
            "Epoch: 190, Loss: 1.1475975952635054e-05\n",
            "Epoch: 191, Loss: 1.0619310160109308e-05\n",
            "Epoch: 192, Loss: 9.912442692439072e-06\n",
            "Epoch: 193, Loss: 9.256661542167421e-06\n",
            "Epoch: 194, Loss: 9.627246981835924e-06\n",
            "Epoch: 195, Loss: 0.05696573480963707\n",
            "Epoch: 196, Loss: 0.004930819850414991\n",
            "Epoch: 197, Loss: 0.000545291812159121\n",
            "Epoch: 198, Loss: 0.0002206274657510221\n",
            "Epoch: 199, Loss: 0.0001929456484504044\n",
            "Epoch: 200, Loss: 0.00017501664115116\n",
            "Epoch: 201, Loss: 0.00016199857054743916\n",
            "Epoch: 202, Loss: 0.00015139240713324398\n",
            "Epoch: 203, Loss: 0.00014276232104748487\n",
            "Epoch: 204, Loss: 0.00013508521078620106\n",
            "Epoch: 205, Loss: 0.00012869495549239218\n",
            "Epoch: 206, Loss: 0.00012266877456568182\n",
            "Epoch: 207, Loss: 0.00011741588241420686\n",
            "Epoch: 208, Loss: 0.0001125522394431755\n",
            "Epoch: 209, Loss: 0.00010807037324411795\n",
            "Epoch: 210, Loss: 0.00010382947948528454\n",
            "Epoch: 211, Loss: 0.00010001427290262654\n",
            "Epoch: 212, Loss: 9.614324517315254e-05\n",
            "Epoch: 213, Loss: 9.253677126253024e-05\n",
            "Epoch: 214, Loss: 8.910608448786661e-05\n",
            "Epoch: 215, Loss: 8.577854896429926e-05\n",
            "Epoch: 216, Loss: 8.266440272564068e-05\n",
            "Epoch: 217, Loss: 7.9538302088622e-05\n",
            "Epoch: 218, Loss: 7.657079549971968e-05\n",
            "Epoch: 219, Loss: 7.38631424610503e-05\n",
            "Epoch: 220, Loss: 7.107897545211017e-05\n",
            "Epoch: 221, Loss: 6.84929545968771e-05\n",
            "Epoch: 222, Loss: 6.586707604583353e-05\n",
            "Epoch: 223, Loss: 6.345439032884315e-05\n",
            "Epoch: 224, Loss: 6.106530781835318e-05\n",
            "Epoch: 225, Loss: 5.878555748495273e-05\n",
            "Epoch: 226, Loss: 5.656608482240699e-05\n",
            "Epoch: 227, Loss: 5.432376201497391e-05\n",
            "Epoch: 228, Loss: 5.2202638471499085e-05\n",
            "Epoch: 229, Loss: 5.0106580602005124e-05\n",
            "Epoch: 230, Loss: 4.80970848002471e-05\n",
            "Epoch: 231, Loss: 4.607630762620829e-05\n",
            "Epoch: 232, Loss: 4.4156520743854344e-05\n",
            "Epoch: 233, Loss: 4.233802974340506e-05\n",
            "Epoch: 234, Loss: 4.039281702716835e-05\n",
            "Epoch: 235, Loss: 3.856822513625957e-05\n",
            "Epoch: 236, Loss: 3.677565109683201e-05\n",
            "Epoch: 237, Loss: 3.500241655274294e-05\n",
            "Epoch: 238, Loss: 3.35405784426257e-05\n",
            "Epoch: 239, Loss: 3.162185748806223e-05\n",
            "Epoch: 240, Loss: 3.0111219530226663e-05\n",
            "Epoch: 241, Loss: 2.842914909706451e-05\n",
            "Epoch: 242, Loss: 2.6720215828390792e-05\n",
            "Epoch: 243, Loss: 2.521302485547494e-05\n",
            "Epoch: 244, Loss: 2.3726897779852152e-05\n",
            "Epoch: 245, Loss: 2.2308009647531435e-05\n",
            "Epoch: 246, Loss: 2.093905641231686e-05\n",
            "Epoch: 247, Loss: 1.9695064111147076e-05\n",
            "Epoch: 248, Loss: 1.8961345631396398e-05\n",
            "Epoch: 249, Loss: 1.728962524794042e-05\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 1]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size = 15  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 64\n",
        "activation = 'relu'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(250):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_3': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'trained_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 2]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size = 15  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 128\n",
        "activation = 'tanh'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(500):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_6': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'trained_model_9.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6adYSOZM24KD",
        "outputId": "a2e3a9d2-68aa-4b47-a479-171a6d2bdd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.980344772338867\n",
            "Epoch: 1, Loss: 7.859348297119141\n",
            "Epoch: 2, Loss: 6.989047050476074\n",
            "Epoch: 3, Loss: 6.230449676513672\n",
            "Epoch: 4, Loss: 5.526468753814697\n",
            "Epoch: 5, Loss: 4.863089561462402\n",
            "Epoch: 6, Loss: 4.238917350769043\n",
            "Epoch: 7, Loss: 3.6551342010498047\n",
            "Epoch: 8, Loss: 3.118349075317383\n",
            "Epoch: 9, Loss: 2.6330983638763428\n",
            "Epoch: 10, Loss: 2.1926698684692383\n",
            "Epoch: 11, Loss: 1.7913987636566162\n",
            "Epoch: 12, Loss: 1.4308441877365112\n",
            "Epoch: 13, Loss: 1.1136871576309204\n",
            "Epoch: 14, Loss: 0.856682538986206\n",
            "Epoch: 15, Loss: 0.6746461987495422\n",
            "Epoch: 16, Loss: 0.5383383631706238\n",
            "Epoch: 17, Loss: 0.435543030500412\n",
            "Epoch: 18, Loss: 0.3558567762374878\n",
            "Epoch: 19, Loss: 0.2934077978134155\n",
            "Epoch: 20, Loss: 0.24554777145385742\n",
            "Epoch: 21, Loss: 0.20778611302375793\n",
            "Epoch: 22, Loss: 0.17726805806159973\n",
            "Epoch: 23, Loss: 0.1525116264820099\n",
            "Epoch: 24, Loss: 0.13209417462348938\n",
            "Epoch: 25, Loss: 0.11487172544002533\n",
            "Epoch: 26, Loss: 0.10045398026704788\n",
            "Epoch: 27, Loss: 0.088351309299469\n",
            "Epoch: 28, Loss: 0.07820339500904083\n",
            "Epoch: 29, Loss: 0.06952723115682602\n",
            "Epoch: 30, Loss: 0.06184368208050728\n",
            "Epoch: 31, Loss: 0.05516342446208\n",
            "Epoch: 32, Loss: 0.04948057234287262\n",
            "Epoch: 33, Loss: 0.04444098472595215\n",
            "Epoch: 34, Loss: 0.03994683548808098\n",
            "Epoch: 35, Loss: 0.035959746688604355\n",
            "Epoch: 36, Loss: 0.03245612978935242\n",
            "Epoch: 37, Loss: 0.029446685686707497\n",
            "Epoch: 38, Loss: 0.02667567878961563\n",
            "Epoch: 39, Loss: 0.024183254688978195\n",
            "Epoch: 40, Loss: 0.021901588886976242\n",
            "Epoch: 41, Loss: 0.019776277244091034\n",
            "Epoch: 42, Loss: 0.01782580465078354\n",
            "Epoch: 43, Loss: 0.01616206206381321\n",
            "Epoch: 44, Loss: 0.014665543101727962\n",
            "Epoch: 45, Loss: 0.013323809020221233\n",
            "Epoch: 46, Loss: 0.012133398093283176\n",
            "Epoch: 47, Loss: 0.011042109690606594\n",
            "Epoch: 48, Loss: 0.010066382586956024\n",
            "Epoch: 49, Loss: 0.009159557521343231\n",
            "Epoch: 50, Loss: 0.00834581721574068\n",
            "Epoch: 51, Loss: 0.007608757354319096\n",
            "Epoch: 52, Loss: 0.006966220214962959\n",
            "Epoch: 53, Loss: 0.006387954577803612\n",
            "Epoch: 54, Loss: 0.005861908197402954\n",
            "Epoch: 55, Loss: 0.005383259151130915\n",
            "Epoch: 56, Loss: 0.004948582965880632\n",
            "Epoch: 57, Loss: 0.0045513431541621685\n",
            "Epoch: 58, Loss: 0.004191284067928791\n",
            "Epoch: 59, Loss: 0.00385585380718112\n",
            "Epoch: 60, Loss: 0.0035510354209691286\n",
            "Epoch: 61, Loss: 0.003267957828938961\n",
            "Epoch: 62, Loss: 0.003010813845321536\n",
            "Epoch: 63, Loss: 0.002774191554635763\n",
            "Epoch: 64, Loss: 0.002556852763518691\n",
            "Epoch: 65, Loss: 0.002357805846258998\n",
            "Epoch: 66, Loss: 0.002175977686420083\n",
            "Epoch: 67, Loss: 0.0020081575494259596\n",
            "Epoch: 68, Loss: 0.0018542737234383821\n",
            "Epoch: 69, Loss: 0.0017105466686189175\n",
            "Epoch: 70, Loss: 0.001579150091856718\n",
            "Epoch: 71, Loss: 0.0014571527717635036\n",
            "Epoch: 72, Loss: 0.0013452586717903614\n",
            "Epoch: 73, Loss: 0.0012407212052494287\n",
            "Epoch: 74, Loss: 0.0011461327085271478\n",
            "Epoch: 75, Loss: 0.001057748799212277\n",
            "Epoch: 76, Loss: 0.0009764687274582684\n",
            "Epoch: 77, Loss: 0.0009010353242047131\n",
            "Epoch: 78, Loss: 0.0008315874729305506\n",
            "Epoch: 79, Loss: 0.0007668495527468622\n",
            "Epoch: 80, Loss: 0.0007075417088344693\n",
            "Epoch: 81, Loss: 0.0006522048497572541\n",
            "Epoch: 82, Loss: 0.0006019639549776912\n",
            "Epoch: 83, Loss: 0.000554867903701961\n",
            "Epoch: 84, Loss: 0.0005122817237861454\n",
            "Epoch: 85, Loss: 0.00047222315333783627\n",
            "Epoch: 86, Loss: 0.00043599942000582814\n",
            "Epoch: 87, Loss: 0.0004022694774903357\n",
            "Epoch: 88, Loss: 0.0003715419734362513\n",
            "Epoch: 89, Loss: 0.00034301273990422487\n",
            "Epoch: 90, Loss: 0.0003171518328599632\n",
            "Epoch: 91, Loss: 0.00029308247030712664\n",
            "Epoch: 92, Loss: 0.0002707884996198118\n",
            "Epoch: 93, Loss: 0.0002501985873095691\n",
            "Epoch: 94, Loss: 0.00023120995319914073\n",
            "Epoch: 95, Loss: 0.00021339778322726488\n",
            "Epoch: 96, Loss: 0.00019718280236702412\n",
            "Epoch: 97, Loss: 0.00018208885740023106\n",
            "Epoch: 98, Loss: 0.00016860992764122784\n",
            "Epoch: 99, Loss: 0.00015594030264765024\n",
            "Epoch: 100, Loss: 0.00014451215974986553\n",
            "Epoch: 101, Loss: 0.0001337265712209046\n",
            "Epoch: 102, Loss: 0.00012391871132422239\n",
            "Epoch: 103, Loss: 0.00011473818449303508\n",
            "Epoch: 104, Loss: 0.00010618626401992515\n",
            "Epoch: 105, Loss: 9.822184802033007e-05\n",
            "Epoch: 106, Loss: 9.088305523619056e-05\n",
            "Epoch: 107, Loss: 8.406647975789383e-05\n",
            "Epoch: 108, Loss: 7.78277826611884e-05\n",
            "Epoch: 109, Loss: 7.200929394457489e-05\n",
            "Epoch: 110, Loss: 6.660875806119293e-05\n",
            "Epoch: 111, Loss: 6.164070509839803e-05\n",
            "Epoch: 112, Loss: 5.7112574722850695e-05\n",
            "Epoch: 113, Loss: 5.30375364178326e-05\n",
            "Epoch: 114, Loss: 4.932576121063903e-05\n",
            "Epoch: 115, Loss: 4.578837615554221e-05\n",
            "Epoch: 116, Loss: 4.25761281803716e-05\n",
            "Epoch: 117, Loss: 3.96154064219445e-05\n",
            "Epoch: 118, Loss: 3.6865480069536716e-05\n",
            "Epoch: 119, Loss: 3.432543235248886e-05\n",
            "Epoch: 120, Loss: 3.196375837433152e-05\n",
            "Epoch: 121, Loss: 2.9805400117766112e-05\n",
            "Epoch: 122, Loss: 2.7771067834692076e-05\n",
            "Epoch: 123, Loss: 2.5894496502587572e-05\n",
            "Epoch: 124, Loss: 2.4134054910973646e-05\n",
            "Epoch: 125, Loss: 2.2502457795781083e-05\n",
            "Epoch: 126, Loss: 2.0986100935260765e-05\n",
            "Epoch: 127, Loss: 1.9603859982453287e-05\n",
            "Epoch: 128, Loss: 1.8289974832441658e-05\n",
            "Epoch: 129, Loss: 1.7087841115426272e-05\n",
            "Epoch: 130, Loss: 1.5951882232911885e-05\n",
            "Epoch: 131, Loss: 1.4903136616339907e-05\n",
            "Epoch: 132, Loss: 1.418001738784369e-05\n",
            "Epoch: 133, Loss: 1.3129072613082826e-05\n",
            "Epoch: 134, Loss: 1.2231518667249475e-05\n",
            "Epoch: 135, Loss: 1.1436954082455486e-05\n",
            "Epoch: 136, Loss: 1.0707250112318434e-05\n",
            "Epoch: 137, Loss: 1.0044163900602143e-05\n",
            "Epoch: 138, Loss: 9.407369361724705e-06\n",
            "Epoch: 139, Loss: 8.82053700479446e-06\n",
            "Epoch: 140, Loss: 8.288924618682358e-06\n",
            "Epoch: 141, Loss: 7.781416570651345e-06\n",
            "Epoch: 142, Loss: 7.315981292776996e-06\n",
            "Epoch: 143, Loss: 6.876842235215008e-06\n",
            "Epoch: 144, Loss: 6.485911399067845e-06\n",
            "Epoch: 145, Loss: 6.094540822232375e-06\n",
            "Epoch: 146, Loss: 5.757953204010846e-06\n",
            "Epoch: 147, Loss: 5.421803507488221e-06\n",
            "Epoch: 148, Loss: 5.101432179799303e-06\n",
            "Epoch: 149, Loss: 4.814806288777618e-06\n",
            "Epoch: 150, Loss: 4.550970515992958e-06\n",
            "Epoch: 151, Loss: 4.334467575972667e-06\n",
            "Epoch: 152, Loss: 4.0798354348225985e-06\n",
            "Epoch: 153, Loss: 3.855881459458033e-06\n",
            "Epoch: 154, Loss: 3.647704716058797e-06\n",
            "Epoch: 155, Loss: 3.449169753366732e-06\n",
            "Epoch: 156, Loss: 3.276492634540773e-06\n",
            "Epoch: 157, Loss: 3.1086362923815614e-06\n",
            "Epoch: 158, Loss: 2.952175236714538e-06\n",
            "Epoch: 159, Loss: 2.8049180400557816e-06\n",
            "Epoch: 160, Loss: 2.672561095096171e-06\n",
            "Epoch: 161, Loss: 2.555982518970268e-06\n",
            "Epoch: 162, Loss: 2.4551809474360198e-06\n",
            "Epoch: 163, Loss: 2.3329039322561584e-06\n",
            "Epoch: 164, Loss: 2.220268925157143e-06\n",
            "Epoch: 165, Loss: 2.1628559352393495e-06\n",
            "Epoch: 166, Loss: 2.038825869021821e-06\n",
            "Epoch: 167, Loss: 1.9494195839797612e-06\n",
            "Epoch: 168, Loss: 1.8692163621381042e-06\n",
            "Epoch: 169, Loss: 1.7846307400759542e-06\n",
            "Epoch: 170, Loss: 1.7180138911498943e-06\n",
            "Epoch: 171, Loss: 1.6500823676324217e-06\n",
            "Epoch: 172, Loss: 1.5874098835411132e-06\n",
            "Epoch: 173, Loss: 1.5308734191421536e-06\n",
            "Epoch: 174, Loss: 1.4848553746560356e-06\n",
            "Epoch: 175, Loss: 1.4291952084022341e-06\n",
            "Epoch: 176, Loss: 1.379670834467106e-06\n",
            "Epoch: 177, Loss: 1.322257730862475e-06\n",
            "Epoch: 178, Loss: 1.2968382634426234e-06\n",
            "Epoch: 179, Loss: 1.2486284504120704e-06\n",
            "Epoch: 180, Loss: 1.1999808293694514e-06\n",
            "Epoch: 181, Loss: 1.1666722912195837e-06\n",
            "Epoch: 182, Loss: 1.1281047136435518e-06\n",
            "Epoch: 183, Loss: 1.1004937050529406e-06\n",
            "Epoch: 184, Loss: 1.0711296454246622e-06\n",
            "Epoch: 185, Loss: 1.0439570132803055e-06\n",
            "Epoch: 186, Loss: 1.0233585499008768e-06\n",
            "Epoch: 187, Loss: 9.904881608235883e-07\n",
            "Epoch: 188, Loss: 9.681366464064922e-07\n",
            "Epoch: 189, Loss: 9.449084927837248e-07\n",
            "Epoch: 190, Loss: 9.190504783873621e-07\n",
            "Epoch: 191, Loss: 9.063408015208552e-07\n",
            "Epoch: 192, Loss: 8.89686646132759e-07\n",
            "Epoch: 193, Loss: 8.699645377419074e-07\n",
            "Epoch: 194, Loss: 8.454214821540518e-07\n",
            "Epoch: 195, Loss: 8.318351092384546e-07\n",
            "Epoch: 196, Loss: 8.143042578012682e-07\n",
            "Epoch: 197, Loss: 7.989648906914226e-07\n",
            "Epoch: 198, Loss: 7.858168373786611e-07\n",
            "Epoch: 199, Loss: 7.752983606224007e-07\n",
            "Epoch: 200, Loss: 7.6740951726606e-07\n",
            "Epoch: 201, Loss: 7.4900214031004e-07\n",
            "Epoch: 202, Loss: 7.402367714348657e-07\n",
            "Epoch: 203, Loss: 7.380454007943626e-07\n",
            "Epoch: 204, Loss: 7.200764002845972e-07\n",
            "Epoch: 205, Loss: 7.108727118065872e-07\n",
            "Epoch: 206, Loss: 7.073665528878337e-07\n",
            "Epoch: 207, Loss: 6.907123974997376e-07\n",
            "Epoch: 208, Loss: 6.828235541433969e-07\n",
            "Epoch: 209, Loss: 6.885210268592346e-07\n",
            "Epoch: 210, Loss: 6.714285518683027e-07\n",
            "Epoch: 211, Loss: 6.631013889091264e-07\n",
            "Epoch: 212, Loss: 6.534596082019561e-07\n",
            "Epoch: 213, Loss: 6.49076923764369e-07\n",
            "Epoch: 214, Loss: 6.37681864645856e-07\n",
            "Epoch: 215, Loss: 6.350523449327738e-07\n",
            "Epoch: 216, Loss: 6.267251819735975e-07\n",
            "Epoch: 217, Loss: 6.188363386172568e-07\n",
            "Epoch: 218, Loss: 6.210277092577599e-07\n",
            "Epoch: 219, Loss: 6.166450248201727e-07\n",
            "Epoch: 220, Loss: 6.074413363421627e-07\n",
            "Epoch: 221, Loss: 6.078795991015795e-07\n",
            "Epoch: 222, Loss: 6.026203891451587e-07\n",
            "Epoch: 223, Loss: 5.969228595859022e-07\n",
            "Epoch: 224, Loss: 5.92540175148315e-07\n",
            "Epoch: 225, Loss: 5.912253868700645e-07\n",
            "Epoch: 226, Loss: 5.85527857310808e-07\n",
            "Epoch: 227, Loss: 5.872809651918942e-07\n",
            "Epoch: 228, Loss: 5.846513317919744e-07\n",
            "Epoch: 229, Loss: 5.763242256762169e-07\n",
            "Epoch: 230, Loss: 5.802686473543872e-07\n",
            "Epoch: 231, Loss: 5.745711177951307e-07\n",
            "Epoch: 232, Loss: 5.745711177951307e-07\n",
            "Epoch: 233, Loss: 5.715032784792129e-07\n",
            "Epoch: 234, Loss: 5.684353823198762e-07\n",
            "Epoch: 235, Loss: 5.636143782794534e-07\n",
            "Epoch: 236, Loss: 5.640526978822891e-07\n",
            "Epoch: 237, Loss: 5.622995900012029e-07\n",
            "Epoch: 238, Loss: 5.609848017229524e-07\n",
            "Epoch: 239, Loss: 5.587934310824494e-07\n",
            "Epoch: 240, Loss: 5.574786428041989e-07\n",
            "Epoch: 241, Loss: 5.548490094042791e-07\n",
            "Epoch: 242, Loss: 5.566021172853652e-07\n",
            "Epoch: 243, Loss: 5.522193760043592e-07\n",
            "Epoch: 244, Loss: 5.52657638763776e-07\n",
            "Epoch: 245, Loss: 5.500280622072751e-07\n",
            "Epoch: 246, Loss: 5.535342211260286e-07\n",
            "Epoch: 247, Loss: 5.487132170856057e-07\n",
            "Epoch: 248, Loss: 5.509045877261087e-07\n",
            "Epoch: 249, Loss: 5.452070581668522e-07\n",
            "Epoch: 250, Loss: 5.500280622072751e-07\n",
            "Epoch: 251, Loss: 5.438922698886017e-07\n",
            "Epoch: 252, Loss: 5.465219032885216e-07\n",
            "Epoch: 253, Loss: 5.465219032885216e-07\n",
            "Epoch: 254, Loss: 5.478366915667721e-07\n",
            "Epoch: 255, Loss: 5.465219032885216e-07\n",
            "Epoch: 256, Loss: 5.421392188509344e-07\n",
            "Epoch: 257, Loss: 5.430157443697681e-07\n",
            "Epoch: 258, Loss: 5.460836405291047e-07\n",
            "Epoch: 259, Loss: 5.434540071291849e-07\n",
            "Epoch: 260, Loss: 5.443305326480186e-07\n",
            "Epoch: 261, Loss: 5.386330599321809e-07\n",
            "Epoch: 262, Loss: 5.456453777696879e-07\n",
            "Epoch: 263, Loss: 5.465219032885216e-07\n",
            "Epoch: 264, Loss: 5.465219032885216e-07\n",
            "Epoch: 265, Loss: 5.390713226915977e-07\n",
            "Epoch: 266, Loss: 5.430157443697681e-07\n",
            "Epoch: 267, Loss: 5.469601660479384e-07\n",
            "Epoch: 268, Loss: 5.417009560915176e-07\n",
            "Epoch: 269, Loss: 5.425774816103512e-07\n",
            "Epoch: 270, Loss: 5.390713226915977e-07\n",
            "Epoch: 271, Loss: 5.500280622072751e-07\n",
            "Epoch: 272, Loss: 5.465219032885216e-07\n",
            "Epoch: 273, Loss: 5.473984288073552e-07\n",
            "Epoch: 274, Loss: 5.421392188509344e-07\n",
            "Epoch: 275, Loss: 5.469601660479384e-07\n",
            "Epoch: 276, Loss: 5.460836405291047e-07\n",
            "Epoch: 277, Loss: 5.438922698886017e-07\n",
            "Epoch: 278, Loss: 5.456453777696879e-07\n",
            "Epoch: 279, Loss: 5.478366915667721e-07\n",
            "Epoch: 280, Loss: 5.452070581668522e-07\n",
            "Epoch: 281, Loss: 5.487132170856057e-07\n",
            "Epoch: 282, Loss: 5.447687954074354e-07\n",
            "Epoch: 283, Loss: 5.447687954074354e-07\n",
            "Epoch: 284, Loss: 5.473984288073552e-07\n",
            "Epoch: 285, Loss: 5.460836405291047e-07\n",
            "Epoch: 286, Loss: 5.447687954074354e-07\n",
            "Epoch: 287, Loss: 5.487132170856057e-07\n",
            "Epoch: 288, Loss: 5.500280622072751e-07\n",
            "Epoch: 289, Loss: 5.491515366884414e-07\n",
            "Epoch: 290, Loss: 5.430157443697681e-07\n",
            "Epoch: 291, Loss: 5.509045877261087e-07\n",
            "Epoch: 292, Loss: 5.487132170856057e-07\n",
            "Epoch: 293, Loss: 5.465219032885216e-07\n",
            "Epoch: 294, Loss: 5.500280622072751e-07\n",
            "Epoch: 295, Loss: 5.500280622072751e-07\n",
            "Epoch: 296, Loss: 5.465219032885216e-07\n",
            "Epoch: 297, Loss: 5.509045877261087e-07\n",
            "Epoch: 298, Loss: 5.504663249666919e-07\n",
            "Epoch: 299, Loss: 5.495897994478582e-07\n",
            "Epoch: 300, Loss: 5.509045877261087e-07\n",
            "Epoch: 301, Loss: 5.495897994478582e-07\n",
            "Epoch: 302, Loss: 5.522193760043592e-07\n",
            "Epoch: 303, Loss: 5.52657638763776e-07\n",
            "Epoch: 304, Loss: 5.469601660479384e-07\n",
            "Epoch: 305, Loss: 5.500280622072751e-07\n",
            "Epoch: 306, Loss: 5.530959583666117e-07\n",
            "Epoch: 307, Loss: 5.517811132449424e-07\n",
            "Epoch: 308, Loss: 5.561637976825295e-07\n",
            "Epoch: 309, Loss: 5.487132170856057e-07\n",
            "Epoch: 310, Loss: 5.495897994478582e-07\n",
            "Epoch: 311, Loss: 5.539724838854454e-07\n",
            "Epoch: 312, Loss: 5.561637976825295e-07\n",
            "Epoch: 313, Loss: 5.517811132449424e-07\n",
            "Epoch: 314, Loss: 5.522193760043592e-07\n",
            "Epoch: 315, Loss: 5.509045877261087e-07\n",
            "Epoch: 316, Loss: 5.570403800447821e-07\n",
            "Epoch: 317, Loss: 5.548490094042791e-07\n",
            "Epoch: 318, Loss: 5.548490094042791e-07\n",
            "Epoch: 319, Loss: 5.504663249666919e-07\n",
            "Epoch: 320, Loss: 5.566021172853652e-07\n",
            "Epoch: 321, Loss: 5.557255349231127e-07\n",
            "Epoch: 322, Loss: 5.579169055636157e-07\n",
            "Epoch: 323, Loss: 5.552872721636959e-07\n",
            "Epoch: 324, Loss: 5.517811132449424e-07\n",
            "Epoch: 325, Loss: 5.592316938418662e-07\n",
            "Epoch: 326, Loss: 5.552872721636959e-07\n",
            "Epoch: 327, Loss: 5.561637976825295e-07\n",
            "Epoch: 328, Loss: 5.583551683230326e-07\n",
            "Epoch: 329, Loss: 5.544107466448622e-07\n",
            "Epoch: 330, Loss: 5.557255349231127e-07\n",
            "Epoch: 331, Loss: 5.583551683230326e-07\n",
            "Epoch: 332, Loss: 5.513428504855256e-07\n",
            "Epoch: 333, Loss: 5.601082193606999e-07\n",
            "Epoch: 334, Loss: 5.631761155200365e-07\n",
            "Epoch: 335, Loss: 5.579169055636157e-07\n",
            "Epoch: 336, Loss: 5.548490094042791e-07\n",
            "Epoch: 337, Loss: 5.662440116793732e-07\n",
            "Epoch: 338, Loss: 5.601082193606999e-07\n",
            "Epoch: 339, Loss: 5.614230644823692e-07\n",
            "Epoch: 340, Loss: 5.640526978822891e-07\n",
            "Epoch: 341, Loss: 5.618613272417861e-07\n",
            "Epoch: 342, Loss: 5.644909606417059e-07\n",
            "Epoch: 343, Loss: 5.636143782794534e-07\n",
            "Epoch: 344, Loss: 5.622995900012029e-07\n",
            "Epoch: 345, Loss: 5.640526978822891e-07\n",
            "Epoch: 346, Loss: 5.631761155200365e-07\n",
            "Epoch: 347, Loss: 5.6668227443879e-07\n",
            "Epoch: 348, Loss: 5.653674861605396e-07\n",
            "Epoch: 349, Loss: 5.684353823198762e-07\n",
            "Epoch: 350, Loss: 5.618613272417861e-07\n",
            "Epoch: 351, Loss: 5.715032784792129e-07\n",
            "Epoch: 352, Loss: 5.715032784792129e-07\n",
            "Epoch: 353, Loss: 5.644909606417059e-07\n",
            "Epoch: 354, Loss: 5.719415412386297e-07\n",
            "Epoch: 355, Loss: 5.688736450792931e-07\n",
            "Epoch: 356, Loss: 5.684353823198762e-07\n",
            "Epoch: 357, Loss: 5.644909606417059e-07\n",
            "Epoch: 358, Loss: 5.723798039980466e-07\n",
            "Epoch: 359, Loss: 5.653674861605396e-07\n",
            "Epoch: 360, Loss: 5.868427024324774e-07\n",
            "Epoch: 361, Loss: 5.776390139544674e-07\n",
            "Epoch: 362, Loss: 5.728180667574634e-07\n",
            "Epoch: 363, Loss: 5.693119078387099e-07\n",
            "Epoch: 364, Loss: 5.693119078387099e-07\n",
            "Epoch: 365, Loss: 5.728180667574634e-07\n",
            "Epoch: 366, Loss: 5.754477001573832e-07\n",
            "Epoch: 367, Loss: 5.6668227443879e-07\n",
            "Epoch: 368, Loss: 5.715032784792129e-07\n",
            "Epoch: 369, Loss: 5.741328550357139e-07\n",
            "Epoch: 370, Loss: 5.719415412386297e-07\n",
            "Epoch: 371, Loss: 5.772007511950505e-07\n",
            "Epoch: 372, Loss: 5.697501705981267e-07\n",
            "Epoch: 373, Loss: 5.710649588763772e-07\n",
            "Epoch: 374, Loss: 5.767624884356337e-07\n",
            "Epoch: 375, Loss: 5.688736450792931e-07\n",
            "Epoch: 376, Loss: 5.80706910113804e-07\n",
            "Epoch: 377, Loss: 5.820216983920545e-07\n",
            "Epoch: 378, Loss: 6.004289616612368e-07\n",
            "Epoch: 379, Loss: 5.855278004673892e-07\n",
            "Epoch: 380, Loss: 5.842130121891387e-07\n",
            "Epoch: 381, Loss: 5.798303845949704e-07\n",
            "Epoch: 382, Loss: 5.772007511950505e-07\n",
            "Epoch: 383, Loss: 5.767624884356337e-07\n",
            "Epoch: 384, Loss: 5.763242256762169e-07\n",
            "Epoch: 385, Loss: 5.776390139544674e-07\n",
            "Epoch: 386, Loss: 5.741328550357139e-07\n",
            "Epoch: 387, Loss: 5.789538590761367e-07\n",
            "Epoch: 388, Loss: 5.767624884356337e-07\n",
            "Epoch: 389, Loss: 5.789538590761367e-07\n",
            "Epoch: 390, Loss: 5.824599611514714e-07\n",
            "Epoch: 391, Loss: 5.78515539473301e-07\n",
            "Epoch: 392, Loss: 5.80706910113804e-07\n",
            "Epoch: 393, Loss: 5.802686473543872e-07\n",
            "Epoch: 394, Loss: 5.868427024324774e-07\n",
            "Epoch: 395, Loss: 5.80706910113804e-07\n",
            "Epoch: 396, Loss: 5.824599611514714e-07\n",
            "Epoch: 397, Loss: 5.798303845949704e-07\n",
            "Epoch: 398, Loss: 5.837748062731407e-07\n",
            "Epoch: 399, Loss: 5.846513317919744e-07\n",
            "Epoch: 400, Loss: 5.82898280754307e-07\n",
            "Epoch: 401, Loss: 5.815834356326377e-07\n",
            "Epoch: 402, Loss: 5.85527857310808e-07\n",
            "Epoch: 403, Loss: 5.85527857310808e-07\n",
            "Epoch: 404, Loss: 5.87719227951311e-07\n",
            "Epoch: 405, Loss: 5.811451728732209e-07\n",
            "Epoch: 406, Loss: 5.850895945513912e-07\n",
            "Epoch: 407, Loss: 5.881574907107279e-07\n",
            "Epoch: 408, Loss: 5.894722789889784e-07\n",
            "Epoch: 409, Loss: 5.842130690325575e-07\n",
            "Epoch: 410, Loss: 5.903488613512309e-07\n",
            "Epoch: 411, Loss: 5.824599611514714e-07\n",
            "Epoch: 412, Loss: 5.885957534701447e-07\n",
            "Epoch: 413, Loss: 5.903488613512309e-07\n",
            "Epoch: 414, Loss: 5.850895945513912e-07\n",
            "Epoch: 415, Loss: 5.85527857310808e-07\n",
            "Epoch: 416, Loss: 5.903488613512309e-07\n",
            "Epoch: 417, Loss: 5.885957534701447e-07\n",
            "Epoch: 418, Loss: 5.899105417483952e-07\n",
            "Epoch: 419, Loss: 5.894722789889784e-07\n",
            "Epoch: 420, Loss: 5.881574907107279e-07\n",
            "Epoch: 421, Loss: 5.885957534701447e-07\n",
            "Epoch: 422, Loss: 5.890340162295615e-07\n",
            "Epoch: 423, Loss: 5.921019123888982e-07\n",
            "Epoch: 424, Loss: 5.837748062731407e-07\n",
            "Epoch: 425, Loss: 5.912253868700645e-07\n",
            "Epoch: 426, Loss: 5.881574907107279e-07\n",
            "Epoch: 427, Loss: 5.916636496294814e-07\n",
            "Epoch: 428, Loss: 5.885957534701447e-07\n",
            "Epoch: 429, Loss: 5.899105417483952e-07\n",
            "Epoch: 430, Loss: 5.903488613512309e-07\n",
            "Epoch: 431, Loss: 5.87719227951311e-07\n",
            "Epoch: 432, Loss: 5.951698085482349e-07\n",
            "Epoch: 433, Loss: 5.885957534701447e-07\n",
            "Epoch: 434, Loss: 5.929784379077319e-07\n",
            "Epoch: 435, Loss: 5.92540175148315e-07\n",
            "Epoch: 436, Loss: 5.929784379077319e-07\n",
            "Epoch: 437, Loss: 5.859661200702249e-07\n",
            "Epoch: 438, Loss: 5.938550202699844e-07\n",
            "Epoch: 439, Loss: 5.890340162295615e-07\n",
            "Epoch: 440, Loss: 5.95169751704816e-07\n",
            "Epoch: 441, Loss: 5.890340162295615e-07\n",
            "Epoch: 442, Loss: 5.885957534701447e-07\n",
            "Epoch: 443, Loss: 5.960463340670685e-07\n",
            "Epoch: 444, Loss: 5.92540175148315e-07\n",
            "Epoch: 445, Loss: 5.942932830294012e-07\n",
            "Epoch: 446, Loss: 5.890340162295615e-07\n",
            "Epoch: 447, Loss: 6.008672244206537e-07\n",
            "Epoch: 448, Loss: 5.890339593861427e-07\n",
            "Epoch: 449, Loss: 5.890340162295615e-07\n",
            "Epoch: 450, Loss: 5.916636496294814e-07\n",
            "Epoch: 451, Loss: 5.94731545788818e-07\n",
            "Epoch: 452, Loss: 5.903488613512309e-07\n",
            "Epoch: 453, Loss: 5.942932830294012e-07\n",
            "Epoch: 454, Loss: 5.960463340670685e-07\n",
            "Epoch: 455, Loss: 5.890340162295615e-07\n",
            "Epoch: 456, Loss: 5.899105417483952e-07\n",
            "Epoch: 457, Loss: 5.938550202699844e-07\n",
            "Epoch: 458, Loss: 5.921018555454793e-07\n",
            "Epoch: 459, Loss: 5.969228027424833e-07\n",
            "Epoch: 460, Loss: 5.951698085482349e-07\n",
            "Epoch: 461, Loss: 5.995524361424032e-07\n",
            "Epoch: 462, Loss: 5.97361122345319e-07\n",
            "Epoch: 463, Loss: 5.964845399830665e-07\n",
            "Epoch: 464, Loss: 5.942932830294012e-07\n",
            "Epoch: 465, Loss: 5.97361122345319e-07\n",
            "Epoch: 466, Loss: 5.938550202699844e-07\n",
            "Epoch: 467, Loss: 5.991142302264052e-07\n",
            "Epoch: 468, Loss: 5.956080713076517e-07\n",
            "Epoch: 469, Loss: 6.008672812640725e-07\n",
            "Epoch: 470, Loss: 5.951698085482349e-07\n",
            "Epoch: 471, Loss: 5.977994419481547e-07\n",
            "Epoch: 472, Loss: 5.982377047075715e-07\n",
            "Epoch: 473, Loss: 5.977994419481547e-07\n",
            "Epoch: 474, Loss: 5.934167006671487e-07\n",
            "Epoch: 475, Loss: 6.013055440234893e-07\n",
            "Epoch: 476, Loss: 5.9999069890182e-07\n",
            "Epoch: 477, Loss: 5.986759674669884e-07\n",
            "Epoch: 478, Loss: 5.95169751704816e-07\n",
            "Epoch: 479, Loss: 6.030586519045755e-07\n",
            "Epoch: 480, Loss: 6.004289616612368e-07\n",
            "Epoch: 481, Loss: 5.964845968264854e-07\n",
            "Epoch: 482, Loss: 6.013055440234893e-07\n",
            "Epoch: 483, Loss: 5.991141733829863e-07\n",
            "Epoch: 484, Loss: 5.969228595859022e-07\n",
            "Epoch: 485, Loss: 6.004290185046557e-07\n",
            "Epoch: 486, Loss: 6.004289616612368e-07\n",
            "Epoch: 487, Loss: 6.030585950611567e-07\n",
            "Epoch: 488, Loss: 5.942932261859823e-07\n",
            "Epoch: 489, Loss: 6.004290185046557e-07\n",
            "Epoch: 490, Loss: 5.999907557452389e-07\n",
            "Epoch: 491, Loss: 5.97361122345319e-07\n",
            "Epoch: 492, Loss: 5.982377047075715e-07\n",
            "Epoch: 493, Loss: 6.065647539799102e-07\n",
            "Epoch: 494, Loss: 5.995524361424032e-07\n",
            "Epoch: 495, Loss: 5.960462772236497e-07\n",
            "Epoch: 496, Loss: 6.013055440234893e-07\n",
            "Epoch: 497, Loss: 5.97361122345319e-07\n",
            "Epoch: 498, Loss: 5.94731545788818e-07\n",
            "Epoch: 499, Loss: 6.004289616612368e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 block size, 64 embed size, tanh"
      ],
      "metadata": {
        "id": "gEFn689J0_bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 2]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size = 5  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 64\n",
        "activation = 'tanh'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(500):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_4': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'trained_model.pth')"
      ],
      "metadata": {
        "id": "Na-bo278TKb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ebc20d-fabd-4483-f7a6-36269ba36bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.516241073608398\n",
            "Epoch: 1, Loss: 7.718062400817871\n",
            "Epoch: 2, Loss: 7.1289143562316895\n",
            "Epoch: 3, Loss: 6.616388320922852\n",
            "Epoch: 4, Loss: 6.157711982727051\n",
            "Epoch: 5, Loss: 5.755841255187988\n",
            "Epoch: 6, Loss: 5.386589527130127\n",
            "Epoch: 7, Loss: 5.033901691436768\n",
            "Epoch: 8, Loss: 4.7020111083984375\n",
            "Epoch: 9, Loss: 4.394742012023926\n",
            "Epoch: 10, Loss: 4.113502502441406\n",
            "Epoch: 11, Loss: 3.8550710678100586\n",
            "Epoch: 12, Loss: 3.6159396171569824\n",
            "Epoch: 13, Loss: 3.393036127090454\n",
            "Epoch: 14, Loss: 3.1850616931915283\n",
            "Epoch: 15, Loss: 2.9908998012542725\n",
            "Epoch: 16, Loss: 2.810222625732422\n",
            "Epoch: 17, Loss: 2.6422717571258545\n",
            "Epoch: 18, Loss: 2.486422300338745\n",
            "Epoch: 19, Loss: 2.3414251804351807\n",
            "Epoch: 20, Loss: 2.2064359188079834\n",
            "Epoch: 21, Loss: 2.080505132675171\n",
            "Epoch: 22, Loss: 1.9633115530014038\n",
            "Epoch: 23, Loss: 1.8541810512542725\n",
            "Epoch: 24, Loss: 1.7529630661010742\n",
            "Epoch: 25, Loss: 1.6585289239883423\n",
            "Epoch: 26, Loss: 1.570908546447754\n",
            "Epoch: 27, Loss: 1.4880222082138062\n",
            "Epoch: 28, Loss: 1.415982723236084\n",
            "Epoch: 29, Loss: 1.3400115966796875\n",
            "Epoch: 30, Loss: 1.2730616331100464\n",
            "Epoch: 31, Loss: 1.2049150466918945\n",
            "Epoch: 32, Loss: 1.1432546377182007\n",
            "Epoch: 33, Loss: 1.0851606130599976\n",
            "Epoch: 34, Loss: 1.0308709144592285\n",
            "Epoch: 35, Loss: 0.9795024394989014\n",
            "Epoch: 36, Loss: 0.9312551617622375\n",
            "Epoch: 37, Loss: 0.8854357004165649\n",
            "Epoch: 38, Loss: 0.8420610427856445\n",
            "Epoch: 39, Loss: 0.8009296655654907\n",
            "Epoch: 40, Loss: 0.7622057199478149\n",
            "Epoch: 41, Loss: 0.7259377241134644\n",
            "Epoch: 42, Loss: 0.6925697922706604\n",
            "Epoch: 43, Loss: 0.6604176759719849\n",
            "Epoch: 44, Loss: 0.6304696202278137\n",
            "Epoch: 45, Loss: 0.6019214391708374\n",
            "Epoch: 46, Loss: 0.5750976800918579\n",
            "Epoch: 47, Loss: 0.5502203106880188\n",
            "Epoch: 48, Loss: 0.5262136459350586\n",
            "Epoch: 49, Loss: 0.5040080547332764\n",
            "Epoch: 50, Loss: 0.4820227324962616\n",
            "Epoch: 51, Loss: 0.46182090044021606\n",
            "Epoch: 52, Loss: 0.4416595995426178\n",
            "Epoch: 53, Loss: 0.42333081364631653\n",
            "Epoch: 54, Loss: 0.40501588582992554\n",
            "Epoch: 55, Loss: 0.3883014917373657\n",
            "Epoch: 56, Loss: 0.3717128038406372\n",
            "Epoch: 57, Loss: 0.35648199915885925\n",
            "Epoch: 58, Loss: 0.34120914340019226\n",
            "Epoch: 59, Loss: 0.3272533118724823\n",
            "Epoch: 60, Loss: 0.31288450956344604\n",
            "Epoch: 61, Loss: 0.300340861082077\n",
            "Epoch: 62, Loss: 0.286798357963562\n",
            "Epoch: 63, Loss: 0.2756659984588623\n",
            "Epoch: 64, Loss: 0.26291781663894653\n",
            "Epoch: 65, Loss: 0.25304046273231506\n",
            "Epoch: 66, Loss: 0.24123641848564148\n",
            "Epoch: 67, Loss: 0.23258818686008453\n",
            "Epoch: 68, Loss: 0.22197356820106506\n",
            "Epoch: 69, Loss: 0.2144520878791809\n",
            "Epoch: 70, Loss: 0.20476959645748138\n",
            "Epoch: 71, Loss: 0.197986900806427\n",
            "Epoch: 72, Loss: 0.1889946609735489\n",
            "Epoch: 73, Loss: 0.1828816533088684\n",
            "Epoch: 74, Loss: 0.17468297481536865\n",
            "Epoch: 75, Loss: 0.16929461061954498\n",
            "Epoch: 76, Loss: 0.1618751883506775\n",
            "Epoch: 77, Loss: 0.15711747109889984\n",
            "Epoch: 78, Loss: 0.15046632289886475\n",
            "Epoch: 79, Loss: 0.1460287719964981\n",
            "Epoch: 80, Loss: 0.13931378722190857\n",
            "Epoch: 81, Loss: 0.13542519509792328\n",
            "Epoch: 82, Loss: 0.12939444184303284\n",
            "Epoch: 83, Loss: 0.12613047659397125\n",
            "Epoch: 84, Loss: 0.12067367881536484\n",
            "Epoch: 85, Loss: 0.11810224503278732\n",
            "Epoch: 86, Loss: 0.11300817131996155\n",
            "Epoch: 87, Loss: 0.11084333062171936\n",
            "Epoch: 88, Loss: 0.10640813410282135\n",
            "Epoch: 89, Loss: 0.10444161295890808\n",
            "Epoch: 90, Loss: 0.10014189034700394\n",
            "Epoch: 91, Loss: 0.09831713885068893\n",
            "Epoch: 92, Loss: 0.09454230219125748\n",
            "Epoch: 93, Loss: 0.09271658957004547\n",
            "Epoch: 94, Loss: 0.08880221843719482\n",
            "Epoch: 95, Loss: 0.08720164746046066\n",
            "Epoch: 96, Loss: 0.08363046497106552\n",
            "Epoch: 97, Loss: 0.08205032348632812\n",
            "Epoch: 98, Loss: 0.07840714603662491\n",
            "Epoch: 99, Loss: 0.0768434926867485\n",
            "Epoch: 100, Loss: 0.07372095435857773\n",
            "Epoch: 101, Loss: 0.07214099913835526\n",
            "Epoch: 102, Loss: 0.06901990622282028\n",
            "Epoch: 103, Loss: 0.06782501190900803\n",
            "Epoch: 104, Loss: 0.06511901319026947\n",
            "Epoch: 105, Loss: 0.06394795328378677\n",
            "Epoch: 106, Loss: 0.06122875586152077\n",
            "Epoch: 107, Loss: 0.06015181168913841\n",
            "Epoch: 108, Loss: 0.058162517845630646\n",
            "Epoch: 109, Loss: 0.05703890323638916\n",
            "Epoch: 110, Loss: 0.05516335368156433\n",
            "Epoch: 111, Loss: 0.054041530936956406\n",
            "Epoch: 112, Loss: 0.05266973748803139\n",
            "Epoch: 113, Loss: 0.05142989382147789\n",
            "Epoch: 114, Loss: 0.04991907253861427\n",
            "Epoch: 115, Loss: 0.04879231005907059\n",
            "Epoch: 116, Loss: 0.04767727851867676\n",
            "Epoch: 117, Loss: 0.046511225402355194\n",
            "Epoch: 118, Loss: 0.045469727367162704\n",
            "Epoch: 119, Loss: 0.04429085925221443\n",
            "Epoch: 120, Loss: 0.04373376443982124\n",
            "Epoch: 121, Loss: 0.042401235550642014\n",
            "Epoch: 122, Loss: 0.04175775498151779\n",
            "Epoch: 123, Loss: 0.040416419506073\n",
            "Epoch: 124, Loss: 0.04010746255517006\n",
            "Epoch: 125, Loss: 0.038689542561769485\n",
            "Epoch: 126, Loss: 0.03822164610028267\n",
            "Epoch: 127, Loss: 0.036901697516441345\n",
            "Epoch: 128, Loss: 0.036724407225847244\n",
            "Epoch: 129, Loss: 0.03547058254480362\n",
            "Epoch: 130, Loss: 0.03506859764456749\n",
            "Epoch: 131, Loss: 0.03406175225973129\n",
            "Epoch: 132, Loss: 0.03387315198779106\n",
            "Epoch: 133, Loss: 0.03294381871819496\n",
            "Epoch: 134, Loss: 0.03254409506917\n",
            "Epoch: 135, Loss: 0.0317886583507061\n",
            "Epoch: 136, Loss: 0.03172248601913452\n",
            "Epoch: 137, Loss: 0.03096696548163891\n",
            "Epoch: 138, Loss: 0.030659589916467667\n",
            "Epoch: 139, Loss: 0.030041376128792763\n",
            "Epoch: 140, Loss: 0.029946496710181236\n",
            "Epoch: 141, Loss: 0.029325785115361214\n",
            "Epoch: 142, Loss: 0.02899211458861828\n",
            "Epoch: 143, Loss: 0.02840849943459034\n",
            "Epoch: 144, Loss: 0.028391681611537933\n",
            "Epoch: 145, Loss: 0.02772076055407524\n",
            "Epoch: 146, Loss: 0.02761836163699627\n",
            "Epoch: 147, Loss: 0.026877427473664284\n",
            "Epoch: 148, Loss: 0.027306903153657913\n",
            "Epoch: 149, Loss: 0.026264114305377007\n",
            "Epoch: 150, Loss: 0.026826122775673866\n",
            "Epoch: 151, Loss: 0.025502726435661316\n",
            "Epoch: 152, Loss: 0.026636457070708275\n",
            "Epoch: 153, Loss: 0.025005098432302475\n",
            "Epoch: 154, Loss: 0.026097837835550308\n",
            "Epoch: 155, Loss: 0.024392174556851387\n",
            "Epoch: 156, Loss: 0.02560606598854065\n",
            "Epoch: 157, Loss: 0.02388269640505314\n",
            "Epoch: 158, Loss: 0.025003276765346527\n",
            "Epoch: 159, Loss: 0.0232803113758564\n",
            "Epoch: 160, Loss: 0.02459627576172352\n",
            "Epoch: 161, Loss: 0.022932199761271477\n",
            "Epoch: 162, Loss: 0.023794732987880707\n",
            "Epoch: 163, Loss: 0.02239816263318062\n",
            "Epoch: 164, Loss: 0.023038843646645546\n",
            "Epoch: 165, Loss: 0.02177371457219124\n",
            "Epoch: 166, Loss: 0.022599462419748306\n",
            "Epoch: 167, Loss: 0.021237680688500404\n",
            "Epoch: 168, Loss: 0.022319337353110313\n",
            "Epoch: 169, Loss: 0.02091209404170513\n",
            "Epoch: 170, Loss: 0.021915793418884277\n",
            "Epoch: 171, Loss: 0.02055475488305092\n",
            "Epoch: 172, Loss: 0.021431975066661835\n",
            "Epoch: 173, Loss: 0.02020234614610672\n",
            "Epoch: 174, Loss: 0.021061327308416367\n",
            "Epoch: 175, Loss: 0.019793961197137833\n",
            "Epoch: 176, Loss: 0.020632440224289894\n",
            "Epoch: 177, Loss: 0.019493456929922104\n",
            "Epoch: 178, Loss: 0.020319363102316856\n",
            "Epoch: 179, Loss: 0.019222620874643326\n",
            "Epoch: 180, Loss: 0.01985148899257183\n",
            "Epoch: 181, Loss: 0.018926000222563744\n",
            "Epoch: 182, Loss: 0.019611869007349014\n",
            "Epoch: 183, Loss: 0.018626362085342407\n",
            "Epoch: 184, Loss: 0.019359124824404716\n",
            "Epoch: 185, Loss: 0.018433326855301857\n",
            "Epoch: 186, Loss: 0.019030513241887093\n",
            "Epoch: 187, Loss: 0.01821850799024105\n",
            "Epoch: 188, Loss: 0.018704324960708618\n",
            "Epoch: 189, Loss: 0.018000435084104538\n",
            "Epoch: 190, Loss: 0.018416933715343475\n",
            "Epoch: 191, Loss: 0.0177138801664114\n",
            "Epoch: 192, Loss: 0.018368197605013847\n",
            "Epoch: 193, Loss: 0.017599811777472496\n",
            "Epoch: 194, Loss: 0.01795736327767372\n",
            "Epoch: 195, Loss: 0.017407670617103577\n",
            "Epoch: 196, Loss: 0.01772625744342804\n",
            "Epoch: 197, Loss: 0.017240585759282112\n",
            "Epoch: 198, Loss: 0.017410501837730408\n",
            "Epoch: 199, Loss: 0.016967684030532837\n",
            "Epoch: 200, Loss: 0.01737215556204319\n",
            "Epoch: 201, Loss: 0.01689607836306095\n",
            "Epoch: 202, Loss: 0.01696692407131195\n",
            "Epoch: 203, Loss: 0.016690216958522797\n",
            "Epoch: 204, Loss: 0.01688223145902157\n",
            "Epoch: 205, Loss: 0.01660996489226818\n",
            "Epoch: 206, Loss: 0.01651114411652088\n",
            "Epoch: 207, Loss: 0.016337912529706955\n",
            "Epoch: 208, Loss: 0.01653057150542736\n",
            "Epoch: 209, Loss: 0.016267715021967888\n",
            "Epoch: 210, Loss: 0.01616649329662323\n",
            "Epoch: 211, Loss: 0.016114573925733566\n",
            "Epoch: 212, Loss: 0.016042815521359444\n",
            "Epoch: 213, Loss: 0.015917178243398666\n",
            "Epoch: 214, Loss: 0.01598741114139557\n",
            "Epoch: 215, Loss: 0.01569162681698799\n",
            "Epoch: 216, Loss: 0.01586742512881756\n",
            "Epoch: 217, Loss: 0.01545694936066866\n",
            "Epoch: 218, Loss: 0.015670081600546837\n",
            "Epoch: 219, Loss: 0.015320247039198875\n",
            "Epoch: 220, Loss: 0.015437119640409946\n",
            "Epoch: 221, Loss: 0.01532763708382845\n",
            "Epoch: 222, Loss: 0.015178900212049484\n",
            "Epoch: 223, Loss: 0.015352276153862476\n",
            "Epoch: 224, Loss: 0.015317128039896488\n",
            "Epoch: 225, Loss: 0.015727674588561058\n",
            "Epoch: 226, Loss: 0.015174989588558674\n",
            "Epoch: 227, Loss: 0.015291529707610607\n",
            "Epoch: 228, Loss: 0.014730695635080338\n",
            "Epoch: 229, Loss: 0.015178503468632698\n",
            "Epoch: 230, Loss: 0.014478371478617191\n",
            "Epoch: 231, Loss: 0.015283291228115559\n",
            "Epoch: 232, Loss: 0.014536923728883266\n",
            "Epoch: 233, Loss: 0.01563897542655468\n",
            "Epoch: 234, Loss: 0.01433136872947216\n",
            "Epoch: 235, Loss: 0.015218466520309448\n",
            "Epoch: 236, Loss: 0.014356291852891445\n",
            "Epoch: 237, Loss: 0.014916501939296722\n",
            "Epoch: 238, Loss: 0.01394280232489109\n",
            "Epoch: 239, Loss: 0.014491703361272812\n",
            "Epoch: 240, Loss: 0.014264512807130814\n",
            "Epoch: 241, Loss: 0.014434336684644222\n",
            "Epoch: 242, Loss: 0.0141372699290514\n",
            "Epoch: 243, Loss: 0.01434618141502142\n",
            "Epoch: 244, Loss: 0.014268415048718452\n",
            "Epoch: 245, Loss: 0.014169932343065739\n",
            "Epoch: 246, Loss: 0.014082476496696472\n",
            "Epoch: 247, Loss: 0.014465154148638248\n",
            "Epoch: 248, Loss: 0.014039737172424793\n",
            "Epoch: 249, Loss: 0.014542040415108204\n",
            "Epoch: 250, Loss: 0.013678589835762978\n",
            "Epoch: 251, Loss: 0.014074064791202545\n",
            "Epoch: 252, Loss: 0.013832340948283672\n",
            "Epoch: 253, Loss: 0.014177452772855759\n",
            "Epoch: 254, Loss: 0.013627104461193085\n",
            "Epoch: 255, Loss: 0.01414540596306324\n",
            "Epoch: 256, Loss: 0.01378820650279522\n",
            "Epoch: 257, Loss: 0.014105376787483692\n",
            "Epoch: 258, Loss: 0.013722008094191551\n",
            "Epoch: 259, Loss: 0.013874571770429611\n",
            "Epoch: 260, Loss: 0.013846006244421005\n",
            "Epoch: 261, Loss: 0.014049144461750984\n",
            "Epoch: 262, Loss: 0.013538509607315063\n",
            "Epoch: 263, Loss: 0.014086639508605003\n",
            "Epoch: 264, Loss: 0.013438242487609386\n",
            "Epoch: 265, Loss: 0.013773092068731785\n",
            "Epoch: 266, Loss: 0.013455713167786598\n",
            "Epoch: 267, Loss: 0.01386154256761074\n",
            "Epoch: 268, Loss: 0.013333176262676716\n",
            "Epoch: 269, Loss: 0.014089561067521572\n",
            "Epoch: 270, Loss: 0.013054254464805126\n",
            "Epoch: 271, Loss: 0.014013079926371574\n",
            "Epoch: 272, Loss: 0.0132015161216259\n",
            "Epoch: 273, Loss: 0.01399591937661171\n",
            "Epoch: 274, Loss: 0.013119758106768131\n",
            "Epoch: 275, Loss: 0.013920650817453861\n",
            "Epoch: 276, Loss: 0.013302396051585674\n",
            "Epoch: 277, Loss: 0.01387254148721695\n",
            "Epoch: 278, Loss: 0.013210915960371494\n",
            "Epoch: 279, Loss: 0.013721931725740433\n",
            "Epoch: 280, Loss: 0.01321681123226881\n",
            "Epoch: 281, Loss: 0.01378186047077179\n",
            "Epoch: 282, Loss: 0.01304619014263153\n",
            "Epoch: 283, Loss: 0.013712622225284576\n",
            "Epoch: 284, Loss: 0.013145425356924534\n",
            "Epoch: 285, Loss: 0.01376537699252367\n",
            "Epoch: 286, Loss: 0.013022927567362785\n",
            "Epoch: 287, Loss: 0.013663147576153278\n",
            "Epoch: 288, Loss: 0.013192807324230671\n",
            "Epoch: 289, Loss: 0.013743099756538868\n",
            "Epoch: 290, Loss: 0.012982629239559174\n",
            "Epoch: 291, Loss: 0.013690012507140636\n",
            "Epoch: 292, Loss: 0.012984316796064377\n",
            "Epoch: 293, Loss: 0.013688396662473679\n",
            "Epoch: 294, Loss: 0.0128936143592\n",
            "Epoch: 295, Loss: 0.01359727792441845\n",
            "Epoch: 296, Loss: 0.012984040193259716\n",
            "Epoch: 297, Loss: 0.013697315007448196\n",
            "Epoch: 298, Loss: 0.012865367345511913\n",
            "Epoch: 299, Loss: 0.01358408760279417\n",
            "Epoch: 300, Loss: 0.012993572279810905\n",
            "Epoch: 301, Loss: 0.0136362100020051\n",
            "Epoch: 302, Loss: 0.01278857421129942\n",
            "Epoch: 303, Loss: 0.01355040818452835\n",
            "Epoch: 304, Loss: 0.012851301580667496\n",
            "Epoch: 305, Loss: 0.013655015267431736\n",
            "Epoch: 306, Loss: 0.012649407610297203\n",
            "Epoch: 307, Loss: 0.013543221168220043\n",
            "Epoch: 308, Loss: 0.012848027050495148\n",
            "Epoch: 309, Loss: 0.013580643571913242\n",
            "Epoch: 310, Loss: 0.012702814303338528\n",
            "Epoch: 311, Loss: 0.013515462167561054\n",
            "Epoch: 312, Loss: 0.012767298147082329\n",
            "Epoch: 313, Loss: 0.013589310459792614\n",
            "Epoch: 314, Loss: 0.012619652785360813\n",
            "Epoch: 315, Loss: 0.013478122651576996\n",
            "Epoch: 316, Loss: 0.012751315720379353\n",
            "Epoch: 317, Loss: 0.013563767075538635\n",
            "Epoch: 318, Loss: 0.012571544386446476\n",
            "Epoch: 319, Loss: 0.013529288582503796\n",
            "Epoch: 320, Loss: 0.012683681212365627\n",
            "Epoch: 321, Loss: 0.013509627431631088\n",
            "Epoch: 322, Loss: 0.012600747868418694\n",
            "Epoch: 323, Loss: 0.013388008810579777\n",
            "Epoch: 324, Loss: 0.012749128974974155\n",
            "Epoch: 325, Loss: 0.01345789898186922\n",
            "Epoch: 326, Loss: 0.012596599757671356\n",
            "Epoch: 327, Loss: 0.01339074969291687\n",
            "Epoch: 328, Loss: 0.01264697965234518\n",
            "Epoch: 329, Loss: 0.013422610238194466\n",
            "Epoch: 330, Loss: 0.012537168338894844\n",
            "Epoch: 331, Loss: 0.013328755274415016\n",
            "Epoch: 332, Loss: 0.012673607096076012\n",
            "Epoch: 333, Loss: 0.013400736264884472\n",
            "Epoch: 334, Loss: 0.012524822726845741\n",
            "Epoch: 335, Loss: 0.013320556841790676\n",
            "Epoch: 336, Loss: 0.012621304951608181\n",
            "Epoch: 337, Loss: 0.013364334590733051\n",
            "Epoch: 338, Loss: 0.012518690899014473\n",
            "Epoch: 339, Loss: 0.013267550617456436\n",
            "Epoch: 340, Loss: 0.0126369409263134\n",
            "Epoch: 341, Loss: 0.013336991891264915\n",
            "Epoch: 342, Loss: 0.01246930193156004\n",
            "Epoch: 343, Loss: 0.01328293327242136\n",
            "Epoch: 344, Loss: 0.012565622106194496\n",
            "Epoch: 345, Loss: 0.013318891637027264\n",
            "Epoch: 346, Loss: 0.012453373521566391\n",
            "Epoch: 347, Loss: 0.01324150338768959\n",
            "Epoch: 348, Loss: 0.012606585398316383\n",
            "Epoch: 349, Loss: 0.013312391936779022\n",
            "Epoch: 350, Loss: 0.012436943128705025\n",
            "Epoch: 351, Loss: 0.013238965533673763\n",
            "Epoch: 352, Loss: 0.012540425173938274\n",
            "Epoch: 353, Loss: 0.01328594982624054\n",
            "Epoch: 354, Loss: 0.012434693053364754\n",
            "Epoch: 355, Loss: 0.013239537365734577\n",
            "Epoch: 356, Loss: 0.01257115788757801\n",
            "Epoch: 357, Loss: 0.013279017992317677\n",
            "Epoch: 358, Loss: 0.012419986538589\n",
            "Epoch: 359, Loss: 0.013199266977608204\n",
            "Epoch: 360, Loss: 0.0125369718298316\n",
            "Epoch: 361, Loss: 0.013258028775453568\n",
            "Epoch: 362, Loss: 0.012430681847035885\n",
            "Epoch: 363, Loss: 0.013182414695620537\n",
            "Epoch: 364, Loss: 0.012517914175987244\n",
            "Epoch: 365, Loss: 0.01325002871453762\n",
            "Epoch: 366, Loss: 0.012398666702210903\n",
            "Epoch: 367, Loss: 0.013163442723453045\n",
            "Epoch: 368, Loss: 0.012513430789113045\n",
            "Epoch: 369, Loss: 0.01321737002581358\n",
            "Epoch: 370, Loss: 0.012391177006065845\n",
            "Epoch: 371, Loss: 0.01314330194145441\n",
            "Epoch: 372, Loss: 0.012529658153653145\n",
            "Epoch: 373, Loss: 0.013182466849684715\n",
            "Epoch: 374, Loss: 0.012409868650138378\n",
            "Epoch: 375, Loss: 0.013108144514262676\n",
            "Epoch: 376, Loss: 0.012530901469290257\n",
            "Epoch: 377, Loss: 0.013158813118934631\n",
            "Epoch: 378, Loss: 0.012442477978765965\n",
            "Epoch: 379, Loss: 0.01308410707861185\n",
            "Epoch: 380, Loss: 0.012527702376246452\n",
            "Epoch: 381, Loss: 0.013110587373375893\n",
            "Epoch: 382, Loss: 0.012437584809958935\n",
            "Epoch: 383, Loss: 0.01306170504540205\n",
            "Epoch: 384, Loss: 0.01260534580796957\n",
            "Epoch: 385, Loss: 0.013072400353848934\n",
            "Epoch: 386, Loss: 0.012478192336857319\n",
            "Epoch: 387, Loss: 0.01299767941236496\n",
            "Epoch: 388, Loss: 0.012626695446670055\n",
            "Epoch: 389, Loss: 0.013010765425860882\n",
            "Epoch: 390, Loss: 0.012571989558637142\n",
            "Epoch: 391, Loss: 0.012906702235341072\n",
            "Epoch: 392, Loss: 0.012737948447465897\n",
            "Epoch: 393, Loss: 0.012948361225426197\n",
            "Epoch: 394, Loss: 0.012609388679265976\n",
            "Epoch: 395, Loss: 0.01290726289153099\n",
            "Epoch: 396, Loss: 0.012710658833384514\n",
            "Epoch: 397, Loss: 0.012955283746123314\n",
            "Epoch: 398, Loss: 0.01257003378123045\n",
            "Epoch: 399, Loss: 0.012897534295916557\n",
            "Epoch: 400, Loss: 0.012679590843617916\n",
            "Epoch: 401, Loss: 0.012957085855305195\n",
            "Epoch: 402, Loss: 0.012551813386380672\n",
            "Epoch: 403, Loss: 0.01288756262511015\n",
            "Epoch: 404, Loss: 0.0126862907782197\n",
            "Epoch: 405, Loss: 0.012954965233802795\n",
            "Epoch: 406, Loss: 0.012542535550892353\n",
            "Epoch: 407, Loss: 0.012898157350718975\n",
            "Epoch: 408, Loss: 0.012657455168664455\n",
            "Epoch: 409, Loss: 0.012936617247760296\n",
            "Epoch: 410, Loss: 0.012540469877421856\n",
            "Epoch: 411, Loss: 0.012871254235506058\n",
            "Epoch: 412, Loss: 0.012657992541790009\n",
            "Epoch: 413, Loss: 0.012954174540936947\n",
            "Epoch: 414, Loss: 0.012516723945736885\n",
            "Epoch: 415, Loss: 0.012880004942417145\n",
            "Epoch: 416, Loss: 0.012622000649571419\n",
            "Epoch: 417, Loss: 0.012924912385642529\n",
            "Epoch: 418, Loss: 0.012487820349633694\n",
            "Epoch: 419, Loss: 0.012880039401352406\n",
            "Epoch: 420, Loss: 0.012574426829814911\n",
            "Epoch: 421, Loss: 0.01294692512601614\n",
            "Epoch: 422, Loss: 0.012476989068090916\n",
            "Epoch: 423, Loss: 0.012862708419561386\n",
            "Epoch: 424, Loss: 0.012566329911351204\n",
            "Epoch: 425, Loss: 0.012931887991726398\n",
            "Epoch: 426, Loss: 0.012459294870495796\n",
            "Epoch: 427, Loss: 0.012872826308012009\n",
            "Epoch: 428, Loss: 0.012583096511662006\n",
            "Epoch: 429, Loss: 0.012950723990797997\n",
            "Epoch: 430, Loss: 0.012467925436794758\n",
            "Epoch: 431, Loss: 0.012870986945927143\n",
            "Epoch: 432, Loss: 0.012584445998072624\n",
            "Epoch: 433, Loss: 0.012930866330862045\n",
            "Epoch: 434, Loss: 0.012460747733712196\n",
            "Epoch: 435, Loss: 0.012848774902522564\n",
            "Epoch: 436, Loss: 0.01258787326514721\n",
            "Epoch: 437, Loss: 0.012910565361380577\n",
            "Epoch: 438, Loss: 0.012470902875065804\n",
            "Epoch: 439, Loss: 0.012854769825935364\n",
            "Epoch: 440, Loss: 0.01256487425416708\n",
            "Epoch: 441, Loss: 0.0129164457321167\n",
            "Epoch: 442, Loss: 0.012427113950252533\n",
            "Epoch: 443, Loss: 0.012841295450925827\n",
            "Epoch: 444, Loss: 0.012544958852231503\n",
            "Epoch: 445, Loss: 0.012905762530863285\n",
            "Epoch: 446, Loss: 0.012422400526702404\n",
            "Epoch: 447, Loss: 0.012851603329181671\n",
            "Epoch: 448, Loss: 0.012524370104074478\n",
            "Epoch: 449, Loss: 0.012911851517856121\n",
            "Epoch: 450, Loss: 0.01241844892501831\n",
            "Epoch: 451, Loss: 0.012845084071159363\n",
            "Epoch: 452, Loss: 0.01252154354006052\n",
            "Epoch: 453, Loss: 0.012912915088236332\n",
            "Epoch: 454, Loss: 0.012418530881404877\n",
            "Epoch: 455, Loss: 0.012847606092691422\n",
            "Epoch: 456, Loss: 0.012518413364887238\n",
            "Epoch: 457, Loss: 0.012914009392261505\n",
            "Epoch: 458, Loss: 0.012387516908347607\n",
            "Epoch: 459, Loss: 0.012843869626522064\n",
            "Epoch: 460, Loss: 0.012487691827118397\n",
            "Epoch: 461, Loss: 0.012891939841210842\n",
            "Epoch: 462, Loss: 0.012384825386106968\n",
            "Epoch: 463, Loss: 0.012831784784793854\n",
            "Epoch: 464, Loss: 0.012500429525971413\n",
            "Epoch: 465, Loss: 0.012907076627016068\n",
            "Epoch: 466, Loss: 0.012384566478431225\n",
            "Epoch: 467, Loss: 0.012835586443543434\n",
            "Epoch: 468, Loss: 0.01249280571937561\n",
            "Epoch: 469, Loss: 0.012892798520624638\n",
            "Epoch: 470, Loss: 0.012380344793200493\n",
            "Epoch: 471, Loss: 0.012833425775170326\n",
            "Epoch: 472, Loss: 0.012488052248954773\n",
            "Epoch: 473, Loss: 0.012899529188871384\n",
            "Epoch: 474, Loss: 0.01236790418624878\n",
            "Epoch: 475, Loss: 0.012842027470469475\n",
            "Epoch: 476, Loss: 0.012462932616472244\n",
            "Epoch: 477, Loss: 0.01288362592458725\n",
            "Epoch: 478, Loss: 0.01236522663384676\n",
            "Epoch: 479, Loss: 0.012804420664906502\n",
            "Epoch: 480, Loss: 0.012483017519116402\n",
            "Epoch: 481, Loss: 0.012865500524640083\n",
            "Epoch: 482, Loss: 0.01236171554774046\n",
            "Epoch: 483, Loss: 0.01280845608562231\n",
            "Epoch: 484, Loss: 0.012438966892659664\n",
            "Epoch: 485, Loss: 0.012865377590060234\n",
            "Epoch: 486, Loss: 0.012345296330749989\n",
            "Epoch: 487, Loss: 0.01281039696186781\n",
            "Epoch: 488, Loss: 0.01242933887988329\n",
            "Epoch: 489, Loss: 0.012894093059003353\n",
            "Epoch: 490, Loss: 0.012340463697910309\n",
            "Epoch: 491, Loss: 0.012809683568775654\n",
            "Epoch: 492, Loss: 0.012448507361114025\n",
            "Epoch: 493, Loss: 0.012874861247837543\n",
            "Epoch: 494, Loss: 0.012332573533058167\n",
            "Epoch: 495, Loss: 0.012810437940061092\n",
            "Epoch: 496, Loss: 0.012442367151379585\n",
            "Epoch: 497, Loss: 0.01286387164145708\n",
            "Epoch: 498, Loss: 0.012340379878878593\n",
            "Epoch: 499, Loss: 0.012788059189915657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "block size 10, embed dim 64, tanh"
      ],
      "metadata": {
        "id": "cgIVLy0J1Q3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 2]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size =10  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 64\n",
        "activation = 'tanh'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(500):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_10': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'block_10_emd_64_tanh.pth')"
      ],
      "metadata": {
        "id": "o0_IZcCuTLRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb000c6-2bf1-408a-abca-386ff95e8474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.948491096496582\n",
            "Epoch: 1, Loss: 8.084049224853516\n",
            "Epoch: 2, Loss: 7.408543109893799\n",
            "Epoch: 3, Loss: 6.788146495819092\n",
            "Epoch: 4, Loss: 6.201324939727783\n",
            "Epoch: 5, Loss: 5.641578197479248\n",
            "Epoch: 6, Loss: 5.107080936431885\n",
            "Epoch: 7, Loss: 4.597298622131348\n",
            "Epoch: 8, Loss: 4.115732669830322\n",
            "Epoch: 9, Loss: 3.6675100326538086\n",
            "Epoch: 10, Loss: 3.256507635116577\n",
            "Epoch: 11, Loss: 2.8848979473114014\n",
            "Epoch: 12, Loss: 2.555560827255249\n",
            "Epoch: 13, Loss: 2.2716119289398193\n",
            "Epoch: 14, Loss: 2.024683713912964\n",
            "Epoch: 15, Loss: 1.8077731132507324\n",
            "Epoch: 16, Loss: 1.6157633066177368\n",
            "Epoch: 17, Loss: 1.4451078176498413\n",
            "Epoch: 18, Loss: 1.2932358980178833\n",
            "Epoch: 19, Loss: 1.15825617313385\n",
            "Epoch: 20, Loss: 1.038442611694336\n",
            "Epoch: 21, Loss: 0.9320326447486877\n",
            "Epoch: 22, Loss: 0.8373070359230042\n",
            "Epoch: 23, Loss: 0.7527148723602295\n",
            "Epoch: 24, Loss: 0.6772108674049377\n",
            "Epoch: 25, Loss: 0.6101592183113098\n",
            "Epoch: 26, Loss: 0.5507832765579224\n",
            "Epoch: 27, Loss: 0.4980655908584595\n",
            "Epoch: 28, Loss: 0.4509660601615906\n",
            "Epoch: 29, Loss: 0.4087778627872467\n",
            "Epoch: 30, Loss: 0.3709069490432739\n",
            "Epoch: 31, Loss: 0.33675485849380493\n",
            "Epoch: 32, Loss: 0.3058744966983795\n",
            "Epoch: 33, Loss: 0.27804243564605713\n",
            "Epoch: 34, Loss: 0.2531971335411072\n",
            "Epoch: 35, Loss: 0.23114712536334991\n",
            "Epoch: 36, Loss: 0.2115706503391266\n",
            "Epoch: 37, Loss: 0.1941901445388794\n",
            "Epoch: 38, Loss: 0.17869596183300018\n",
            "Epoch: 39, Loss: 0.16486889123916626\n",
            "Epoch: 40, Loss: 0.15244220197200775\n",
            "Epoch: 41, Loss: 0.1412501186132431\n",
            "Epoch: 42, Loss: 0.1309618353843689\n",
            "Epoch: 43, Loss: 0.12160228937864304\n",
            "Epoch: 44, Loss: 0.11281393468379974\n",
            "Epoch: 45, Loss: 0.1052417978644371\n",
            "Epoch: 46, Loss: 0.09747294336557388\n",
            "Epoch: 47, Loss: 0.09163825958967209\n",
            "Epoch: 48, Loss: 0.08390620350837708\n",
            "Epoch: 49, Loss: 0.07786107808351517\n",
            "Epoch: 50, Loss: 0.07206804305315018\n",
            "Epoch: 51, Loss: 0.06678767502307892\n",
            "Epoch: 52, Loss: 0.06204492971301079\n",
            "Epoch: 53, Loss: 0.057727545499801636\n",
            "Epoch: 54, Loss: 0.05377207323908806\n",
            "Epoch: 55, Loss: 0.0501539446413517\n",
            "Epoch: 56, Loss: 0.04682929441332817\n",
            "Epoch: 57, Loss: 0.043748628348112106\n",
            "Epoch: 58, Loss: 0.04088374599814415\n",
            "Epoch: 59, Loss: 0.03823189064860344\n",
            "Epoch: 60, Loss: 0.03577307239174843\n",
            "Epoch: 61, Loss: 0.03348841518163681\n",
            "Epoch: 62, Loss: 0.0313582569360733\n",
            "Epoch: 63, Loss: 0.029351595789194107\n",
            "Epoch: 64, Loss: 0.027476754039525986\n",
            "Epoch: 65, Loss: 0.025736337527632713\n",
            "Epoch: 66, Loss: 0.024125469848513603\n",
            "Epoch: 67, Loss: 0.022619368508458138\n",
            "Epoch: 68, Loss: 0.021231098100543022\n",
            "Epoch: 69, Loss: 0.01992707885801792\n",
            "Epoch: 70, Loss: 0.018693003803491592\n",
            "Epoch: 71, Loss: 0.017539089545607567\n",
            "Epoch: 72, Loss: 0.01645522005856037\n",
            "Epoch: 73, Loss: 0.015428309328854084\n",
            "Epoch: 74, Loss: 0.014452842995524406\n",
            "Epoch: 75, Loss: 0.013534408994019032\n",
            "Epoch: 76, Loss: 0.012681208550930023\n",
            "Epoch: 77, Loss: 0.011895867064595222\n",
            "Epoch: 78, Loss: 0.011172981932759285\n",
            "Epoch: 79, Loss: 0.01051842886954546\n",
            "Epoch: 80, Loss: 0.009923194535076618\n",
            "Epoch: 81, Loss: 0.009358209557831287\n",
            "Epoch: 82, Loss: 0.008797124959528446\n",
            "Epoch: 83, Loss: 0.00827140361070633\n",
            "Epoch: 84, Loss: 0.007777796592563391\n",
            "Epoch: 85, Loss: 0.007311921566724777\n",
            "Epoch: 86, Loss: 0.006863804534077644\n",
            "Epoch: 87, Loss: 0.0064523047767579556\n",
            "Epoch: 88, Loss: 0.006076773162931204\n",
            "Epoch: 89, Loss: 0.005710702855139971\n",
            "Epoch: 90, Loss: 0.005364995915442705\n",
            "Epoch: 91, Loss: 0.005031122826039791\n",
            "Epoch: 92, Loss: 0.004716914612799883\n",
            "Epoch: 93, Loss: 0.0044183083809912205\n",
            "Epoch: 94, Loss: 0.004142947029322386\n",
            "Epoch: 95, Loss: 0.003875422989949584\n",
            "Epoch: 96, Loss: 0.003620424075052142\n",
            "Epoch: 97, Loss: 0.0033824287820607424\n",
            "Epoch: 98, Loss: 0.0031637551728636026\n",
            "Epoch: 99, Loss: 0.0029449264984577894\n",
            "Epoch: 100, Loss: 0.002759531605988741\n",
            "Epoch: 101, Loss: 0.002568233758211136\n",
            "Epoch: 102, Loss: 0.0024172081612050533\n",
            "Epoch: 103, Loss: 0.002257219050079584\n",
            "Epoch: 104, Loss: 0.002105819061398506\n",
            "Epoch: 105, Loss: 0.001957233063876629\n",
            "Epoch: 106, Loss: 0.0018339193193241954\n",
            "Epoch: 107, Loss: 0.0017140033887699246\n",
            "Epoch: 108, Loss: 0.0016064560040831566\n",
            "Epoch: 109, Loss: 0.0015041131991893053\n",
            "Epoch: 110, Loss: 0.0014101024717092514\n",
            "Epoch: 111, Loss: 0.001321395393460989\n",
            "Epoch: 112, Loss: 0.0012390922056511045\n",
            "Epoch: 113, Loss: 0.0011613790411502123\n",
            "Epoch: 114, Loss: 0.0010903639486059546\n",
            "Epoch: 115, Loss: 0.0010268186451867223\n",
            "Epoch: 116, Loss: 0.0009584051440469921\n",
            "Epoch: 117, Loss: 0.0008978484547697008\n",
            "Epoch: 118, Loss: 0.0008413995965383947\n",
            "Epoch: 119, Loss: 0.0007878124597482383\n",
            "Epoch: 120, Loss: 0.0007377483416348696\n",
            "Epoch: 121, Loss: 0.0006896333652548492\n",
            "Epoch: 122, Loss: 0.0006438472191803157\n",
            "Epoch: 123, Loss: 0.0006008720374666154\n",
            "Epoch: 124, Loss: 0.0005613332032226026\n",
            "Epoch: 125, Loss: 0.0005252548144198954\n",
            "Epoch: 126, Loss: 0.000492416846100241\n",
            "Epoch: 127, Loss: 0.00046181975631043315\n",
            "Epoch: 128, Loss: 0.00043362643918953836\n",
            "Epoch: 129, Loss: 0.00040700563113205135\n",
            "Epoch: 130, Loss: 0.00038225221214815974\n",
            "Epoch: 131, Loss: 0.00035899339127354324\n",
            "Epoch: 132, Loss: 0.0003373667423147708\n",
            "Epoch: 133, Loss: 0.000316834106342867\n",
            "Epoch: 134, Loss: 0.0002979318087454885\n",
            "Epoch: 135, Loss: 0.00028001933242194355\n",
            "Epoch: 136, Loss: 0.00026320700999349356\n",
            "Epoch: 137, Loss: 0.0002473702479619533\n",
            "Epoch: 138, Loss: 0.00023266428615897894\n",
            "Epoch: 139, Loss: 0.00021826601005159318\n",
            "Epoch: 140, Loss: 0.0002052903437288478\n",
            "Epoch: 141, Loss: 0.00019275616796221584\n",
            "Epoch: 142, Loss: 0.00018124227062799037\n",
            "Epoch: 143, Loss: 0.00017013121396303177\n",
            "Epoch: 144, Loss: 0.00016017170855775476\n",
            "Epoch: 145, Loss: 0.00015014249947853386\n",
            "Epoch: 146, Loss: 0.00014156581892166287\n",
            "Epoch: 147, Loss: 0.00013265783491078764\n",
            "Epoch: 148, Loss: 0.00012491557572502643\n",
            "Epoch: 149, Loss: 0.00011703262862283736\n",
            "Epoch: 150, Loss: 0.00011037784133804962\n",
            "Epoch: 151, Loss: 0.00010329542419640347\n",
            "Epoch: 152, Loss: 9.744440467329696e-05\n",
            "Epoch: 153, Loss: 9.128911915468052e-05\n",
            "Epoch: 154, Loss: 8.595579856773838e-05\n",
            "Epoch: 155, Loss: 8.059099491219968e-05\n",
            "Epoch: 156, Loss: 7.592775364173576e-05\n",
            "Epoch: 157, Loss: 7.10351305315271e-05\n",
            "Epoch: 158, Loss: 6.697406934108585e-05\n",
            "Epoch: 159, Loss: 6.272018799791113e-05\n",
            "Epoch: 160, Loss: 5.903376222704537e-05\n",
            "Epoch: 161, Loss: 5.5424272431991994e-05\n",
            "Epoch: 162, Loss: 5.215251076151617e-05\n",
            "Epoch: 163, Loss: 4.90235433971975e-05\n",
            "Epoch: 164, Loss: 4.6172095608199015e-05\n",
            "Epoch: 165, Loss: 4.3460444430820644e-05\n",
            "Epoch: 166, Loss: 4.098284989595413e-05\n",
            "Epoch: 167, Loss: 3.846003528451547e-05\n",
            "Epoch: 168, Loss: 3.633911910583265e-05\n",
            "Epoch: 169, Loss: 3.4181161026936024e-05\n",
            "Epoch: 170, Loss: 3.236658449168317e-05\n",
            "Epoch: 171, Loss: 3.0458171750069596e-05\n",
            "Epoch: 172, Loss: 2.8835487682954408e-05\n",
            "Epoch: 173, Loss: 2.714479705900885e-05\n",
            "Epoch: 174, Loss: 2.564772694313433e-05\n",
            "Epoch: 175, Loss: 2.4203580323955975e-05\n",
            "Epoch: 176, Loss: 2.2948343030293472e-05\n",
            "Epoch: 177, Loss: 2.1638870748574845e-05\n",
            "Epoch: 178, Loss: 2.056262565020006e-05\n",
            "Epoch: 179, Loss: 1.9412806068430655e-05\n",
            "Epoch: 180, Loss: 1.8318907677894458e-05\n",
            "Epoch: 181, Loss: 1.748278555169236e-05\n",
            "Epoch: 182, Loss: 1.6532183508388698e-05\n",
            "Epoch: 183, Loss: 1.5591043847962283e-05\n",
            "Epoch: 184, Loss: 1.4797506992181297e-05\n",
            "Epoch: 185, Loss: 1.4007842764840461e-05\n",
            "Epoch: 186, Loss: 1.3295201824803371e-05\n",
            "Epoch: 187, Loss: 1.2544692253868561e-05\n",
            "Epoch: 188, Loss: 1.1889719644386787e-05\n",
            "Epoch: 189, Loss: 1.1209350304852705e-05\n",
            "Epoch: 190, Loss: 1.0640011169016361e-05\n",
            "Epoch: 191, Loss: 1.0077123988594394e-05\n",
            "Epoch: 192, Loss: 9.529295311949681e-06\n",
            "Epoch: 193, Loss: 9.154465260508005e-06\n",
            "Epoch: 194, Loss: 8.615673323220108e-06\n",
            "Epoch: 195, Loss: 8.175426955858711e-06\n",
            "Epoch: 196, Loss: 7.998556611710228e-06\n",
            "Epoch: 197, Loss: 7.456315415765857e-06\n",
            "Epoch: 198, Loss: 7.281164926098427e-06\n",
            "Epoch: 199, Loss: 6.868028322060127e-06\n",
            "Epoch: 200, Loss: 0.01614629104733467\n",
            "Epoch: 201, Loss: 0.0006514906999655068\n",
            "Epoch: 202, Loss: 0.00011256810830673203\n",
            "Epoch: 203, Loss: 8.74951365403831e-05\n",
            "Epoch: 204, Loss: 7.720832945778966e-05\n",
            "Epoch: 205, Loss: 6.993013084866107e-05\n",
            "Epoch: 206, Loss: 6.442464655265212e-05\n",
            "Epoch: 207, Loss: 6.001460860716179e-05\n",
            "Epoch: 208, Loss: 5.633060573018156e-05\n",
            "Epoch: 209, Loss: 5.31898949702736e-05\n",
            "Epoch: 210, Loss: 5.045954094384797e-05\n",
            "Epoch: 211, Loss: 4.802988769370131e-05\n",
            "Epoch: 212, Loss: 4.5855755161028355e-05\n",
            "Epoch: 213, Loss: 4.3890700908377767e-05\n",
            "Epoch: 214, Loss: 4.2091280192835256e-05\n",
            "Epoch: 215, Loss: 4.0444581827614456e-05\n",
            "Epoch: 216, Loss: 3.891662345267832e-05\n",
            "Epoch: 217, Loss: 3.750870382646099e-05\n",
            "Epoch: 218, Loss: 3.618811024352908e-05\n",
            "Epoch: 219, Loss: 3.4959171898663044e-05\n",
            "Epoch: 220, Loss: 3.3798634831327945e-05\n",
            "Epoch: 221, Loss: 3.2710813684388995e-05\n",
            "Epoch: 222, Loss: 3.167893009958789e-05\n",
            "Epoch: 223, Loss: 3.070556340389885e-05\n",
            "Epoch: 224, Loss: 2.9779099349980243e-05\n",
            "Epoch: 225, Loss: 2.8897378797410056e-05\n",
            "Epoch: 226, Loss: 2.8038895834470168e-05\n",
            "Epoch: 227, Loss: 2.722086173889693e-05\n",
            "Epoch: 228, Loss: 2.64406917267479e-05\n",
            "Epoch: 229, Loss: 2.568677518866025e-05\n",
            "Epoch: 230, Loss: 2.4959101210697554e-05\n",
            "Epoch: 231, Loss: 2.4261984435725026e-05\n",
            "Epoch: 232, Loss: 2.3579921617056243e-05\n",
            "Epoch: 233, Loss: 2.2931861167307943e-05\n",
            "Epoch: 234, Loss: 2.22988601308316e-05\n",
            "Epoch: 235, Loss: 2.1692963855457492e-05\n",
            "Epoch: 236, Loss: 2.1106003259774297e-05\n",
            "Epoch: 237, Loss: 2.0534540453809313e-05\n",
            "Epoch: 238, Loss: 1.9969955246779136e-05\n",
            "Epoch: 239, Loss: 1.944065843417775e-05\n",
            "Epoch: 240, Loss: 1.8921255104942247e-05\n",
            "Epoch: 241, Loss: 1.8421651475364342e-05\n",
            "Epoch: 242, Loss: 1.792462353478186e-05\n",
            "Epoch: 243, Loss: 1.7458158254157752e-05\n",
            "Epoch: 244, Loss: 1.699082349659875e-05\n",
            "Epoch: 245, Loss: 1.655231972108595e-05\n",
            "Epoch: 246, Loss: 1.61099414981436e-05\n",
            "Epoch: 247, Loss: 1.567746039654594e-05\n",
            "Epoch: 248, Loss: 1.529145629319828e-05\n",
            "Epoch: 249, Loss: 1.4874038242851384e-05\n",
            "Epoch: 250, Loss: 1.4463071238424163e-05\n",
            "Epoch: 251, Loss: 1.4058559827390127e-05\n",
            "Epoch: 252, Loss: 1.3681158634426538e-05\n",
            "Epoch: 253, Loss: 1.3291275536175817e-05\n",
            "Epoch: 254, Loss: 1.2917745152662974e-05\n",
            "Epoch: 255, Loss: 1.2550239262054674e-05\n",
            "Epoch: 256, Loss: 1.2191771020297892e-05\n",
            "Epoch: 257, Loss: 1.1829861250589602e-05\n",
            "Epoch: 258, Loss: 1.1480854482215364e-05\n",
            "Epoch: 259, Loss: 1.1136153261759318e-05\n",
            "Epoch: 260, Loss: 1.0800489690154791e-05\n",
            "Epoch: 261, Loss: 1.0470419510966167e-05\n",
            "Epoch: 262, Loss: 1.0158421900996473e-05\n",
            "Epoch: 263, Loss: 9.80425193120027e-06\n",
            "Epoch: 264, Loss: 9.488810974289663e-06\n",
            "Epoch: 265, Loss: 9.191016943077557e-06\n",
            "Epoch: 266, Loss: 8.89795228431467e-06\n",
            "Epoch: 267, Loss: 8.611771590949502e-06\n",
            "Epoch: 268, Loss: 8.330756827490404e-06\n",
            "Epoch: 269, Loss: 8.042426088650245e-06\n",
            "Epoch: 270, Loss: 7.760551852697972e-06\n",
            "Epoch: 271, Loss: 7.531179562647594e-06\n",
            "Epoch: 272, Loss: 7.267806722666137e-06\n",
            "Epoch: 273, Loss: 7.019496024440741e-06\n",
            "Epoch: 274, Loss: 6.7642995418282226e-06\n",
            "Epoch: 275, Loss: 6.520292572531616e-06\n",
            "Epoch: 276, Loss: 6.293068054219475e-06\n",
            "Epoch: 277, Loss: 6.086501798563404e-06\n",
            "Epoch: 278, Loss: 5.851101377629675e-06\n",
            "Epoch: 279, Loss: 5.662177954945946e-06\n",
            "Epoch: 280, Loss: 5.433661499409936e-06\n",
            "Epoch: 281, Loss: 5.2348409553815145e-06\n",
            "Epoch: 282, Loss: 5.076040906715207e-06\n",
            "Epoch: 283, Loss: 4.866028575634118e-06\n",
            "Epoch: 284, Loss: 4.7089506551856175e-06\n",
            "Epoch: 285, Loss: 4.533366791292792e-06\n",
            "Epoch: 286, Loss: 4.370263468445046e-06\n",
            "Epoch: 287, Loss: 4.227386853017379e-06\n",
            "Epoch: 288, Loss: 4.087522029294632e-06\n",
            "Epoch: 289, Loss: 3.932595063815825e-06\n",
            "Epoch: 290, Loss: 3.8142475204949733e-06\n",
            "Epoch: 291, Loss: 3.7910081118752714e-06\n",
            "Epoch: 292, Loss: 3.666208158392692e-06\n",
            "Epoch: 293, Loss: 3.474268396530533e-06\n",
            "Epoch: 294, Loss: 3.342580157550401e-06\n",
            "Epoch: 295, Loss: 3.1790457342140144e-06\n",
            "Epoch: 296, Loss: 3.056394689338049e-06\n",
            "Epoch: 297, Loss: 2.95052700494125e-06\n",
            "Epoch: 298, Loss: 2.854557806131197e-06\n",
            "Epoch: 299, Loss: 2.8330396162346005e-06\n",
            "Epoch: 300, Loss: 2.7616010811470915e-06\n",
            "Epoch: 301, Loss: 2.7796768335974775e-06\n",
            "Epoch: 302, Loss: 2.691022700673784e-06\n",
            "Epoch: 303, Loss: 2.535232169975643e-06\n",
            "Epoch: 304, Loss: 2.4069879600574495e-06\n",
            "Epoch: 305, Loss: 2.2722865651303437e-06\n",
            "Epoch: 306, Loss: 2.27314649237087e-06\n",
            "Epoch: 307, Loss: 2.1694315819331678e-06\n",
            "Epoch: 308, Loss: 2.758150003501214e-06\n",
            "Epoch: 309, Loss: 0.01096025574952364\n",
            "Epoch: 310, Loss: 0.0014590541832149029\n",
            "Epoch: 311, Loss: 0.00010047206160379574\n",
            "Epoch: 312, Loss: 6.945462519070134e-05\n",
            "Epoch: 313, Loss: 5.819961734232493e-05\n",
            "Epoch: 314, Loss: 5.115425301482901e-05\n",
            "Epoch: 315, Loss: 4.61457711935509e-05\n",
            "Epoch: 316, Loss: 4.229820842738263e-05\n",
            "Epoch: 317, Loss: 3.92064503103029e-05\n",
            "Epoch: 318, Loss: 3.661585651570931e-05\n",
            "Epoch: 319, Loss: 3.4418895666021854e-05\n",
            "Epoch: 320, Loss: 3.250588633818552e-05\n",
            "Epoch: 321, Loss: 3.080368333030492e-05\n",
            "Epoch: 322, Loss: 2.928432877524756e-05\n",
            "Epoch: 323, Loss: 2.7910835342481732e-05\n",
            "Epoch: 324, Loss: 2.664446219569072e-05\n",
            "Epoch: 325, Loss: 2.549039709265344e-05\n",
            "Epoch: 326, Loss: 2.4421513444394805e-05\n",
            "Epoch: 327, Loss: 2.3427506675943732e-05\n",
            "Epoch: 328, Loss: 2.2490285118692555e-05\n",
            "Epoch: 329, Loss: 2.1610727344523184e-05\n",
            "Epoch: 330, Loss: 2.078623765555676e-05\n",
            "Epoch: 331, Loss: 2.0015111658722162e-05\n",
            "Epoch: 332, Loss: 1.9276252714917064e-05\n",
            "Epoch: 333, Loss: 1.85743974725483e-05\n",
            "Epoch: 334, Loss: 1.7919013771461323e-05\n",
            "Epoch: 335, Loss: 1.7290743926423602e-05\n",
            "Epoch: 336, Loss: 1.669172888796311e-05\n",
            "Epoch: 337, Loss: 1.6124127796501853e-05\n",
            "Epoch: 338, Loss: 1.558234180265572e-05\n",
            "Epoch: 339, Loss: 1.5060783880471718e-05\n",
            "Epoch: 340, Loss: 1.456203062843997e-05\n",
            "Epoch: 341, Loss: 1.4092968740442302e-05\n",
            "Epoch: 342, Loss: 1.3648003914568108e-05\n",
            "Epoch: 343, Loss: 1.3215951184974983e-05\n",
            "Epoch: 344, Loss: 1.2804981452063657e-05\n",
            "Epoch: 345, Loss: 1.2406489986460656e-05\n",
            "Epoch: 346, Loss: 1.2021770999126602e-05\n",
            "Epoch: 347, Loss: 1.1660718882922083e-05\n",
            "Epoch: 348, Loss: 1.1314296898490284e-05\n",
            "Epoch: 349, Loss: 1.098035409086151e-05\n",
            "Epoch: 350, Loss: 1.066018103301758e-05\n",
            "Epoch: 351, Loss: 1.0351628588978201e-05\n",
            "Epoch: 352, Loss: 1.0058568477688823e-05\n",
            "Epoch: 353, Loss: 9.779278116184287e-06\n",
            "Epoch: 354, Loss: 9.50299909163732e-06\n",
            "Epoch: 355, Loss: 9.230166142515372e-06\n",
            "Epoch: 356, Loss: 8.988311492430512e-06\n",
            "Epoch: 357, Loss: 8.748182153794914e-06\n",
            "Epoch: 358, Loss: 8.526984856871422e-06\n",
            "Epoch: 359, Loss: 8.298043212562334e-06\n",
            "Epoch: 360, Loss: 8.085454282991122e-06\n",
            "Epoch: 361, Loss: 7.872004061937332e-06\n",
            "Epoch: 362, Loss: 7.683944204472937e-06\n",
            "Epoch: 363, Loss: 7.476518931071041e-06\n",
            "Epoch: 364, Loss: 7.313419700949453e-06\n",
            "Epoch: 365, Loss: 7.128800461941864e-06\n",
            "Epoch: 366, Loss: 6.951069281058153e-06\n",
            "Epoch: 367, Loss: 6.781512638553977e-06\n",
            "Epoch: 368, Loss: 6.622715318371775e-06\n",
            "Epoch: 369, Loss: 6.445411599997897e-06\n",
            "Epoch: 370, Loss: 6.324484729702817e-06\n",
            "Epoch: 371, Loss: 6.176015176606597e-06\n",
            "Epoch: 372, Loss: 6.010761353536509e-06\n",
            "Epoch: 373, Loss: 5.866595074621728e-06\n",
            "Epoch: 374, Loss: 5.745236194343306e-06\n",
            "Epoch: 375, Loss: 5.5937534853001125e-06\n",
            "Epoch: 376, Loss: 5.45776356375427e-06\n",
            "Epoch: 377, Loss: 5.339848030416761e-06\n",
            "Epoch: 378, Loss: 5.225374479778111e-06\n",
            "Epoch: 379, Loss: 5.063132448412944e-06\n",
            "Epoch: 380, Loss: 4.949950380250812e-06\n",
            "Epoch: 381, Loss: 4.834185801882995e-06\n",
            "Epoch: 382, Loss: 4.7042203732416965e-06\n",
            "Epoch: 383, Loss: 4.585012447932968e-06\n",
            "Epoch: 384, Loss: 4.46623562311288e-06\n",
            "Epoch: 385, Loss: 4.3435852603579406e-06\n",
            "Epoch: 386, Loss: 4.218352387397317e-06\n",
            "Epoch: 387, Loss: 4.1322809920529835e-06\n",
            "Epoch: 388, Loss: 4.0182376324082725e-06\n",
            "Epoch: 389, Loss: 3.893866050930228e-06\n",
            "Epoch: 390, Loss: 3.809086138062412e-06\n",
            "Epoch: 391, Loss: 3.6829928831139114e-06\n",
            "Epoch: 392, Loss: 3.577986262826016e-06\n",
            "Epoch: 393, Loss: 3.518597395668621e-06\n",
            "Epoch: 394, Loss: 3.400680270715384e-06\n",
            "Epoch: 395, Loss: 3.301267952338094e-06\n",
            "Epoch: 396, Loss: 3.205729171895655e-06\n",
            "Epoch: 397, Loss: 3.0955586680647684e-06\n",
            "Epoch: 398, Loss: 3.0555356715922244e-06\n",
            "Epoch: 399, Loss: 2.9173916118452325e-06\n",
            "Epoch: 400, Loss: 2.827877551681013e-06\n",
            "Epoch: 401, Loss: 2.7538560516404686e-06\n",
            "Epoch: 402, Loss: 2.6725188035925385e-06\n",
            "Epoch: 403, Loss: 2.5838653527898714e-06\n",
            "Epoch: 404, Loss: 2.515009100534371e-06\n",
            "Epoch: 405, Loss: 2.421191766188713e-06\n",
            "Epoch: 406, Loss: 2.3747127215756336e-06\n",
            "Epoch: 407, Loss: 2.3080074242898263e-06\n",
            "Epoch: 408, Loss: 2.2395809082809137e-06\n",
            "Epoch: 409, Loss: 2.1866469523956766e-06\n",
            "Epoch: 410, Loss: 2.1263972485030536e-06\n",
            "Epoch: 411, Loss: 2.0497943751252024e-06\n",
            "Epoch: 412, Loss: 1.9865315152856056e-06\n",
            "Epoch: 413, Loss: 1.94091353478143e-06\n",
            "Epoch: 414, Loss: 1.8862579054257367e-06\n",
            "Epoch: 415, Loss: 1.8285901433046092e-06\n",
            "Epoch: 416, Loss: 1.768340098351473e-06\n",
            "Epoch: 417, Loss: 1.722722231534135e-06\n",
            "Epoch: 418, Loss: 1.7024956378008937e-06\n",
            "Epoch: 419, Loss: 1.6465492080897093e-06\n",
            "Epoch: 420, Loss: 1.6017922916944372e-06\n",
            "Epoch: 421, Loss: 1.5415423604281386e-06\n",
            "Epoch: 422, Loss: 1.507974502601428e-06\n",
            "Epoch: 423, Loss: 1.4442815654547303e-06\n",
            "Epoch: 424, Loss: 1.444281679141568e-06\n",
            "Epoch: 425, Loss: 1.3904868865211029e-06\n",
            "Epoch: 426, Loss: 1.3595011978395632e-06\n",
            "Epoch: 427, Loss: 1.3211994200901245e-06\n",
            "Epoch: 428, Loss: 1.280745777876291e-06\n",
            "Epoch: 429, Loss: 1.2424438864400145e-06\n",
            "Epoch: 430, Loss: 1.2136098348491942e-06\n",
            "Epoch: 431, Loss: 1.1886490938195493e-06\n",
            "Epoch: 432, Loss: 1.1671312449834659e-06\n",
            "Epoch: 433, Loss: 1.153790208263672e-06\n",
            "Epoch: 434, Loss: 1.1197918183825095e-06\n",
            "Epoch: 435, Loss: 1.096552637136483e-06\n",
            "Epoch: 436, Loss: 1.080199012903904e-06\n",
            "Epoch: 437, Loss: 1.0526562164159259e-06\n",
            "Epoch: 438, Loss: 1.063845388671325e-06\n",
            "Epoch: 439, Loss: 1.065566721081268e-06\n",
            "Epoch: 440, Loss: 1.0169364941248205e-06\n",
            "Epoch: 441, Loss: 9.863809964372194e-07\n",
            "Epoch: 442, Loss: 9.67014898378693e-07\n",
            "Epoch: 443, Loss: 9.519523587186995e-07\n",
            "Epoch: 444, Loss: 9.338772883893398e-07\n",
            "Epoch: 445, Loss: 9.12359610083513e-07\n",
            "Epoch: 446, Loss: 8.985880981526861e-07\n",
            "Epoch: 447, Loss: 8.938540645431203e-07\n",
            "Epoch: 448, Loss: 8.830951969684975e-07\n",
            "Epoch: 449, Loss: 0.0046166665852069855\n",
            "Epoch: 450, Loss: 0.0002888728631660342\n",
            "Epoch: 451, Loss: 6.267066055443138e-05\n",
            "Epoch: 452, Loss: 5.020690514356829e-05\n",
            "Epoch: 453, Loss: 4.3317904783179983e-05\n",
            "Epoch: 454, Loss: 3.853153611999005e-05\n",
            "Epoch: 455, Loss: 3.489606751827523e-05\n",
            "Epoch: 456, Loss: 3.200484934495762e-05\n",
            "Epoch: 457, Loss: 2.9615852326969616e-05\n",
            "Epoch: 458, Loss: 2.76009723165771e-05\n",
            "Epoch: 459, Loss: 2.586648042779416e-05\n",
            "Epoch: 460, Loss: 2.4333670808118768e-05\n",
            "Epoch: 461, Loss: 2.2986674593994394e-05\n",
            "Epoch: 462, Loss: 2.1774725610157475e-05\n",
            "Epoch: 463, Loss: 2.067331297439523e-05\n",
            "Epoch: 464, Loss: 1.967557182069868e-05\n",
            "Epoch: 465, Loss: 1.8753096810542047e-05\n",
            "Epoch: 466, Loss: 1.79106409632368e-05\n",
            "Epoch: 467, Loss: 1.712151788524352e-05\n",
            "Epoch: 468, Loss: 1.6372399841202423e-05\n",
            "Epoch: 469, Loss: 1.5684805475757457e-05\n",
            "Epoch: 470, Loss: 1.50441028381465e-05\n",
            "Epoch: 471, Loss: 1.4435664525080938e-05\n",
            "Epoch: 472, Loss: 1.385949599352898e-05\n",
            "Epoch: 473, Loss: 1.3319466233951971e-05\n",
            "Epoch: 474, Loss: 1.2800944205082487e-05\n",
            "Epoch: 475, Loss: 1.2316412721702363e-05\n",
            "Epoch: 476, Loss: 1.185683868243359e-05\n",
            "Epoch: 477, Loss: 1.1425229786254931e-05\n",
            "Epoch: 478, Loss: 1.1009109584847465e-05\n",
            "Epoch: 479, Loss: 1.0615796782076359e-05\n",
            "Epoch: 480, Loss: 1.0247438694932498e-05\n",
            "Epoch: 481, Loss: 9.88036936178105e-06\n",
            "Epoch: 482, Loss: 9.543850865156855e-06\n",
            "Epoch: 483, Loss: 9.224976565747056e-06\n",
            "Epoch: 484, Loss: 8.917719242163002e-06\n",
            "Epoch: 485, Loss: 8.620357220934238e-06\n",
            "Epoch: 486, Loss: 8.341069587913807e-06\n",
            "Epoch: 487, Loss: 8.072968739725184e-06\n",
            "Epoch: 488, Loss: 7.819500751793385e-06\n",
            "Epoch: 489, Loss: 7.579371413157787e-06\n",
            "Epoch: 490, Loss: 7.344834102696041e-06\n",
            "Epoch: 491, Loss: 7.118042049114592e-06\n",
            "Epoch: 492, Loss: 6.9127690949244425e-06\n",
            "Epoch: 493, Loss: 6.707925422233529e-06\n",
            "Epoch: 494, Loss: 6.50824449621723e-06\n",
            "Epoch: 495, Loss: 6.321473847492598e-06\n",
            "Epoch: 496, Loss: 6.14933605902479e-06\n",
            "Epoch: 497, Loss: 5.97676580582629e-06\n",
            "Epoch: 498, Loss: 5.810651600768324e-06\n",
            "Epoch: 499, Loss: 5.651853825838771e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(15,64,tanh)"
      ],
      "metadata": {
        "id": "pwHDUr3L1lB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 2]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size = 15  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 64\n",
        "activation = 'tanh'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(500):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_15': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'block_15_emd_64_tanh.pth')"
      ],
      "metadata": {
        "id": "9WO9X-gfaZXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb8ff2f-afa0-4aa6-8b13-9f5cc20752c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.951933860778809\n",
            "Epoch: 1, Loss: 8.007486343383789\n",
            "Epoch: 2, Loss: 7.259562015533447\n",
            "Epoch: 3, Loss: 6.602184295654297\n",
            "Epoch: 4, Loss: 5.995013236999512\n",
            "Epoch: 5, Loss: 5.416737079620361\n",
            "Epoch: 6, Loss: 4.859874725341797\n",
            "Epoch: 7, Loss: 4.328752517700195\n",
            "Epoch: 8, Loss: 3.8310227394104004\n",
            "Epoch: 9, Loss: 3.373142957687378\n",
            "Epoch: 10, Loss: 2.9589321613311768\n",
            "Epoch: 11, Loss: 2.587606191635132\n",
            "Epoch: 12, Loss: 2.2550032138824463\n",
            "Epoch: 13, Loss: 1.9582115411758423\n",
            "Epoch: 14, Loss: 1.693142294883728\n",
            "Epoch: 15, Loss: 1.4564403295516968\n",
            "Epoch: 16, Loss: 1.2614535093307495\n",
            "Epoch: 17, Loss: 1.0979951620101929\n",
            "Epoch: 18, Loss: 0.9591729640960693\n",
            "Epoch: 19, Loss: 0.8411192893981934\n",
            "Epoch: 20, Loss: 0.7402994632720947\n",
            "Epoch: 21, Loss: 0.6532464027404785\n",
            "Epoch: 22, Loss: 0.5780172348022461\n",
            "Epoch: 23, Loss: 0.5131217837333679\n",
            "Epoch: 24, Loss: 0.45730507373809814\n",
            "Epoch: 25, Loss: 0.4092724919319153\n",
            "Epoch: 26, Loss: 0.3674492835998535\n",
            "Epoch: 27, Loss: 0.3309326469898224\n",
            "Epoch: 28, Loss: 0.29908254742622375\n",
            "Epoch: 29, Loss: 0.2708280086517334\n",
            "Epoch: 30, Loss: 0.24560581147670746\n",
            "Epoch: 31, Loss: 0.22317862510681152\n",
            "Epoch: 32, Loss: 0.20326246321201324\n",
            "Epoch: 33, Loss: 0.18541879951953888\n",
            "Epoch: 34, Loss: 0.1693689525127411\n",
            "Epoch: 35, Loss: 0.15522149205207825\n",
            "Epoch: 36, Loss: 0.14267826080322266\n",
            "Epoch: 37, Loss: 0.13141922652721405\n",
            "Epoch: 38, Loss: 0.12121826410293579\n",
            "Epoch: 39, Loss: 0.11186578869819641\n",
            "Epoch: 40, Loss: 0.1032581627368927\n",
            "Epoch: 41, Loss: 0.09535381942987442\n",
            "Epoch: 42, Loss: 0.08813466876745224\n",
            "Epoch: 43, Loss: 0.08148606866598129\n",
            "Epoch: 44, Loss: 0.07524806261062622\n",
            "Epoch: 45, Loss: 0.06927058845758438\n",
            "Epoch: 46, Loss: 0.0634050965309143\n",
            "Epoch: 47, Loss: 0.058110062032938004\n",
            "Epoch: 48, Loss: 0.05377313494682312\n",
            "Epoch: 49, Loss: 0.04986555129289627\n",
            "Epoch: 50, Loss: 0.04633137583732605\n",
            "Epoch: 51, Loss: 0.04304913058876991\n",
            "Epoch: 52, Loss: 0.03997811675071716\n",
            "Epoch: 53, Loss: 0.037137221544981\n",
            "Epoch: 54, Loss: 0.03453996405005455\n",
            "Epoch: 55, Loss: 0.032178446650505066\n",
            "Epoch: 56, Loss: 0.029969723895192146\n",
            "Epoch: 57, Loss: 0.02789769321680069\n",
            "Epoch: 58, Loss: 0.02596082165837288\n",
            "Epoch: 59, Loss: 0.024147700518369675\n",
            "Epoch: 60, Loss: 0.022476114332675934\n",
            "Epoch: 61, Loss: 0.020924028009176254\n",
            "Epoch: 62, Loss: 0.019456440582871437\n",
            "Epoch: 63, Loss: 0.01809369958937168\n",
            "Epoch: 64, Loss: 0.016818063333630562\n",
            "Epoch: 65, Loss: 0.015671182423830032\n",
            "Epoch: 66, Loss: 0.01454685628414154\n",
            "Epoch: 67, Loss: 0.01353435032069683\n",
            "Epoch: 68, Loss: 0.01247907243669033\n",
            "Epoch: 69, Loss: 0.011551592499017715\n",
            "Epoch: 70, Loss: 0.010713677853345871\n",
            "Epoch: 71, Loss: 0.009972721338272095\n",
            "Epoch: 72, Loss: 0.00928403902798891\n",
            "Epoch: 73, Loss: 0.008665272034704685\n",
            "Epoch: 74, Loss: 0.008065583184361458\n",
            "Epoch: 75, Loss: 0.007506607100367546\n",
            "Epoch: 76, Loss: 0.006995806936174631\n",
            "Epoch: 77, Loss: 0.006500720977783203\n",
            "Epoch: 78, Loss: 0.006035441067069769\n",
            "Epoch: 79, Loss: 0.005607005208730698\n",
            "Epoch: 80, Loss: 0.005214015021920204\n",
            "Epoch: 81, Loss: 0.004846479743719101\n",
            "Epoch: 82, Loss: 0.004505667835474014\n",
            "Epoch: 83, Loss: 0.004184926860034466\n",
            "Epoch: 84, Loss: 0.0038804332725703716\n",
            "Epoch: 85, Loss: 0.0036100009456276894\n",
            "Epoch: 86, Loss: 0.0033634190913289785\n",
            "Epoch: 87, Loss: 0.0031328813638538122\n",
            "Epoch: 88, Loss: 0.0029195852112025023\n",
            "Epoch: 89, Loss: 0.002716425573453307\n",
            "Epoch: 90, Loss: 0.002524200826883316\n",
            "Epoch: 91, Loss: 0.002348216250538826\n",
            "Epoch: 92, Loss: 0.0021867717150598764\n",
            "Epoch: 93, Loss: 0.0020342173520475626\n",
            "Epoch: 94, Loss: 0.0018931495724245906\n",
            "Epoch: 95, Loss: 0.0017610574141144753\n",
            "Epoch: 96, Loss: 0.0016416115686297417\n",
            "Epoch: 97, Loss: 0.001526708947494626\n",
            "Epoch: 98, Loss: 0.0014269468374550343\n",
            "Epoch: 99, Loss: 0.0013299987185746431\n",
            "Epoch: 100, Loss: 0.0012408661423251033\n",
            "Epoch: 101, Loss: 0.0011531177442520857\n",
            "Epoch: 102, Loss: 0.0010719172423705459\n",
            "Epoch: 103, Loss: 0.0009954245761036873\n",
            "Epoch: 104, Loss: 0.0009259338257834315\n",
            "Epoch: 105, Loss: 0.0008621030719950795\n",
            "Epoch: 106, Loss: 0.0008033855119720101\n",
            "Epoch: 107, Loss: 0.0007487025577574968\n",
            "Epoch: 108, Loss: 0.0006979374447837472\n",
            "Epoch: 109, Loss: 0.0006495718844234943\n",
            "Epoch: 110, Loss: 0.0006037873681634665\n",
            "Epoch: 111, Loss: 0.0005604883190244436\n",
            "Epoch: 112, Loss: 0.0005199441220611334\n",
            "Epoch: 113, Loss: 0.00048245134530588984\n",
            "Epoch: 114, Loss: 0.0004494389577303082\n",
            "Epoch: 115, Loss: 0.00041818470344878733\n",
            "Epoch: 116, Loss: 0.0003905338526237756\n",
            "Epoch: 117, Loss: 0.0003636523615568876\n",
            "Epoch: 118, Loss: 0.0003395030798856169\n",
            "Epoch: 119, Loss: 0.00031618945649825037\n",
            "Epoch: 120, Loss: 0.00029559453832916915\n",
            "Epoch: 121, Loss: 0.0002754760498646647\n",
            "Epoch: 122, Loss: 0.00025751639623194933\n",
            "Epoch: 123, Loss: 0.00023986530140973628\n",
            "Epoch: 124, Loss: 0.00022414732666220516\n",
            "Epoch: 125, Loss: 0.0002088371547870338\n",
            "Epoch: 126, Loss: 0.00019539448840077966\n",
            "Epoch: 127, Loss: 0.00018183447537012398\n",
            "Epoch: 128, Loss: 0.0001700807479210198\n",
            "Epoch: 129, Loss: 0.00015837640967220068\n",
            "Epoch: 130, Loss: 0.0001482037187088281\n",
            "Epoch: 131, Loss: 0.0001379565946990624\n",
            "Epoch: 132, Loss: 0.00012923369649797678\n",
            "Epoch: 133, Loss: 0.00012023794988635927\n",
            "Epoch: 134, Loss: 0.00011274108692305163\n",
            "Epoch: 135, Loss: 0.0001050354985636659\n",
            "Epoch: 136, Loss: 9.86250743153505e-05\n",
            "Epoch: 137, Loss: 9.19174199225381e-05\n",
            "Epoch: 138, Loss: 8.629951480543241e-05\n",
            "Epoch: 139, Loss: 8.039711246965453e-05\n",
            "Epoch: 140, Loss: 7.547537825303152e-05\n",
            "Epoch: 141, Loss: 7.014907896518707e-05\n",
            "Epoch: 142, Loss: 6.590686098206788e-05\n",
            "Epoch: 143, Loss: 6.132149428594857e-05\n",
            "Epoch: 144, Loss: 5.760328713222407e-05\n",
            "Epoch: 145, Loss: 5.3712388762505725e-05\n",
            "Epoch: 146, Loss: 5.059491013525985e-05\n",
            "Epoch: 147, Loss: 4.719080243376084e-05\n",
            "Epoch: 148, Loss: 4.450098276720382e-05\n",
            "Epoch: 149, Loss: 4.1487284761387855e-05\n",
            "Epoch: 150, Loss: 3.908578582922928e-05\n",
            "Epoch: 151, Loss: 3.653308885986917e-05\n",
            "Epoch: 152, Loss: 3.435244070715271e-05\n",
            "Epoch: 153, Loss: 3.209814531146549e-05\n",
            "Epoch: 154, Loss: 3.0241324566304684e-05\n",
            "Epoch: 155, Loss: 2.830299490597099e-05\n",
            "Epoch: 156, Loss: 2.6661771698854864e-05\n",
            "Epoch: 157, Loss: 2.4900913558667526e-05\n",
            "Epoch: 158, Loss: 2.346215296711307e-05\n",
            "Epoch: 159, Loss: 2.198482252424583e-05\n",
            "Epoch: 160, Loss: 2.0782705178135075e-05\n",
            "Epoch: 161, Loss: 1.9512664948706515e-05\n",
            "Epoch: 162, Loss: 1.833507849369198e-05\n",
            "Epoch: 163, Loss: 1.726049231365323e-05\n",
            "Epoch: 164, Loss: 1.6247686289716512e-05\n",
            "Epoch: 165, Loss: 1.532998248876538e-05\n",
            "Epoch: 166, Loss: 1.4453910807787906e-05\n",
            "Epoch: 167, Loss: 1.3701424904866144e-05\n",
            "Epoch: 168, Loss: 1.2942364264745265e-05\n",
            "Epoch: 169, Loss: 1.2200832316011656e-05\n",
            "Epoch: 170, Loss: 1.1516272934386507e-05\n",
            "Epoch: 171, Loss: 1.0925500646408182e-05\n",
            "Epoch: 172, Loss: 1.0315881809219718e-05\n",
            "Epoch: 173, Loss: 9.813636097533163e-06\n",
            "Epoch: 174, Loss: 9.293856237491127e-06\n",
            "Epoch: 175, Loss: 8.801251169643365e-06\n",
            "Epoch: 176, Loss: 8.338884072145447e-06\n",
            "Epoch: 177, Loss: 7.918154551589396e-06\n",
            "Epoch: 178, Loss: 7.512320735258982e-06\n",
            "Epoch: 179, Loss: 7.120954251149669e-06\n",
            "Epoch: 180, Loss: 6.793571174057433e-06\n",
            "Epoch: 181, Loss: 6.453039532061666e-06\n",
            "Epoch: 182, Loss: 6.131792360974941e-06\n",
            "Epoch: 183, Loss: 5.8241312217433006e-06\n",
            "Epoch: 184, Loss: 5.53706695427536e-06\n",
            "Epoch: 185, Loss: 5.272355338092893e-06\n",
            "Epoch: 186, Loss: 5.0457711040508e-06\n",
            "Epoch: 187, Loss: 4.816121872863732e-06\n",
            "Epoch: 188, Loss: 4.599618932843441e-06\n",
            "Epoch: 189, Loss: 4.401084424898727e-06\n",
            "Epoch: 190, Loss: 4.261715730535798e-06\n",
            "Epoch: 191, Loss: 3.997879503003787e-06\n",
            "Epoch: 192, Loss: 3.830024070339277e-06\n",
            "Epoch: 193, Loss: 3.648143319878727e-06\n",
            "Epoch: 194, Loss: 3.5008856684726197e-06\n",
            "Epoch: 195, Loss: 3.343548087286763e-06\n",
            "Epoch: 196, Loss: 3.205931761840475e-06\n",
            "Epoch: 197, Loss: 3.05341472994769e-06\n",
            "Epoch: 198, Loss: 2.9363980047492078e-06\n",
            "Epoch: 199, Loss: 2.81280699709896e-06\n",
            "Epoch: 200, Loss: 2.700610366446199e-06\n",
            "Epoch: 201, Loss: 2.6055065518448828e-06\n",
            "Epoch: 202, Loss: 2.502513552826713e-06\n",
            "Epoch: 203, Loss: 2.4122307422658196e-06\n",
            "Epoch: 204, Loss: 2.316688551218249e-06\n",
            "Epoch: 205, Loss: 2.243936251034029e-06\n",
            "Epoch: 206, Loss: 2.1527760054596e-06\n",
            "Epoch: 207, Loss: 2.082653281831881e-06\n",
            "Epoch: 208, Loss: 2.0081474758626428e-06\n",
            "Epoch: 209, Loss: 1.946789780049585e-06\n",
            "Epoch: 210, Loss: 1.8915681039288756e-06\n",
            "Epoch: 211, Loss: 1.8302106354894931e-06\n",
            "Epoch: 212, Loss: 1.771044367160357e-06\n",
            "Epoch: 213, Loss: 1.7074958122975659e-06\n",
            "Epoch: 214, Loss: 1.6535889244551072e-06\n",
            "Epoch: 215, Loss: 1.5992435464795562e-06\n",
            "Epoch: 216, Loss: 1.5593611806252738e-06\n",
            "Epoch: 217, Loss: 1.5111515949683962e-06\n",
            "Epoch: 218, Loss: 1.469077801630192e-06\n",
            "Epoch: 219, Loss: 1.428757059329655e-06\n",
            "Epoch: 220, Loss: 1.398078211423126e-06\n",
            "Epoch: 221, Loss: 1.3520600532501703e-06\n",
            "Epoch: 222, Loss: 1.325764060311485e-06\n",
            "Epoch: 223, Loss: 1.2841285297326976e-06\n",
            "Epoch: 224, Loss: 1.2530115327535896e-06\n",
            "Epoch: 225, Loss: 1.2205795201225556e-06\n",
            "Epoch: 226, Loss: 1.204801947096712e-06\n",
            "Epoch: 227, Loss: 1.1710552598742652e-06\n",
            "Epoch: 228, Loss: 1.1473886161184055e-06\n",
            "Epoch: 229, Loss: 1.1153950936204637e-06\n",
            "Epoch: 230, Loss: 1.09611119114561e-06\n",
            "Epoch: 231, Loss: 1.0781421906358446e-06\n",
            "Epoch: 232, Loss: 1.0527226095291553e-06\n",
            "Epoch: 233, Loss: 1.0268646519762115e-06\n",
            "Epoch: 234, Loss: 1.0106487025041133e-06\n",
            "Epoch: 235, Loss: 9.861057606030954e-07\n",
            "Epoch: 236, Loss: 9.747107014845824e-07\n",
            "Epoch: 237, Loss: 9.523589596938109e-07\n",
            "Epoch: 238, Loss: 9.440318535780534e-07\n",
            "Epoch: 239, Loss: 9.75587227003416e-07\n",
            "Epoch: 240, Loss: 9.365813866679673e-07\n",
            "Epoch: 241, Loss: 9.054641623151838e-07\n",
            "Epoch: 242, Loss: 8.888101206139254e-07\n",
            "Epoch: 243, Loss: 8.712793260201579e-07\n",
            "Epoch: 244, Loss: 8.541868510292261e-07\n",
            "Epoch: 245, Loss: 8.397239525947953e-07\n",
            "Epoch: 246, Loss: 8.292054758385348e-07\n",
            "Epoch: 247, Loss: 8.094833674476831e-07\n",
            "Epoch: 248, Loss: 8.103598929665168e-07\n",
            "Epoch: 249, Loss: 7.972118396537553e-07\n",
            "Epoch: 250, Loss: 7.871316825003305e-07\n",
            "Epoch: 251, Loss: 7.744217782601481e-07\n",
            "Epoch: 252, Loss: 7.74860041019565e-07\n",
            "Epoch: 253, Loss: 0.014504475519061089\n",
            "Epoch: 254, Loss: 0.003924367018043995\n",
            "Epoch: 255, Loss: 0.00044871761929243803\n",
            "Epoch: 256, Loss: 9.475249680690467e-05\n",
            "Epoch: 257, Loss: 7.657799869775772e-05\n",
            "Epoch: 258, Loss: 6.56026677461341e-05\n",
            "Epoch: 259, Loss: 5.790748036815785e-05\n",
            "Epoch: 260, Loss: 5.2092786063440144e-05\n",
            "Epoch: 261, Loss: 4.747309139929712e-05\n",
            "Epoch: 262, Loss: 4.3685453420039266e-05\n",
            "Epoch: 263, Loss: 4.051359064760618e-05\n",
            "Epoch: 264, Loss: 3.779373946599662e-05\n",
            "Epoch: 265, Loss: 3.541290061548352e-05\n",
            "Epoch: 266, Loss: 3.330977415316738e-05\n",
            "Epoch: 267, Loss: 3.143837602692656e-05\n",
            "Epoch: 268, Loss: 2.9746586733381264e-05\n",
            "Epoch: 269, Loss: 2.8223905246704817e-05\n",
            "Epoch: 270, Loss: 2.681950900296215e-05\n",
            "Epoch: 271, Loss: 2.5530333005008288e-05\n",
            "Epoch: 272, Loss: 2.4337543436558917e-05\n",
            "Epoch: 273, Loss: 2.3229318685480393e-05\n",
            "Epoch: 274, Loss: 2.2201265892363153e-05\n",
            "Epoch: 275, Loss: 2.1223604562692344e-05\n",
            "Epoch: 276, Loss: 2.0319992472650483e-05\n",
            "Epoch: 277, Loss: 1.945712574524805e-05\n",
            "Epoch: 278, Loss: 1.865385456767399e-05\n",
            "Epoch: 279, Loss: 1.787993824109435e-05\n",
            "Epoch: 280, Loss: 1.715247162792366e-05\n",
            "Epoch: 281, Loss: 1.6466196029796265e-05\n",
            "Epoch: 282, Loss: 1.5807085219421424e-05\n",
            "Epoch: 283, Loss: 1.5189167243079282e-05\n",
            "Epoch: 284, Loss: 1.4593595551559702e-05\n",
            "Epoch: 285, Loss: 1.402606903866399e-05\n",
            "Epoch: 286, Loss: 1.3483520888257772e-05\n",
            "Epoch: 287, Loss: 1.2976906873518601e-05\n",
            "Epoch: 288, Loss: 1.247686395799974e-05\n",
            "Epoch: 289, Loss: 1.201319810206769e-05\n",
            "Epoch: 290, Loss: 1.1557854122656863e-05\n",
            "Epoch: 291, Loss: 1.114020142267691e-05\n",
            "Epoch: 292, Loss: 1.0733938324847259e-05\n",
            "Epoch: 293, Loss: 1.0332060810469557e-05\n",
            "Epoch: 294, Loss: 9.964365744963288e-06\n",
            "Epoch: 295, Loss: 9.597543794370722e-06\n",
            "Epoch: 296, Loss: 9.253512871509884e-06\n",
            "Epoch: 297, Loss: 8.926573173084762e-06\n",
            "Epoch: 298, Loss: 8.617601451987866e-06\n",
            "Epoch: 299, Loss: 8.318708751176018e-06\n",
            "Epoch: 300, Loss: 8.03909915703116e-06\n",
            "Epoch: 301, Loss: 7.765187547192909e-06\n",
            "Epoch: 302, Loss: 7.498725153709529e-06\n",
            "Epoch: 303, Loss: 7.245848792081233e-06\n",
            "Epoch: 304, Loss: 7.008310603850987e-06\n",
            "Epoch: 305, Loss: 6.778662282158621e-06\n",
            "Epoch: 306, Loss: 6.563475380971795e-06\n",
            "Epoch: 307, Loss: 6.3566149037797e-06\n",
            "Epoch: 308, Loss: 6.151509751362028e-06\n",
            "Epoch: 309, Loss: 5.9503468037291896e-06\n",
            "Epoch: 310, Loss: 5.766275080532068e-06\n",
            "Epoch: 311, Loss: 5.590093678620178e-06\n",
            "Epoch: 312, Loss: 5.4209240261116065e-06\n",
            "Epoch: 313, Loss: 5.248685738479253e-06\n",
            "Epoch: 314, Loss: 5.094854714116082e-06\n",
            "Epoch: 315, Loss: 4.934887329000048e-06\n",
            "Epoch: 316, Loss: 4.786754743690835e-06\n",
            "Epoch: 317, Loss: 4.64168851976865e-06\n",
            "Epoch: 318, Loss: 4.499253009271342e-06\n",
            "Epoch: 319, Loss: 4.365144377516117e-06\n",
            "Epoch: 320, Loss: 4.2354176912340336e-06\n",
            "Epoch: 321, Loss: 4.107443601242267e-06\n",
            "Epoch: 322, Loss: 3.9847291191108525e-06\n",
            "Epoch: 323, Loss: 3.866835868393537e-06\n",
            "Epoch: 324, Loss: 3.759899072974804e-06\n",
            "Epoch: 325, Loss: 3.652085979410913e-06\n",
            "Epoch: 326, Loss: 3.54471058017225e-06\n",
            "Epoch: 327, Loss: 3.44566251442302e-06\n",
            "Epoch: 328, Loss: 3.349682401676546e-06\n",
            "Epoch: 329, Loss: 3.2449368063680595e-06\n",
            "Epoch: 330, Loss: 3.156844968543737e-06\n",
            "Epoch: 331, Loss: 3.066124008910265e-06\n",
            "Epoch: 332, Loss: 2.984606453537708e-06\n",
            "Epoch: 333, Loss: 2.9039651963103097e-06\n",
            "Epoch: 334, Loss: 2.825515821314184e-06\n",
            "Epoch: 335, Loss: 2.74531294053304e-06\n",
            "Epoch: 336, Loss: 2.66905431089981e-06\n",
            "Epoch: 337, Loss: 2.5984934381995117e-06\n",
            "Epoch: 338, Loss: 2.5371359697601292e-06\n",
            "Epoch: 339, Loss: 2.459124516462907e-06\n",
            "Epoch: 340, Loss: 2.412667981843697e-06\n",
            "Epoch: 341, Loss: 2.347366489630076e-06\n",
            "Epoch: 342, Loss: 2.289076974193449e-06\n",
            "Epoch: 343, Loss: 2.232978886240744e-06\n",
            "Epoch: 344, Loss: 2.1895905319979647e-06\n",
            "Epoch: 345, Loss: 2.1391897462308407e-06\n",
            "Epoch: 346, Loss: 2.0830916582781356e-06\n",
            "Epoch: 347, Loss: 2.0243635390215786e-06\n",
            "Epoch: 348, Loss: 1.987987161555793e-06\n",
            "Epoch: 349, Loss: 1.9551171135390177e-06\n",
            "Epoch: 350, Loss: 1.9042779513256392e-06\n",
            "Epoch: 351, Loss: 1.8538771655585151e-06\n",
            "Epoch: 352, Loss: 1.8043529053102247e-06\n",
            "Epoch: 353, Loss: 1.7771802731658681e-06\n",
            "Epoch: 354, Loss: 1.7508840528535075e-06\n",
            "Epoch: 355, Loss: 1.7267793737119064e-06\n",
            "Epoch: 356, Loss: 1.6763784742579446e-06\n",
            "Epoch: 357, Loss: 1.6369345985367545e-06\n",
            "Epoch: 358, Loss: 1.5948608051985502e-06\n",
            "Epoch: 359, Loss: 1.562428906254354e-06\n",
            "Epoch: 360, Loss: 1.5278058071999112e-06\n",
            "Epoch: 361, Loss: 1.4997565358498832e-06\n",
            "Epoch: 362, Loss: 1.4730221664649434e-06\n",
            "Epoch: 363, Loss: 1.4397138556887512e-06\n",
            "Epoch: 364, Loss: 1.4178004903442343e-06\n",
            "Epoch: 365, Loss: 1.3827389011566993e-06\n",
            "Epoch: 366, Loss: 1.352936578769004e-06\n",
            "Epoch: 367, Loss: 1.3318997389433207e-06\n",
            "Epoch: 368, Loss: 1.3095479971525492e-06\n",
            "Epoch: 369, Loss: 1.2854432043241104e-06\n",
            "Epoch: 370, Loss: 1.2661595292229322e-06\n",
            "Epoch: 371, Loss: 1.2315364301684895e-06\n",
            "Epoch: 372, Loss: 1.209184688377718e-06\n",
            "Epoch: 373, Loss: 1.185518158308696e-06\n",
            "Epoch: 374, Loss: 1.1622897773122531e-06\n",
            "Epoch: 375, Loss: 1.1403762982808985e-06\n",
            "Epoch: 376, Loss: 1.1149567171742092e-06\n",
            "Epoch: 377, Loss: 1.0882222341024317e-06\n",
            "Epoch: 378, Loss: 1.0904137752731913e-06\n",
            "Epoch: 379, Loss: 1.160974875347165e-06\n",
            "Epoch: 380, Loss: 1.0798951279866742e-06\n",
            "Epoch: 381, Loss: 1.0321239187760511e-06\n",
            "Epoch: 382, Loss: 1.0071426004287787e-06\n",
            "Epoch: 383, Loss: 9.812847565626726e-07\n",
            "Epoch: 384, Loss: 9.70766336649831e-07\n",
            "Epoch: 385, Loss: 9.479762752562237e-07\n",
            "Epoch: 386, Loss: 9.348282219434623e-07\n",
            "Epoch: 387, Loss: 9.365812729811296e-07\n",
            "Epoch: 388, Loss: 9.094085839933541e-07\n",
            "Epoch: 389, Loss: 8.896865324459213e-07\n",
            "Epoch: 390, Loss: 8.769767418925767e-07\n",
            "Epoch: 391, Loss: 8.611990551798954e-07\n",
            "Epoch: 392, Loss: 8.454213116237952e-07\n",
            "Epoch: 393, Loss: 8.313968464790378e-07\n",
            "Epoch: 394, Loss: 8.243845286415308e-07\n",
            "Epoch: 395, Loss: 8.086067850854306e-07\n",
            "Epoch: 396, Loss: 8.037859515752643e-07\n",
            "Epoch: 397, Loss: 7.875699452597473e-07\n",
            "Epoch: 398, Loss: 7.831872039787413e-07\n",
            "Epoch: 399, Loss: 7.660947289878095e-07\n",
            "Epoch: 400, Loss: 7.551379894721322e-07\n",
            "Epoch: 401, Loss: 7.476873520317895e-07\n",
            "Epoch: 402, Loss: 7.406750341942825e-07\n",
            "Epoch: 403, Loss: 7.38922040000034e-07\n",
            "Epoch: 404, Loss: 7.262121926032705e-07\n",
            "Epoch: 405, Loss: 7.2489740432502e-07\n",
            "Epoch: 406, Loss: 7.082431920935051e-07\n",
            "Epoch: 407, Loss: 7.042987704153347e-07\n",
            "Epoch: 408, Loss: 6.902741915837396e-07\n",
            "Epoch: 409, Loss: 6.915889798619901e-07\n",
            "Epoch: 410, Loss: 6.744964480276394e-07\n",
            "Epoch: 411, Loss: 6.819470286245632e-07\n",
            "Epoch: 412, Loss: 6.784408697058097e-07\n",
            "Epoch: 413, Loss: 6.652928732364671e-07\n",
            "Epoch: 414, Loss: 6.613483947148779e-07\n",
            "Epoch: 415, Loss: 6.53897870961373e-07\n",
            "Epoch: 416, Loss: 6.512682375614531e-07\n",
            "Epoch: 417, Loss: 6.473238158832828e-07\n",
            "Epoch: 418, Loss: 6.442559197239461e-07\n",
            "Epoch: 419, Loss: 6.389967097675253e-07\n",
            "Epoch: 420, Loss: 6.372436018864391e-07\n",
            "Epoch: 421, Loss: 6.337374429676856e-07\n",
            "Epoch: 422, Loss: 6.271634447330143e-07\n",
            "Epoch: 423, Loss: 6.28039970251848e-07\n",
            "Epoch: 424, Loss: 6.240956054170965e-07\n",
            "Epoch: 425, Loss: 6.175214934955875e-07\n",
            "Epoch: 426, Loss: 6.166450248201727e-07\n",
            "Epoch: 427, Loss: 6.127004894551646e-07\n",
            "Epoch: 428, Loss: 6.127004894551646e-07\n",
            "Epoch: 429, Loss: 6.122622835391667e-07\n",
            "Epoch: 430, Loss: 6.087561246204132e-07\n",
            "Epoch: 431, Loss: 6.074412794987438e-07\n",
            "Epoch: 432, Loss: 6.0919438737983e-07\n",
            "Epoch: 433, Loss: 6.078795422581607e-07\n",
            "Epoch: 434, Loss: 6.056882853044954e-07\n",
            "Epoch: 435, Loss: 6.056881716176576e-07\n",
            "Epoch: 436, Loss: 6.02620275458321e-07\n",
            "Epoch: 437, Loss: 6.043733833394072e-07\n",
            "Epoch: 438, Loss: 6.030585950611567e-07\n",
            "Epoch: 439, Loss: 6.039351205799903e-07\n",
            "Epoch: 440, Loss: 6.061264343770745e-07\n",
            "Epoch: 441, Loss: 6.061264343770745e-07\n",
            "Epoch: 442, Loss: 5.9999069890182e-07\n",
            "Epoch: 443, Loss: 5.9999069890182e-07\n",
            "Epoch: 444, Loss: 5.9999069890182e-07\n",
            "Epoch: 445, Loss: 6.284782330112648e-07\n",
            "Epoch: 446, Loss: 6.034968009771546e-07\n",
            "Epoch: 447, Loss: 5.999907557452389e-07\n",
            "Epoch: 448, Loss: 5.982377047075715e-07\n",
            "Epoch: 449, Loss: 6.039350637365715e-07\n",
            "Epoch: 450, Loss: 6.017438067829062e-07\n",
            "Epoch: 451, Loss: 5.991141733829863e-07\n",
            "Epoch: 452, Loss: 5.960462772236497e-07\n",
            "Epoch: 453, Loss: 5.97361122345319e-07\n",
            "Epoch: 454, Loss: 5.982376478641527e-07\n",
            "Epoch: 455, Loss: 5.977993851047358e-07\n",
            "Epoch: 456, Loss: 6.017438067829062e-07\n",
            "Epoch: 457, Loss: 5.969228027424833e-07\n",
            "Epoch: 458, Loss: 6.013055440234893e-07\n",
            "Epoch: 459, Loss: 5.947314321019803e-07\n",
            "Epoch: 460, Loss: 6.030585950611567e-07\n",
            "Epoch: 461, Loss: 5.97361122345319e-07\n",
            "Epoch: 462, Loss: 6.034968578205735e-07\n",
            "Epoch: 463, Loss: 6.013055440234893e-07\n",
            "Epoch: 464, Loss: 6.030585950611567e-07\n",
            "Epoch: 465, Loss: 6.083178050175775e-07\n",
            "Epoch: 466, Loss: 6.056882284610765e-07\n",
            "Epoch: 467, Loss: 6.052500225450785e-07\n",
            "Epoch: 468, Loss: 6.0919438737983e-07\n",
            "Epoch: 469, Loss: 6.179597562550043e-07\n",
            "Epoch: 470, Loss: 6.056882284610765e-07\n",
            "Epoch: 471, Loss: 6.0919438737983e-07\n",
            "Epoch: 472, Loss: 6.056882284610765e-07\n",
            "Epoch: 473, Loss: 6.065647539799102e-07\n",
            "Epoch: 474, Loss: 6.034968578205735e-07\n",
            "Epoch: 475, Loss: 6.078795422581607e-07\n",
            "Epoch: 476, Loss: 6.087561246204132e-07\n",
            "Epoch: 477, Loss: 6.135770718174172e-07\n",
            "Epoch: 478, Loss: 6.113857011769142e-07\n",
            "Epoch: 479, Loss: 6.109474384174973e-07\n",
            "Epoch: 480, Loss: 6.11823963936331e-07\n",
            "Epoch: 481, Loss: 6.100709128986637e-07\n",
            "Epoch: 482, Loss: 6.157683856145013e-07\n",
            "Epoch: 483, Loss: 6.144535973362508e-07\n",
            "Epoch: 484, Loss: 6.14015334576834e-07\n",
            "Epoch: 485, Loss: 6.127005462985835e-07\n",
            "Epoch: 486, Loss: 6.14015334576834e-07\n",
            "Epoch: 487, Loss: 6.153301228550845e-07\n",
            "Epoch: 488, Loss: 6.170832307361707e-07\n",
            "Epoch: 489, Loss: 6.179598130984232e-07\n",
            "Epoch: 490, Loss: 6.148919169390865e-07\n",
            "Epoch: 491, Loss: 6.179598130984232e-07\n",
            "Epoch: 492, Loss: 6.254103368519282e-07\n",
            "Epoch: 493, Loss: 6.219041779331747e-07\n",
            "Epoch: 494, Loss: 6.183980190144212e-07\n",
            "Epoch: 495, Loss: 6.249721309359302e-07\n",
            "Epoch: 496, Loss: 6.23219023054844e-07\n",
            "Epoch: 497, Loss: 6.210277092577599e-07\n",
            "Epoch: 498, Loss: 6.236572858142608e-07\n",
            "Epoch: 499, Loss: 6.262868623707618e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "block_5_emd_128_relu"
      ],
      "metadata": {
        "id": "4-WkdaOZ4869"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 2]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size = 5  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 128\n",
        "activation = 'relu'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(500):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_block_5_emd_64_relu': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'block_5_emd_128_relu.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbtktDor5LcA",
        "outputId": "6f25deb1-92b8-4ca7-c317-7f918e8c106e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.63348388671875\n",
            "Epoch: 1, Loss: 7.856359481811523\n",
            "Epoch: 2, Loss: 7.1545867919921875\n",
            "Epoch: 3, Loss: 6.368104934692383\n",
            "Epoch: 4, Loss: 5.366426467895508\n",
            "Epoch: 5, Loss: 4.207087516784668\n",
            "Epoch: 6, Loss: 3.2168354988098145\n",
            "Epoch: 7, Loss: 2.587195873260498\n",
            "Epoch: 8, Loss: 2.1908743381500244\n",
            "Epoch: 9, Loss: 1.898285150527954\n",
            "Epoch: 10, Loss: 1.667164921760559\n",
            "Epoch: 11, Loss: 1.4722579717636108\n",
            "Epoch: 12, Loss: 1.3136098384857178\n",
            "Epoch: 13, Loss: 1.1753268241882324\n",
            "Epoch: 14, Loss: 1.0522266626358032\n",
            "Epoch: 15, Loss: 0.9435500502586365\n",
            "Epoch: 16, Loss: 0.8448870778083801\n",
            "Epoch: 17, Loss: 0.7542304396629333\n",
            "Epoch: 18, Loss: 0.6686856150627136\n",
            "Epoch: 19, Loss: 0.5934704542160034\n",
            "Epoch: 20, Loss: 0.5230286717414856\n",
            "Epoch: 21, Loss: 0.45843306183815\n",
            "Epoch: 22, Loss: 0.39737680554389954\n",
            "Epoch: 23, Loss: 0.349508672952652\n",
            "Epoch: 24, Loss: 0.3137682378292084\n",
            "Epoch: 25, Loss: 0.2714890241622925\n",
            "Epoch: 26, Loss: 0.23242518305778503\n",
            "Epoch: 27, Loss: 0.2036321461200714\n",
            "Epoch: 28, Loss: 0.17679327726364136\n",
            "Epoch: 29, Loss: 0.1576177477836609\n",
            "Epoch: 30, Loss: 0.13756263256072998\n",
            "Epoch: 31, Loss: 0.12166809290647507\n",
            "Epoch: 32, Loss: 0.10608940571546555\n",
            "Epoch: 33, Loss: 0.09601548314094543\n",
            "Epoch: 34, Loss: 0.08396740257740021\n",
            "Epoch: 35, Loss: 0.07680980116128922\n",
            "Epoch: 36, Loss: 0.0682809129357338\n",
            "Epoch: 37, Loss: 0.06520583480596542\n",
            "Epoch: 38, Loss: 0.05823247507214546\n",
            "Epoch: 39, Loss: 0.050337113440036774\n",
            "Epoch: 40, Loss: 0.048453204333782196\n",
            "Epoch: 41, Loss: 0.042106252163648605\n",
            "Epoch: 42, Loss: 0.040086809545755386\n",
            "Epoch: 43, Loss: 0.03613433986902237\n",
            "Epoch: 44, Loss: 0.03421492129564285\n",
            "Epoch: 45, Loss: 0.030964436009526253\n",
            "Epoch: 46, Loss: 0.029796218499541283\n",
            "Epoch: 47, Loss: 0.026920681819319725\n",
            "Epoch: 48, Loss: 0.027247030287981033\n",
            "Epoch: 49, Loss: 0.02301018312573433\n",
            "Epoch: 50, Loss: 0.025080984458327293\n",
            "Epoch: 51, Loss: 0.020276956260204315\n",
            "Epoch: 52, Loss: 0.021584706380963326\n",
            "Epoch: 53, Loss: 0.01774575747549534\n",
            "Epoch: 54, Loss: 0.018461383879184723\n",
            "Epoch: 55, Loss: 0.016291629523038864\n",
            "Epoch: 56, Loss: 0.014495661482214928\n",
            "Epoch: 57, Loss: 0.02053428627550602\n",
            "Epoch: 58, Loss: 0.012719455175101757\n",
            "Epoch: 59, Loss: 0.033704496920108795\n",
            "Epoch: 60, Loss: 0.015070649795234203\n",
            "Epoch: 61, Loss: 0.011829948052763939\n",
            "Epoch: 62, Loss: 0.011278808116912842\n",
            "Epoch: 63, Loss: 0.008709956891834736\n",
            "Epoch: 64, Loss: 0.010741707868874073\n",
            "Epoch: 65, Loss: 0.04201862961053848\n",
            "Epoch: 66, Loss: 0.01560890581458807\n",
            "Epoch: 67, Loss: 0.011576161719858646\n",
            "Epoch: 68, Loss: 0.010150952264666557\n",
            "Epoch: 69, Loss: 0.06112181767821312\n",
            "Epoch: 70, Loss: 0.03461059182882309\n",
            "Epoch: 71, Loss: 0.015041581355035305\n",
            "Epoch: 72, Loss: 0.008268925361335278\n",
            "Epoch: 73, Loss: 0.012828542850911617\n",
            "Epoch: 74, Loss: 0.07472650706768036\n",
            "Epoch: 75, Loss: 0.03844049945473671\n",
            "Epoch: 76, Loss: 0.010526471771299839\n",
            "Epoch: 77, Loss: 0.01739000342786312\n",
            "Epoch: 78, Loss: 0.005549947265535593\n",
            "Epoch: 79, Loss: 0.006307767238467932\n",
            "Epoch: 80, Loss: 0.04967334866523743\n",
            "Epoch: 81, Loss: 0.030216723680496216\n",
            "Epoch: 82, Loss: 0.03746287152171135\n",
            "Epoch: 83, Loss: 0.009245638735592365\n",
            "Epoch: 84, Loss: 0.006666496396064758\n",
            "Epoch: 85, Loss: 0.04586295783519745\n",
            "Epoch: 86, Loss: 0.01851111836731434\n",
            "Epoch: 87, Loss: 0.009383883327245712\n",
            "Epoch: 88, Loss: 0.020121775567531586\n",
            "Epoch: 89, Loss: 0.004261808004230261\n",
            "Epoch: 90, Loss: 0.026900775730609894\n",
            "Epoch: 91, Loss: 0.009378878399729729\n",
            "Epoch: 92, Loss: 0.011164149269461632\n",
            "Epoch: 93, Loss: 0.0033465626183897257\n",
            "Epoch: 94, Loss: 0.05306084081530571\n",
            "Epoch: 95, Loss: 0.04616018384695053\n",
            "Epoch: 96, Loss: 0.017377516254782677\n",
            "Epoch: 97, Loss: 0.009378323331475258\n",
            "Epoch: 98, Loss: 0.006198983173817396\n",
            "Epoch: 99, Loss: 0.01596045307815075\n",
            "Epoch: 100, Loss: 0.010270305909216404\n",
            "Epoch: 101, Loss: 0.05671168118715286\n",
            "Epoch: 102, Loss: 0.01571895182132721\n",
            "Epoch: 103, Loss: 0.014013111591339111\n",
            "Epoch: 104, Loss: 0.004860070534050465\n",
            "Epoch: 105, Loss: 0.009598339907824993\n",
            "Epoch: 106, Loss: 0.007103195413947105\n",
            "Epoch: 107, Loss: 0.007589451968669891\n",
            "Epoch: 108, Loss: 0.008070738054811954\n",
            "Epoch: 109, Loss: 0.005216027610003948\n",
            "Epoch: 110, Loss: 0.017921175807714462\n",
            "Epoch: 111, Loss: 0.008800298906862736\n",
            "Epoch: 112, Loss: 0.009572502225637436\n",
            "Epoch: 113, Loss: 0.006140990648418665\n",
            "Epoch: 114, Loss: 0.008990537375211716\n",
            "Epoch: 115, Loss: 0.007160649169236422\n",
            "Epoch: 116, Loss: 0.00616386579349637\n",
            "Epoch: 117, Loss: 0.011274415999650955\n",
            "Epoch: 118, Loss: 0.008055930957198143\n",
            "Epoch: 119, Loss: 0.006692363880574703\n",
            "Epoch: 120, Loss: 0.010459664277732372\n",
            "Epoch: 121, Loss: 0.005995437037199736\n",
            "Epoch: 122, Loss: 0.007716438267379999\n",
            "Epoch: 123, Loss: 0.006528292782604694\n",
            "Epoch: 124, Loss: 0.008686705492436886\n",
            "Epoch: 125, Loss: 0.006439104210585356\n",
            "Epoch: 126, Loss: 0.007201037369668484\n",
            "Epoch: 127, Loss: 0.005377287045121193\n",
            "Epoch: 128, Loss: 0.011104829609394073\n",
            "Epoch: 129, Loss: 0.006057011894881725\n",
            "Epoch: 130, Loss: 0.011896448209881783\n",
            "Epoch: 131, Loss: 0.0062391385436058044\n",
            "Epoch: 132, Loss: 0.009016403928399086\n",
            "Epoch: 133, Loss: 0.006557986605912447\n",
            "Epoch: 134, Loss: 0.009835831820964813\n",
            "Epoch: 135, Loss: 0.0069348448887467384\n",
            "Epoch: 136, Loss: 0.007940718904137611\n",
            "Epoch: 137, Loss: 0.008098682388663292\n",
            "Epoch: 138, Loss: 0.0066132997162640095\n",
            "Epoch: 139, Loss: 0.009849338792264462\n",
            "Epoch: 140, Loss: 0.006927902810275555\n",
            "Epoch: 141, Loss: 0.010591342113912106\n",
            "Epoch: 142, Loss: 0.0062709772028028965\n",
            "Epoch: 143, Loss: 0.016582390293478966\n",
            "Epoch: 144, Loss: 0.010466794483363628\n",
            "Epoch: 145, Loss: 0.014247247949242592\n",
            "Epoch: 146, Loss: 0.007622969802469015\n",
            "Epoch: 147, Loss: 0.0075626070611178875\n",
            "Epoch: 148, Loss: 0.007555057760328054\n",
            "Epoch: 149, Loss: 0.008076483383774757\n",
            "Epoch: 150, Loss: 0.0063777160830795765\n",
            "Epoch: 151, Loss: 0.009462214075028896\n",
            "Epoch: 152, Loss: 0.0070940651930868626\n",
            "Epoch: 153, Loss: 0.008735007606446743\n",
            "Epoch: 154, Loss: 0.00706857955083251\n",
            "Epoch: 155, Loss: 0.009044441394507885\n",
            "Epoch: 156, Loss: 0.00630725035443902\n",
            "Epoch: 157, Loss: 0.009005573578178883\n",
            "Epoch: 158, Loss: 0.0064996578730642796\n",
            "Epoch: 159, Loss: 0.008827608078718185\n",
            "Epoch: 160, Loss: 0.0072809383273124695\n",
            "Epoch: 161, Loss: 0.0077719190157949924\n",
            "Epoch: 162, Loss: 0.007360666524618864\n",
            "Epoch: 163, Loss: 0.006953137926757336\n",
            "Epoch: 164, Loss: 0.009710286743938923\n",
            "Epoch: 165, Loss: 0.006998597178608179\n",
            "Epoch: 166, Loss: 0.008438121527433395\n",
            "Epoch: 167, Loss: 0.007496169302612543\n",
            "Epoch: 168, Loss: 0.007166466675698757\n",
            "Epoch: 169, Loss: 0.009426448494195938\n",
            "Epoch: 170, Loss: 0.006557815708220005\n",
            "Epoch: 171, Loss: 0.009615832008421421\n",
            "Epoch: 172, Loss: 0.006454240996390581\n",
            "Epoch: 173, Loss: 0.009883743710815907\n",
            "Epoch: 174, Loss: 0.007014439441263676\n",
            "Epoch: 175, Loss: 0.008382127620279789\n",
            "Epoch: 176, Loss: 0.008344636298716068\n",
            "Epoch: 177, Loss: 0.006174472160637379\n",
            "Epoch: 178, Loss: 0.009940726682543755\n",
            "Epoch: 179, Loss: 0.005996251478791237\n",
            "Epoch: 180, Loss: 0.011373990215361118\n",
            "Epoch: 181, Loss: 0.007512487471103668\n",
            "Epoch: 182, Loss: 0.008623570203781128\n",
            "Epoch: 183, Loss: 0.007668650709092617\n",
            "Epoch: 184, Loss: 0.0071795289404690266\n",
            "Epoch: 185, Loss: 0.009226392023265362\n",
            "Epoch: 186, Loss: 0.006020142696797848\n",
            "Epoch: 187, Loss: 0.010083891451358795\n",
            "Epoch: 188, Loss: 0.00755350710824132\n",
            "Epoch: 189, Loss: 0.00778367230668664\n",
            "Epoch: 190, Loss: 0.008361786603927612\n",
            "Epoch: 191, Loss: 0.0069436561316251755\n",
            "Epoch: 192, Loss: 0.009375691413879395\n",
            "Epoch: 193, Loss: 0.007097122725099325\n",
            "Epoch: 194, Loss: 0.00873717200011015\n",
            "Epoch: 195, Loss: 0.007100953254848719\n",
            "Epoch: 196, Loss: 0.01947726495563984\n",
            "Epoch: 197, Loss: 0.009625480510294437\n",
            "Epoch: 198, Loss: 0.009143495932221413\n",
            "Epoch: 199, Loss: 0.006639894098043442\n",
            "Epoch: 200, Loss: 0.010139978490769863\n",
            "Epoch: 201, Loss: 0.006954140495508909\n",
            "Epoch: 202, Loss: 0.010078406892716885\n",
            "Epoch: 203, Loss: 0.006899533327668905\n",
            "Epoch: 204, Loss: 0.010499385185539722\n",
            "Epoch: 205, Loss: 0.005889853462576866\n",
            "Epoch: 206, Loss: 0.008450845256447792\n",
            "Epoch: 207, Loss: 0.007607480511069298\n",
            "Epoch: 208, Loss: 0.007809535134583712\n",
            "Epoch: 209, Loss: 0.007548416033387184\n",
            "Epoch: 210, Loss: 0.007885811850428581\n",
            "Epoch: 211, Loss: 0.007456799037754536\n",
            "Epoch: 212, Loss: 0.008020116947591305\n",
            "Epoch: 213, Loss: 0.007290787063539028\n",
            "Epoch: 214, Loss: 0.008282480761408806\n",
            "Epoch: 215, Loss: 0.007455775048583746\n",
            "Epoch: 216, Loss: 0.007892723195254803\n",
            "Epoch: 217, Loss: 0.007740158587694168\n",
            "Epoch: 218, Loss: 0.007587057538330555\n",
            "Epoch: 219, Loss: 0.008054698817431927\n",
            "Epoch: 220, Loss: 0.006918695755302906\n",
            "Epoch: 221, Loss: 0.008357522077858448\n",
            "Epoch: 222, Loss: 0.007314322050660849\n",
            "Epoch: 223, Loss: 0.008012191392481327\n",
            "Epoch: 224, Loss: 0.007014485541731119\n",
            "Epoch: 225, Loss: 0.008290126919746399\n",
            "Epoch: 226, Loss: 0.007675881031900644\n",
            "Epoch: 227, Loss: 0.007529750000685453\n",
            "Epoch: 228, Loss: 0.007927426137030125\n",
            "Epoch: 229, Loss: 0.0073688882403075695\n",
            "Epoch: 230, Loss: 0.007311446592211723\n",
            "Epoch: 231, Loss: 0.009042659774422646\n",
            "Epoch: 232, Loss: 0.006581089925020933\n",
            "Epoch: 233, Loss: 0.009379242546856403\n",
            "Epoch: 234, Loss: 0.0070676603354513645\n",
            "Epoch: 235, Loss: 0.00892876647412777\n",
            "Epoch: 236, Loss: 0.008219888433814049\n",
            "Epoch: 237, Loss: 0.009379332885146141\n",
            "Epoch: 238, Loss: 0.0065043289214372635\n",
            "Epoch: 239, Loss: 0.009661047719419003\n",
            "Epoch: 240, Loss: 0.007153242360800505\n",
            "Epoch: 241, Loss: 0.008957911282777786\n",
            "Epoch: 242, Loss: 0.007005160208791494\n",
            "Epoch: 243, Loss: 0.008525869809091091\n",
            "Epoch: 244, Loss: 0.006907762493938208\n",
            "Epoch: 245, Loss: 0.008661339990794659\n",
            "Epoch: 246, Loss: 0.007107846438884735\n",
            "Epoch: 247, Loss: 0.01716635376214981\n",
            "Epoch: 248, Loss: 0.010123765096068382\n",
            "Epoch: 249, Loss: 0.007250454276800156\n",
            "Epoch: 250, Loss: 0.008662134408950806\n",
            "Epoch: 251, Loss: 0.007290916983038187\n",
            "Epoch: 252, Loss: 0.008817305788397789\n",
            "Epoch: 253, Loss: 0.007149685174226761\n",
            "Epoch: 254, Loss: 0.009219738654792309\n",
            "Epoch: 255, Loss: 0.006442902144044638\n",
            "Epoch: 256, Loss: 0.008350308984518051\n",
            "Epoch: 257, Loss: 0.007084758952260017\n",
            "Epoch: 258, Loss: 0.007548514753580093\n",
            "Epoch: 259, Loss: 0.007670689839869738\n",
            "Epoch: 260, Loss: 0.007617472670972347\n",
            "Epoch: 261, Loss: 0.007643993943929672\n",
            "Epoch: 262, Loss: 0.007617879193276167\n",
            "Epoch: 263, Loss: 0.0076810079626739025\n",
            "Epoch: 264, Loss: 0.007635518908500671\n",
            "Epoch: 265, Loss: 0.007699290756136179\n",
            "Epoch: 266, Loss: 0.0076195853762328625\n",
            "Epoch: 267, Loss: 0.007747430354356766\n",
            "Epoch: 268, Loss: 0.00759460823610425\n",
            "Epoch: 269, Loss: 0.0077575258910655975\n",
            "Epoch: 270, Loss: 0.007622164208441973\n",
            "Epoch: 271, Loss: 0.008111398667097092\n",
            "Epoch: 272, Loss: 0.007470162119716406\n",
            "Epoch: 273, Loss: 0.0077207088470458984\n",
            "Epoch: 274, Loss: 0.007647037506103516\n",
            "Epoch: 275, Loss: 0.007706031668931246\n",
            "Epoch: 276, Loss: 0.007368407212197781\n",
            "Epoch: 277, Loss: 0.00786399096250534\n",
            "Epoch: 278, Loss: 0.007401797920465469\n",
            "Epoch: 279, Loss: 0.00787401758134365\n",
            "Epoch: 280, Loss: 0.007276606746017933\n",
            "Epoch: 281, Loss: 0.007872127927839756\n",
            "Epoch: 282, Loss: 0.007318203337490559\n",
            "Epoch: 283, Loss: 0.007839192636311054\n",
            "Epoch: 284, Loss: 0.0073632956482470036\n",
            "Epoch: 285, Loss: 0.019524261355400085\n",
            "Epoch: 286, Loss: 0.0070053464733064175\n",
            "Epoch: 287, Loss: 0.007700612302869558\n",
            "Epoch: 288, Loss: 0.007634967099875212\n",
            "Epoch: 289, Loss: 0.007696547545492649\n",
            "Epoch: 290, Loss: 0.007591870613396168\n",
            "Epoch: 291, Loss: 0.007645801641047001\n",
            "Epoch: 292, Loss: 0.007598589640110731\n",
            "Epoch: 293, Loss: 0.007630666717886925\n",
            "Epoch: 294, Loss: 0.007583459373563528\n",
            "Epoch: 295, Loss: 0.00763064157217741\n",
            "Epoch: 296, Loss: 0.007583498023450375\n",
            "Epoch: 297, Loss: 0.007630480919033289\n",
            "Epoch: 298, Loss: 0.0075768292881548405\n",
            "Epoch: 299, Loss: 0.007645676378160715\n",
            "Epoch: 300, Loss: 0.007549835368990898\n",
            "Epoch: 301, Loss: 0.007677492685616016\n",
            "Epoch: 302, Loss: 0.0075143566355109215\n",
            "Epoch: 303, Loss: 0.00773537578061223\n",
            "Epoch: 304, Loss: 0.007458250503987074\n",
            "Epoch: 305, Loss: 0.007823732681572437\n",
            "Epoch: 306, Loss: 0.007322520948946476\n",
            "Epoch: 307, Loss: 0.0079236701130867\n",
            "Epoch: 308, Loss: 0.00720412191003561\n",
            "Epoch: 309, Loss: 0.00794279109686613\n",
            "Epoch: 310, Loss: 0.007140258327126503\n",
            "Epoch: 311, Loss: 0.007910918444395065\n",
            "Epoch: 312, Loss: 0.007140541914850473\n",
            "Epoch: 313, Loss: 0.007992264814674854\n",
            "Epoch: 314, Loss: 0.0071425000205636024\n",
            "Epoch: 315, Loss: 0.007985352538526058\n",
            "Epoch: 316, Loss: 0.007139983121305704\n",
            "Epoch: 317, Loss: 0.008007883094251156\n",
            "Epoch: 318, Loss: 0.007160171400755644\n",
            "Epoch: 319, Loss: 0.008002319373190403\n",
            "Epoch: 320, Loss: 0.007170715369284153\n",
            "Epoch: 321, Loss: 0.008032931946218014\n",
            "Epoch: 322, Loss: 0.007130623795092106\n",
            "Epoch: 323, Loss: 0.007928953506052494\n",
            "Epoch: 324, Loss: 0.007398436311632395\n",
            "Epoch: 325, Loss: 0.00895149540156126\n",
            "Epoch: 326, Loss: 0.009165377356112003\n",
            "Epoch: 327, Loss: 0.00826091319322586\n",
            "Epoch: 328, Loss: 0.008083980530500412\n",
            "Epoch: 329, Loss: 0.007289219181984663\n",
            "Epoch: 330, Loss: 0.00754655571654439\n",
            "Epoch: 331, Loss: 0.007495517376810312\n",
            "Epoch: 332, Loss: 0.007599699776619673\n",
            "Epoch: 333, Loss: 0.007526048459112644\n",
            "Epoch: 334, Loss: 0.007597620598971844\n",
            "Epoch: 335, Loss: 0.00754735479131341\n",
            "Epoch: 336, Loss: 0.00760333426296711\n",
            "Epoch: 337, Loss: 0.00755019998177886\n",
            "Epoch: 338, Loss: 0.007593467831611633\n",
            "Epoch: 339, Loss: 0.007544954773038626\n",
            "Epoch: 340, Loss: 0.007615899201482534\n",
            "Epoch: 341, Loss: 0.007525586057454348\n",
            "Epoch: 342, Loss: 0.0076392339542508125\n",
            "Epoch: 343, Loss: 0.0075257509015500546\n",
            "Epoch: 344, Loss: 0.007611589040607214\n",
            "Epoch: 345, Loss: 0.007533937692642212\n",
            "Epoch: 346, Loss: 0.0076314606703817844\n",
            "Epoch: 347, Loss: 0.007519368547946215\n",
            "Epoch: 348, Loss: 0.007618957199156284\n",
            "Epoch: 349, Loss: 0.0075257811695337296\n",
            "Epoch: 350, Loss: 0.007594458758831024\n",
            "Epoch: 351, Loss: 0.007525929249823093\n",
            "Epoch: 352, Loss: 0.007601110730320215\n",
            "Epoch: 353, Loss: 0.007498082704842091\n",
            "Epoch: 354, Loss: 0.007636032998561859\n",
            "Epoch: 355, Loss: 0.007448194548487663\n",
            "Epoch: 356, Loss: 0.007666607387363911\n",
            "Epoch: 357, Loss: 0.007325389888137579\n",
            "Epoch: 358, Loss: 0.0076603153720498085\n",
            "Epoch: 359, Loss: 0.0072469753213226795\n",
            "Epoch: 360, Loss: 0.007810646668076515\n",
            "Epoch: 361, Loss: 0.007275911048054695\n",
            "Epoch: 362, Loss: 0.007801620755344629\n",
            "Epoch: 363, Loss: 0.0073051732033491135\n",
            "Epoch: 364, Loss: 0.007832101546227932\n",
            "Epoch: 365, Loss: 0.007364312186837196\n",
            "Epoch: 366, Loss: 0.007733011152595282\n",
            "Epoch: 367, Loss: 0.010376064106822014\n",
            "Epoch: 368, Loss: 0.007143690250813961\n",
            "Epoch: 369, Loss: 0.007446977309882641\n",
            "Epoch: 370, Loss: 0.007526827976107597\n",
            "Epoch: 371, Loss: 0.0074897524900734425\n",
            "Epoch: 372, Loss: 0.007539473474025726\n",
            "Epoch: 373, Loss: 0.007519894279539585\n",
            "Epoch: 374, Loss: 0.007524529471993446\n",
            "Epoch: 375, Loss: 0.0075173648074269295\n",
            "Epoch: 376, Loss: 0.007515487261116505\n",
            "Epoch: 377, Loss: 0.007509724237024784\n",
            "Epoch: 378, Loss: 0.0075096189975738525\n",
            "Epoch: 379, Loss: 0.007518330588936806\n",
            "Epoch: 380, Loss: 0.007511757779866457\n",
            "Epoch: 381, Loss: 0.007535419426858425\n",
            "Epoch: 382, Loss: 0.007509852759540081\n",
            "Epoch: 383, Loss: 0.007550560869276524\n",
            "Epoch: 384, Loss: 0.007492631673812866\n",
            "Epoch: 385, Loss: 0.0076075587421655655\n",
            "Epoch: 386, Loss: 0.007397222798317671\n",
            "Epoch: 387, Loss: 0.007586768362671137\n",
            "Epoch: 388, Loss: 0.007453908212482929\n",
            "Epoch: 389, Loss: 0.007584742736071348\n",
            "Epoch: 390, Loss: 0.007439055480062962\n",
            "Epoch: 391, Loss: 0.007602965459227562\n",
            "Epoch: 392, Loss: 0.007410361431539059\n",
            "Epoch: 393, Loss: 0.007629108149558306\n",
            "Epoch: 394, Loss: 0.007361495867371559\n",
            "Epoch: 395, Loss: 0.007662029936909676\n",
            "Epoch: 396, Loss: 0.007315175607800484\n",
            "Epoch: 397, Loss: 0.007661328185349703\n",
            "Epoch: 398, Loss: 0.0073497099801898\n",
            "Epoch: 399, Loss: 0.007666205056011677\n",
            "Epoch: 400, Loss: 0.007316514849662781\n",
            "Epoch: 401, Loss: 0.0076626259833574295\n",
            "Epoch: 402, Loss: 0.007358983159065247\n",
            "Epoch: 403, Loss: 0.007664194330573082\n",
            "Epoch: 404, Loss: 0.007373378612101078\n",
            "Epoch: 405, Loss: 0.007676582783460617\n",
            "Epoch: 406, Loss: 0.0073501295410096645\n",
            "Epoch: 407, Loss: 0.0076936595141887665\n",
            "Epoch: 408, Loss: 0.00732866395264864\n",
            "Epoch: 409, Loss: 0.0076758344657719135\n",
            "Epoch: 410, Loss: 0.00733955716714263\n",
            "Epoch: 411, Loss: 0.007692974526435137\n",
            "Epoch: 412, Loss: 0.007852881215512753\n",
            "Epoch: 413, Loss: 0.010287860408425331\n",
            "Epoch: 414, Loss: 0.007709771394729614\n",
            "Epoch: 415, Loss: 0.008632563054561615\n",
            "Epoch: 416, Loss: 0.006042185705155134\n",
            "Epoch: 417, Loss: 0.008017493411898613\n",
            "Epoch: 418, Loss: 0.007621032185852528\n",
            "Epoch: 419, Loss: 0.007573986891657114\n",
            "Epoch: 420, Loss: 0.007504904642701149\n",
            "Epoch: 421, Loss: 0.007526759523898363\n",
            "Epoch: 422, Loss: 0.007478593848645687\n",
            "Epoch: 423, Loss: 0.007522010710090399\n",
            "Epoch: 424, Loss: 0.007480481173843145\n",
            "Epoch: 425, Loss: 0.007517606019973755\n",
            "Epoch: 426, Loss: 0.007478561718016863\n",
            "Epoch: 427, Loss: 0.007535850629210472\n",
            "Epoch: 428, Loss: 0.007470190059393644\n",
            "Epoch: 429, Loss: 0.007553731091320515\n",
            "Epoch: 430, Loss: 0.00744981225579977\n",
            "Epoch: 431, Loss: 0.007582693360745907\n",
            "Epoch: 432, Loss: 0.007418245077133179\n",
            "Epoch: 433, Loss: 0.007619447074830532\n",
            "Epoch: 434, Loss: 0.007387696299701929\n",
            "Epoch: 435, Loss: 0.0076431287452578545\n",
            "Epoch: 436, Loss: 0.007362517062574625\n",
            "Epoch: 437, Loss: 0.00763433612883091\n",
            "Epoch: 438, Loss: 0.007407146040350199\n",
            "Epoch: 439, Loss: 0.007517858408391476\n",
            "Epoch: 440, Loss: 0.007502879481762648\n",
            "Epoch: 441, Loss: 0.007494047284126282\n",
            "Epoch: 442, Loss: 0.00754856551066041\n",
            "Epoch: 443, Loss: 0.007455976214259863\n",
            "Epoch: 444, Loss: 0.007566894870251417\n",
            "Epoch: 445, Loss: 0.007426771800965071\n",
            "Epoch: 446, Loss: 0.0074745165184140205\n",
            "Epoch: 447, Loss: 0.007372409570962191\n",
            "Epoch: 448, Loss: 0.00766441086307168\n",
            "Epoch: 449, Loss: 0.00706738093867898\n",
            "Epoch: 450, Loss: 0.0076447343453764915\n",
            "Epoch: 451, Loss: 0.00741871353238821\n",
            "Epoch: 452, Loss: 0.0076807476580142975\n",
            "Epoch: 453, Loss: 0.007408650126308203\n",
            "Epoch: 454, Loss: 0.007646698504686356\n",
            "Epoch: 455, Loss: 0.00738717895001173\n",
            "Epoch: 456, Loss: 0.007588946260511875\n",
            "Epoch: 457, Loss: 0.007413211278617382\n",
            "Epoch: 458, Loss: 0.007551094982773066\n",
            "Epoch: 459, Loss: 0.007308404427021742\n",
            "Epoch: 460, Loss: 0.006675629410892725\n",
            "Epoch: 461, Loss: 0.007327082101255655\n",
            "Epoch: 462, Loss: 0.007467295974493027\n",
            "Epoch: 463, Loss: 0.007517276797443628\n",
            "Epoch: 464, Loss: 0.007514482829719782\n",
            "Epoch: 465, Loss: 0.007504102308303118\n",
            "Epoch: 466, Loss: 0.007500731386244297\n",
            "Epoch: 467, Loss: 0.007497308310121298\n",
            "Epoch: 468, Loss: 0.0074896602891385555\n",
            "Epoch: 469, Loss: 0.007491571828722954\n",
            "Epoch: 470, Loss: 0.0074884179048240185\n",
            "Epoch: 471, Loss: 0.007490123622119427\n",
            "Epoch: 472, Loss: 0.007495084777474403\n",
            "Epoch: 473, Loss: 0.007493681274354458\n",
            "Epoch: 474, Loss: 0.0074968463741242886\n",
            "Epoch: 475, Loss: 0.0074843550100922585\n",
            "Epoch: 476, Loss: 0.007482453249394894\n",
            "Epoch: 477, Loss: 0.007487793453037739\n",
            "Epoch: 478, Loss: 0.00748922023922205\n",
            "Epoch: 479, Loss: 0.007485595997422934\n",
            "Epoch: 480, Loss: 0.007487270049750805\n",
            "Epoch: 481, Loss: 0.007491414435207844\n",
            "Epoch: 482, Loss: 0.0074907634407281876\n",
            "Epoch: 483, Loss: 0.007504373788833618\n",
            "Epoch: 484, Loss: 0.007461421191692352\n",
            "Epoch: 485, Loss: 0.007546980399638414\n",
            "Epoch: 486, Loss: 0.007418820168823004\n",
            "Epoch: 487, Loss: 0.007594122085720301\n",
            "Epoch: 488, Loss: 0.007345280610024929\n",
            "Epoch: 489, Loss: 0.007617319002747536\n",
            "Epoch: 490, Loss: 0.007292458321899176\n",
            "Epoch: 491, Loss: 0.007597869262099266\n",
            "Epoch: 492, Loss: 0.007244167383760214\n",
            "Epoch: 493, Loss: 0.007593725807964802\n",
            "Epoch: 494, Loss: 0.007274579722434282\n",
            "Epoch: 495, Loss: 0.007619122043251991\n",
            "Epoch: 496, Loss: 0.007288893684744835\n",
            "Epoch: 497, Loss: 0.007635627873241901\n",
            "Epoch: 498, Loss: 0.007265836000442505\n",
            "Epoch: 499, Loss: 0.007648305501788855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "block_10_emd_128_relu"
      ],
      "metadata": {
        "id": "iBrNLULp5uOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 2]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size = 10  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 128\n",
        "activation = 'relu'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(500):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_block_10_emd_128_relu': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'block_10_emd_128_relu.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kauBuRN952qy",
        "outputId": "7bddcdde-ca60-42bc-dd00-66fb443cfe67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.739297866821289\n",
            "Epoch: 1, Loss: 7.886643886566162\n",
            "Epoch: 2, Loss: 7.148312568664551\n",
            "Epoch: 3, Loss: 6.291247844696045\n",
            "Epoch: 4, Loss: 5.220466613769531\n",
            "Epoch: 5, Loss: 3.99910044670105\n",
            "Epoch: 6, Loss: 3.0172231197357178\n",
            "Epoch: 7, Loss: 2.390960693359375\n",
            "Epoch: 8, Loss: 1.952859878540039\n",
            "Epoch: 9, Loss: 1.6152089834213257\n",
            "Epoch: 10, Loss: 1.3388868570327759\n",
            "Epoch: 11, Loss: 1.11168372631073\n",
            "Epoch: 12, Loss: 0.9142928719520569\n",
            "Epoch: 13, Loss: 0.7472448348999023\n",
            "Epoch: 14, Loss: 0.5981889963150024\n",
            "Epoch: 15, Loss: 0.46636223793029785\n",
            "Epoch: 16, Loss: 0.35080820322036743\n",
            "Epoch: 17, Loss: 0.25561538338661194\n",
            "Epoch: 18, Loss: 0.19444437325000763\n",
            "Epoch: 19, Loss: 0.1565970927476883\n",
            "Epoch: 20, Loss: 0.12750382721424103\n",
            "Epoch: 21, Loss: 0.10602066665887833\n",
            "Epoch: 22, Loss: 0.08841455727815628\n",
            "Epoch: 23, Loss: 0.07553381472826004\n",
            "Epoch: 24, Loss: 0.06361088156700134\n",
            "Epoch: 25, Loss: 0.054468344897031784\n",
            "Epoch: 26, Loss: 0.04630381613969803\n",
            "Epoch: 27, Loss: 0.039391182363033295\n",
            "Epoch: 28, Loss: 0.03351304307579994\n",
            "Epoch: 29, Loss: 0.02832738123834133\n",
            "Epoch: 30, Loss: 0.023722713813185692\n",
            "Epoch: 31, Loss: 0.020502515137195587\n",
            "Epoch: 32, Loss: 0.01771482452750206\n",
            "Epoch: 33, Loss: 0.0156426839530468\n",
            "Epoch: 34, Loss: 0.012521402910351753\n",
            "Epoch: 35, Loss: 0.009465941227972507\n",
            "Epoch: 36, Loss: 0.007852878421545029\n",
            "Epoch: 37, Loss: 0.0067636556923389435\n",
            "Epoch: 38, Loss: 0.005581169854849577\n",
            "Epoch: 39, Loss: 0.004840573761612177\n",
            "Epoch: 40, Loss: 0.004209437873214483\n",
            "Epoch: 41, Loss: 0.0037229193840175867\n",
            "Epoch: 42, Loss: 0.003303824458271265\n",
            "Epoch: 43, Loss: 0.002936163917183876\n",
            "Epoch: 44, Loss: 0.002631301060318947\n",
            "Epoch: 45, Loss: 0.0023706541396677494\n",
            "Epoch: 46, Loss: 0.0021307654678821564\n",
            "Epoch: 47, Loss: 0.00192750443238765\n",
            "Epoch: 48, Loss: 0.0017391223227605224\n",
            "Epoch: 49, Loss: 0.0015931904781609774\n",
            "Epoch: 50, Loss: 0.0014509379398077726\n",
            "Epoch: 51, Loss: 0.0014609771315008402\n",
            "Epoch: 52, Loss: 0.0012195197632536292\n",
            "Epoch: 53, Loss: 0.0013105643447488546\n",
            "Epoch: 54, Loss: 0.0014797613257542253\n",
            "Epoch: 55, Loss: 0.12931177020072937\n",
            "Epoch: 56, Loss: 0.11332476884126663\n",
            "Epoch: 57, Loss: 0.001627588295377791\n",
            "Epoch: 58, Loss: 0.0008871660684235394\n",
            "Epoch: 59, Loss: 0.0007688474142923951\n",
            "Epoch: 60, Loss: 0.0006215489702299237\n",
            "Epoch: 61, Loss: 0.0005523028667084873\n",
            "Epoch: 62, Loss: 0.0005374575848691165\n",
            "Epoch: 63, Loss: 0.00045312027214094996\n",
            "Epoch: 64, Loss: 0.00041413912549614906\n",
            "Epoch: 65, Loss: 0.00038086410495452583\n",
            "Epoch: 66, Loss: 0.0003528278029989451\n",
            "Epoch: 67, Loss: 0.0003255348128732294\n",
            "Epoch: 68, Loss: 0.0003013366076629609\n",
            "Epoch: 69, Loss: 0.00027814999339170754\n",
            "Epoch: 70, Loss: 0.0002573701785877347\n",
            "Epoch: 71, Loss: 0.0002382873499300331\n",
            "Epoch: 72, Loss: 0.0002207283250754699\n",
            "Epoch: 73, Loss: 0.0002037242375081405\n",
            "Epoch: 74, Loss: 0.00018870823259931058\n",
            "Epoch: 75, Loss: 0.00017355571617372334\n",
            "Epoch: 76, Loss: 0.00016116187907755375\n",
            "Epoch: 77, Loss: 0.00014888684381730855\n",
            "Epoch: 78, Loss: 0.0001378714805468917\n",
            "Epoch: 79, Loss: 0.00012710045848507434\n",
            "Epoch: 80, Loss: 0.00011749888653866947\n",
            "Epoch: 81, Loss: 0.00010822300828294829\n",
            "Epoch: 82, Loss: 9.98650721157901e-05\n",
            "Epoch: 83, Loss: 9.207188850268722e-05\n",
            "Epoch: 84, Loss: 8.491501648677513e-05\n",
            "Epoch: 85, Loss: 7.830364484107122e-05\n",
            "Epoch: 86, Loss: 7.226834713947028e-05\n",
            "Epoch: 87, Loss: 6.680097430944443e-05\n",
            "Epoch: 88, Loss: 6.132236012490466e-05\n",
            "Epoch: 89, Loss: 5.685190262738615e-05\n",
            "Epoch: 90, Loss: 5.256427175481804e-05\n",
            "Epoch: 91, Loss: 4.803175761480816e-05\n",
            "Epoch: 92, Loss: 4.5108590711606666e-05\n",
            "Epoch: 93, Loss: 4.168065788690001e-05\n",
            "Epoch: 94, Loss: 3.782410567509942e-05\n",
            "Epoch: 95, Loss: 3.497446596156806e-05\n",
            "Epoch: 96, Loss: 3.250782174291089e-05\n",
            "Epoch: 97, Loss: 2.967537693621125e-05\n",
            "Epoch: 98, Loss: 2.74750855169259e-05\n",
            "Epoch: 99, Loss: 2.5446042855037376e-05\n",
            "Epoch: 100, Loss: 2.39058535953518e-05\n",
            "Epoch: 101, Loss: 0.04834786802530289\n",
            "Epoch: 102, Loss: 0.10012562572956085\n",
            "Epoch: 103, Loss: 0.0023163731675595045\n",
            "Epoch: 104, Loss: 0.0006097987643443048\n",
            "Epoch: 105, Loss: 0.00024208052491303533\n",
            "Epoch: 106, Loss: 0.0002125273458659649\n",
            "Epoch: 107, Loss: 0.00019261690613348037\n",
            "Epoch: 108, Loss: 0.00017749315884429961\n",
            "Epoch: 109, Loss: 0.0001653099461691454\n",
            "Epoch: 110, Loss: 0.00015510045341216028\n",
            "Epoch: 111, Loss: 0.0001465077803004533\n",
            "Epoch: 112, Loss: 0.00013904760999139398\n",
            "Epoch: 113, Loss: 0.00013250774645712227\n",
            "Epoch: 114, Loss: 0.0001266812178073451\n",
            "Epoch: 115, Loss: 0.00012150149996159598\n",
            "Epoch: 116, Loss: 0.00011672660184558481\n",
            "Epoch: 117, Loss: 0.00011231183452764526\n",
            "Epoch: 118, Loss: 0.00010820571333169937\n",
            "Epoch: 119, Loss: 0.00010426111839478835\n",
            "Epoch: 120, Loss: 0.00010056584142148495\n",
            "Epoch: 121, Loss: 9.705149568617344e-05\n",
            "Epoch: 122, Loss: 9.366519225295633e-05\n",
            "Epoch: 123, Loss: 9.047067578649148e-05\n",
            "Epoch: 124, Loss: 8.73556055012159e-05\n",
            "Epoch: 125, Loss: 8.435191557509825e-05\n",
            "Epoch: 126, Loss: 8.144322055159137e-05\n",
            "Epoch: 127, Loss: 7.859597099013627e-05\n",
            "Epoch: 128, Loss: 7.5826101237908e-05\n",
            "Epoch: 129, Loss: 7.311211084015667e-05\n",
            "Epoch: 130, Loss: 7.044024823699147e-05\n",
            "Epoch: 131, Loss: 6.779759132768959e-05\n",
            "Epoch: 132, Loss: 6.522545299958438e-05\n",
            "Epoch: 133, Loss: 6.268168363021687e-05\n",
            "Epoch: 134, Loss: 6.015724648023024e-05\n",
            "Epoch: 135, Loss: 5.76822531002108e-05\n",
            "Epoch: 136, Loss: 5.52360579604283e-05\n",
            "Epoch: 137, Loss: 5.286729719955474e-05\n",
            "Epoch: 138, Loss: 5.0559618102852255e-05\n",
            "Epoch: 139, Loss: 4.8242887714877725e-05\n",
            "Epoch: 140, Loss: 4.602596163749695e-05\n",
            "Epoch: 141, Loss: 4.385204738355242e-05\n",
            "Epoch: 142, Loss: 4.178913513896987e-05\n",
            "Epoch: 143, Loss: 3.9700396882835776e-05\n",
            "Epoch: 144, Loss: 3.77484830096364e-05\n",
            "Epoch: 145, Loss: 3.5806031519314274e-05\n",
            "Epoch: 146, Loss: 3.401460708118975e-05\n",
            "Epoch: 147, Loss: 3.225716864108108e-05\n",
            "Epoch: 148, Loss: 3.053027830901556e-05\n",
            "Epoch: 149, Loss: 2.8848553483840078e-05\n",
            "Epoch: 150, Loss: 2.7312284146319143e-05\n",
            "Epoch: 151, Loss: 2.583107925602235e-05\n",
            "Epoch: 152, Loss: 2.4533626856282353e-05\n",
            "Epoch: 153, Loss: 2.2884585632709786e-05\n",
            "Epoch: 154, Loss: 2.1609066607197747e-05\n",
            "Epoch: 155, Loss: 2.035549550782889e-05\n",
            "Epoch: 156, Loss: 1.9304610759718344e-05\n",
            "Epoch: 157, Loss: 1.9693601643666625e-05\n",
            "Epoch: 158, Loss: 1.7210564692504704e-05\n",
            "Epoch: 159, Loss: 1.600518407940399e-05\n",
            "Epoch: 160, Loss: 1.4953851859900169e-05\n",
            "Epoch: 161, Loss: 1.3991173545946367e-05\n",
            "Epoch: 162, Loss: 1.3096055226924364e-05\n",
            "Epoch: 163, Loss: 1.2474204595491756e-05\n",
            "Epoch: 164, Loss: 1.1609206922003068e-05\n",
            "Epoch: 165, Loss: 1.0895259947574232e-05\n",
            "Epoch: 166, Loss: 1.0189918612013571e-05\n",
            "Epoch: 167, Loss: 9.65026265475899e-06\n",
            "Epoch: 168, Loss: 1.702803092484828e-05\n",
            "Epoch: 169, Loss: 0.0207730270922184\n",
            "Epoch: 170, Loss: 0.004832203034311533\n",
            "Epoch: 171, Loss: 0.0003703571856021881\n",
            "Epoch: 172, Loss: 0.0001761686144163832\n",
            "Epoch: 173, Loss: 0.0001448413822799921\n",
            "Epoch: 174, Loss: 0.00012766968575306237\n",
            "Epoch: 175, Loss: 0.00011608004570007324\n",
            "Epoch: 176, Loss: 0.00010739386198110878\n",
            "Epoch: 177, Loss: 0.00010054629092337564\n",
            "Epoch: 178, Loss: 9.489349031355232e-05\n",
            "Epoch: 179, Loss: 9.012099326355383e-05\n",
            "Epoch: 180, Loss: 8.605531184002757e-05\n",
            "Epoch: 181, Loss: 8.228525257436559e-05\n",
            "Epoch: 182, Loss: 7.885009836172685e-05\n",
            "Epoch: 183, Loss: 7.564236148027703e-05\n",
            "Epoch: 184, Loss: 7.266594184329733e-05\n",
            "Epoch: 185, Loss: 6.984728679526597e-05\n",
            "Epoch: 186, Loss: 6.716837378917262e-05\n",
            "Epoch: 187, Loss: 6.459092401200905e-05\n",
            "Epoch: 188, Loss: 6.21656872681342e-05\n",
            "Epoch: 189, Loss: 5.9792932006530464e-05\n",
            "Epoch: 190, Loss: 5.749324554926716e-05\n",
            "Epoch: 191, Loss: 5.526968743652105e-05\n",
            "Epoch: 192, Loss: 5.3057290642755106e-05\n",
            "Epoch: 193, Loss: 5.0750193622661754e-05\n",
            "Epoch: 194, Loss: 4.89563935843762e-05\n",
            "Epoch: 195, Loss: 4.7027875552885234e-05\n",
            "Epoch: 196, Loss: 4.519184585660696e-05\n",
            "Epoch: 197, Loss: 4.340528175816871e-05\n",
            "Epoch: 198, Loss: 4.164537676842883e-05\n",
            "Epoch: 199, Loss: 3.997883322881535e-05\n",
            "Epoch: 200, Loss: 3.8354439311660826e-05\n",
            "Epoch: 201, Loss: 3.679931614897214e-05\n",
            "Epoch: 202, Loss: 3.5272147215437144e-05\n",
            "Epoch: 203, Loss: 3.378672045073472e-05\n",
            "Epoch: 204, Loss: 3.2359359465772286e-05\n",
            "Epoch: 205, Loss: 3.095868669333868e-05\n",
            "Epoch: 206, Loss: 2.960876918223221e-05\n",
            "Epoch: 207, Loss: 2.8317377655184828e-05\n",
            "Epoch: 208, Loss: 2.7051366487285122e-05\n",
            "Epoch: 209, Loss: 2.583827699709218e-05\n",
            "Epoch: 210, Loss: 2.464756107656285e-05\n",
            "Epoch: 211, Loss: 2.3513219275628217e-05\n",
            "Epoch: 212, Loss: 2.2423180780606344e-05\n",
            "Epoch: 213, Loss: 2.1351226678234525e-05\n",
            "Epoch: 214, Loss: 2.030895666393917e-05\n",
            "Epoch: 215, Loss: 1.9315313693368807e-05\n",
            "Epoch: 216, Loss: 1.8236454707221128e-05\n",
            "Epoch: 217, Loss: 1.753156539052725e-05\n",
            "Epoch: 218, Loss: 1.6534899259568192e-05\n",
            "Epoch: 219, Loss: 1.5659159544156864e-05\n",
            "Epoch: 220, Loss: 1.483806499891216e-05\n",
            "Epoch: 221, Loss: 1.4050536265131086e-05\n",
            "Epoch: 222, Loss: 1.3297004443302285e-05\n",
            "Epoch: 223, Loss: 1.2586937373271212e-05\n",
            "Epoch: 224, Loss: 1.187084308185149e-05\n",
            "Epoch: 225, Loss: 1.2081268323527183e-05\n",
            "Epoch: 226, Loss: 1.0661568012437783e-05\n",
            "Epoch: 227, Loss: 9.994530955736991e-06\n",
            "Epoch: 228, Loss: 9.357614544569515e-06\n",
            "Epoch: 229, Loss: 8.849371624819469e-06\n",
            "Epoch: 230, Loss: 8.29250075184973e-06\n",
            "Epoch: 231, Loss: 8.20384593680501e-06\n",
            "Epoch: 232, Loss: 8.685376997163985e-06\n",
            "Epoch: 233, Loss: 7.329804702749243e-06\n",
            "Epoch: 234, Loss: 6.5926119532377925e-06\n",
            "Epoch: 235, Loss: 6.31460579825216e-06\n",
            "Epoch: 236, Loss: 0.04656605422496796\n",
            "Epoch: 237, Loss: 0.0010780015727505088\n",
            "Epoch: 238, Loss: 0.0001909321581479162\n",
            "Epoch: 239, Loss: 0.00014011286839377135\n",
            "Epoch: 240, Loss: 0.00012055174011038616\n",
            "Epoch: 241, Loss: 0.00010851815750356764\n",
            "Epoch: 242, Loss: 9.967297228286043e-05\n",
            "Epoch: 243, Loss: 9.285649139201269e-05\n",
            "Epoch: 244, Loss: 8.713202259968966e-05\n",
            "Epoch: 245, Loss: 8.250881364801899e-05\n",
            "Epoch: 246, Loss: 7.81417838879861e-05\n",
            "Epoch: 247, Loss: 7.437209569616243e-05\n",
            "Epoch: 248, Loss: 7.085867400746793e-05\n",
            "Epoch: 249, Loss: 6.767727609258145e-05\n",
            "Epoch: 250, Loss: 6.462915189331397e-05\n",
            "Epoch: 251, Loss: 6.189076520968229e-05\n",
            "Epoch: 252, Loss: 5.922585478401743e-05\n",
            "Epoch: 253, Loss: 5.673773921444081e-05\n",
            "Epoch: 254, Loss: 5.437736035673879e-05\n",
            "Epoch: 255, Loss: 5.214516568230465e-05\n",
            "Epoch: 256, Loss: 4.9957685405388474e-05\n",
            "Epoch: 257, Loss: 4.7843750508036464e-05\n",
            "Epoch: 258, Loss: 4.579778033075854e-05\n",
            "Epoch: 259, Loss: 4.387785156723112e-05\n",
            "Epoch: 260, Loss: 4.210721090203151e-05\n",
            "Epoch: 261, Loss: 4.029611955047585e-05\n",
            "Epoch: 262, Loss: 3.855943577946164e-05\n",
            "Epoch: 263, Loss: 3.687868229462765e-05\n",
            "Epoch: 264, Loss: 3.523104533087462e-05\n",
            "Epoch: 265, Loss: 3.365655356901698e-05\n",
            "Epoch: 266, Loss: 3.213799209333956e-05\n",
            "Epoch: 267, Loss: 3.066502540605143e-05\n",
            "Epoch: 268, Loss: 2.924369982792996e-05\n",
            "Epoch: 269, Loss: 2.789895370369777e-05\n",
            "Epoch: 270, Loss: 2.658045559655875e-05\n",
            "Epoch: 271, Loss: 2.5318320695078e-05\n",
            "Epoch: 272, Loss: 2.4100934751913883e-05\n",
            "Epoch: 273, Loss: 2.3045340640237555e-05\n",
            "Epoch: 274, Loss: 2.2129597709863447e-05\n",
            "Epoch: 275, Loss: 2.0861427401541732e-05\n",
            "Epoch: 276, Loss: 1.9766666810028255e-05\n",
            "Epoch: 277, Loss: 1.8736454876489006e-05\n",
            "Epoch: 278, Loss: 1.7775942978914827e-05\n",
            "Epoch: 279, Loss: 1.6850292013259605e-05\n",
            "Epoch: 280, Loss: 1.5975412679836154e-05\n",
            "Epoch: 281, Loss: 1.5128936865949072e-05\n",
            "Epoch: 282, Loss: 1.434356818208471e-05\n",
            "Epoch: 283, Loss: 1.3532806406146847e-05\n",
            "Epoch: 284, Loss: 1.28218807731173e-05\n",
            "Epoch: 285, Loss: 1.2132900337746833e-05\n",
            "Epoch: 286, Loss: 1.1496853403514251e-05\n",
            "Epoch: 287, Loss: 1.0812176697072573e-05\n",
            "Epoch: 288, Loss: 1.0309958270227071e-05\n",
            "Epoch: 289, Loss: 1.0505757018108852e-05\n",
            "Epoch: 290, Loss: 9.345553735329304e-06\n",
            "Epoch: 291, Loss: 8.612239980720915e-06\n",
            "Epoch: 292, Loss: 8.055799298745114e-06\n",
            "Epoch: 293, Loss: 7.6017799983674195e-06\n",
            "Epoch: 294, Loss: 7.273423307196936e-06\n",
            "Epoch: 295, Loss: 6.816390850872267e-06\n",
            "Epoch: 296, Loss: 6.399381618393818e-06\n",
            "Epoch: 297, Loss: 6.0180891523486935e-06\n",
            "Epoch: 298, Loss: 5.674239218933508e-06\n",
            "Epoch: 299, Loss: 5.358358976081945e-06\n",
            "Epoch: 300, Loss: 5.0454937081667595e-06\n",
            "Epoch: 301, Loss: 4.741233169625048e-06\n",
            "Epoch: 302, Loss: 5.554154995479621e-06\n",
            "Epoch: 303, Loss: 4.2536421460681595e-06\n",
            "Epoch: 304, Loss: 3.947660843550693e-06\n",
            "Epoch: 305, Loss: 3.6976250612497097e-06\n",
            "Epoch: 306, Loss: 3.5043954085267615e-06\n",
            "Epoch: 307, Loss: 3.293090912848129e-06\n",
            "Epoch: 308, Loss: 3.181628926540725e-06\n",
            "Epoch: 309, Loss: 2.9092138902342413e-06\n",
            "Epoch: 310, Loss: 2.791726728901267e-06\n",
            "Epoch: 311, Loss: 2.800333049890469e-06\n",
            "Epoch: 312, Loss: 2.406558223810862e-06\n",
            "Epoch: 313, Loss: 3.088214953095303e-06\n",
            "Epoch: 314, Loss: 2.195683919126168e-06\n",
            "Epoch: 315, Loss: 2.003314648391097e-06\n",
            "Epoch: 316, Loss: 1.899598373711342e-06\n",
            "Epoch: 317, Loss: 2.006326440096018e-06\n",
            "Epoch: 318, Loss: 1.7111020724769332e-06\n",
            "Epoch: 319, Loss: 1.6052345017669722e-06\n",
            "Epoch: 320, Loss: 1.4744058489668532e-06\n",
            "Epoch: 321, Loss: 1.3681076325156027e-06\n",
            "Epoch: 322, Loss: 1.2742897297357558e-06\n",
            "Epoch: 323, Loss: 1.2075845461367862e-06\n",
            "Epoch: 324, Loss: 1.1296898492219043e-06\n",
            "Epoch: 325, Loss: 1.1090326097473735e-06\n",
            "Epoch: 326, Loss: 1.0948308499791892e-06\n",
            "Epoch: 327, Loss: 1.126677375395957e-06\n",
            "Epoch: 328, Loss: 1.025112851493759e-06\n",
            "Epoch: 329, Loss: 9.248394690075656e-07\n",
            "Epoch: 330, Loss: 8.387681873500696e-07\n",
            "Epoch: 331, Loss: 7.819608640602382e-07\n",
            "Epoch: 332, Loss: 8.258573416242143e-07\n",
            "Epoch: 333, Loss: 0.013863442465662956\n",
            "Epoch: 334, Loss: 0.0010841903276741505\n",
            "Epoch: 335, Loss: 0.00021243083756417036\n",
            "Epoch: 336, Loss: 4.407690357766114e-05\n",
            "Epoch: 337, Loss: 3.862970697809942e-05\n",
            "Epoch: 338, Loss: 3.2618096156511456e-05\n",
            "Epoch: 339, Loss: 2.9935747079434805e-05\n",
            "Epoch: 340, Loss: 2.7961443265667185e-05\n",
            "Epoch: 341, Loss: 2.6351926862844266e-05\n",
            "Epoch: 342, Loss: 2.504139229131397e-05\n",
            "Epoch: 343, Loss: 2.3891749151516706e-05\n",
            "Epoch: 344, Loss: 2.2902140699443407e-05\n",
            "Epoch: 345, Loss: 2.2007607185514644e-05\n",
            "Epoch: 346, Loss: 2.1193949578446336e-05\n",
            "Epoch: 347, Loss: 2.0459023289731704e-05\n",
            "Epoch: 348, Loss: 1.977141982933972e-05\n",
            "Epoch: 349, Loss: 1.914232416311279e-05\n",
            "Epoch: 350, Loss: 1.853388312156312e-05\n",
            "Epoch: 351, Loss: 1.7949958419194445e-05\n",
            "Epoch: 352, Loss: 1.7388403648510575e-05\n",
            "Epoch: 353, Loss: 1.685266943241004e-05\n",
            "Epoch: 354, Loss: 1.6328114725183696e-05\n",
            "Epoch: 355, Loss: 1.583023367857095e-05\n",
            "Epoch: 356, Loss: 1.53323526319582e-05\n",
            "Epoch: 357, Loss: 1.48516801345977e-05\n",
            "Epoch: 358, Loss: 1.4383918824023567e-05\n",
            "Epoch: 359, Loss: 1.3922176549385767e-05\n",
            "Epoch: 360, Loss: 1.3473340914060827e-05\n",
            "Epoch: 361, Loss: 1.303655699302908e-05\n",
            "Epoch: 362, Loss: 1.2600625268532895e-05\n",
            "Epoch: 363, Loss: 1.2173303730378393e-05\n",
            "Epoch: 364, Loss: 1.1749852092179935e-05\n",
            "Epoch: 365, Loss: 1.1331563655403443e-05\n",
            "Epoch: 366, Loss: 1.091628655558452e-05\n",
            "Epoch: 367, Loss: 1.0533713066251948e-05\n",
            "Epoch: 368, Loss: 1.0150280104426201e-05\n",
            "Epoch: 369, Loss: 9.767274605110288e-06\n",
            "Epoch: 370, Loss: 9.40535574045498e-06\n",
            "Epoch: 371, Loss: 9.04085572983604e-06\n",
            "Epoch: 372, Loss: 8.697869816387538e-06\n",
            "Epoch: 373, Loss: 8.357035767403431e-06\n",
            "Epoch: 374, Loss: 8.029540367715526e-06\n",
            "Epoch: 375, Loss: 7.717539119767025e-06\n",
            "Epoch: 376, Loss: 7.416294920403743e-06\n",
            "Epoch: 377, Loss: 7.118923804227961e-06\n",
            "Epoch: 378, Loss: 6.832741746620741e-06\n",
            "Epoch: 379, Loss: 6.564203431480564e-06\n",
            "Epoch: 380, Loss: 6.312878667813493e-06\n",
            "Epoch: 381, Loss: 6.059402039682027e-06\n",
            "Epoch: 382, Loss: 5.79344396101078e-06\n",
            "Epoch: 383, Loss: 5.576977855525911e-06\n",
            "Epoch: 384, Loss: 5.34415676156641e-06\n",
            "Epoch: 385, Loss: 5.124246854393277e-06\n",
            "Epoch: 386, Loss: 4.900463864032645e-06\n",
            "Epoch: 387, Loss: 4.687869022745872e-06\n",
            "Epoch: 388, Loss: 4.480438292375766e-06\n",
            "Epoch: 389, Loss: 4.286779130779905e-06\n",
            "Epoch: 390, Loss: 4.0961331251310185e-06\n",
            "Epoch: 391, Loss: 3.937332166970009e-06\n",
            "Epoch: 392, Loss: 3.7772406358271837e-06\n",
            "Epoch: 393, Loss: 4.087095476279501e-06\n",
            "Epoch: 394, Loss: 3.4648028304218315e-06\n",
            "Epoch: 395, Loss: 3.271142759331269e-06\n",
            "Epoch: 396, Loss: 3.0839385090075666e-06\n",
            "Epoch: 397, Loss: 2.928580215666443e-06\n",
            "Epoch: 398, Loss: 2.77924709735089e-06\n",
            "Epoch: 399, Loss: 2.6471277578821173e-06\n",
            "Epoch: 400, Loss: 2.5223246211680816e-06\n",
            "Epoch: 401, Loss: 2.4009643766476074e-06\n",
            "Epoch: 402, Loss: 2.320056864846265e-06\n",
            "Epoch: 403, Loss: 2.196974719481659e-06\n",
            "Epoch: 404, Loss: 2.109612523781834e-06\n",
            "Epoch: 405, Loss: 1.9607091417128686e-06\n",
            "Epoch: 406, Loss: 1.8561325987320743e-06\n",
            "Epoch: 407, Loss: 1.7644666741034598e-06\n",
            "Epoch: 408, Loss: 1.6650541283524944e-06\n",
            "Epoch: 409, Loss: 1.5755397271277616e-06\n",
            "Epoch: 410, Loss: 1.4911896641933708e-06\n",
            "Epoch: 411, Loss: 1.4064092965782038e-06\n",
            "Epoch: 412, Loss: 1.3263630762594403e-06\n",
            "Epoch: 413, Loss: 1.688722022663569e-06\n",
            "Epoch: 414, Loss: 1.3737022754867212e-06\n",
            "Epoch: 415, Loss: 1.239431071553554e-06\n",
            "Epoch: 416, Loss: 0.010258563794195652\n",
            "Epoch: 417, Loss: 0.0019345278851687908\n",
            "Epoch: 418, Loss: 9.0267181803938e-05\n",
            "Epoch: 419, Loss: 4.783959229825996e-05\n",
            "Epoch: 420, Loss: 3.4646534913918003e-05\n",
            "Epoch: 421, Loss: 3.125837611150928e-05\n",
            "Epoch: 422, Loss: 2.8983115043956786e-05\n",
            "Epoch: 423, Loss: 2.722694625845179e-05\n",
            "Epoch: 424, Loss: 2.5856208594632335e-05\n",
            "Epoch: 425, Loss: 2.4649352781125344e-05\n",
            "Epoch: 426, Loss: 2.3648140995646827e-05\n",
            "Epoch: 427, Loss: 2.27316704695113e-05\n",
            "Epoch: 428, Loss: 2.193351974710822e-05\n",
            "Epoch: 429, Loss: 2.117622716468759e-05\n",
            "Epoch: 430, Loss: 2.049465911113657e-05\n",
            "Epoch: 431, Loss: 1.986514325835742e-05\n",
            "Epoch: 432, Loss: 1.928984056576155e-05\n",
            "Epoch: 433, Loss: 1.872184657258913e-05\n",
            "Epoch: 434, Loss: 1.820462421164848e-05\n",
            "Epoch: 435, Loss: 1.7676638890407048e-05\n",
            "Epoch: 436, Loss: 1.719211104500573e-05\n",
            "Epoch: 437, Loss: 1.6698544641258195e-05\n",
            "Epoch: 438, Loss: 1.622820673219394e-05\n",
            "Epoch: 439, Loss: 1.575012538523879e-05\n",
            "Epoch: 440, Loss: 1.528710163256619e-05\n",
            "Epoch: 441, Loss: 1.4819343050476164e-05\n",
            "Epoch: 442, Loss: 1.4358467524289154e-05\n",
            "Epoch: 443, Loss: 1.38971590786241e-05\n",
            "Epoch: 444, Loss: 1.3457364730129484e-05\n",
            "Epoch: 445, Loss: 1.3005088476347737e-05\n",
            "Epoch: 446, Loss: 1.2563999916892499e-05\n",
            "Epoch: 447, Loss: 1.2137113117205445e-05\n",
            "Epoch: 448, Loss: 1.1707637895597145e-05\n",
            "Epoch: 449, Loss: 1.1288921996310819e-05\n",
            "Epoch: 450, Loss: 1.0861599548661616e-05\n",
            "Epoch: 451, Loss: 1.0459231816639658e-05\n",
            "Epoch: 452, Loss: 1.0069345989904832e-05\n",
            "Epoch: 453, Loss: 9.66913012234727e-06\n",
            "Epoch: 454, Loss: 9.295162271882873e-06\n",
            "Epoch: 455, Loss: 8.928508577810135e-06\n",
            "Epoch: 456, Loss: 8.566593351133633e-06\n",
            "Epoch: 457, Loss: 8.21672165329801e-06\n",
            "Epoch: 458, Loss: 7.88234501669649e-06\n",
            "Epoch: 459, Loss: 7.552700026280945e-06\n",
            "Epoch: 460, Loss: 7.219611688924488e-06\n",
            "Epoch: 461, Loss: 6.908040631969925e-06\n",
            "Epoch: 462, Loss: 6.61282228975324e-06\n",
            "Epoch: 463, Loss: 6.3227657847164664e-06\n",
            "Epoch: 464, Loss: 6.094680884416448e-06\n",
            "Epoch: 465, Loss: 5.8812274801312014e-06\n",
            "Epoch: 466, Loss: 5.580413471761858e-06\n",
            "Epoch: 467, Loss: 5.306281309458427e-06\n",
            "Epoch: 468, Loss: 5.056676855019759e-06\n",
            "Epoch: 469, Loss: 4.810085556528065e-06\n",
            "Epoch: 470, Loss: 4.5759743443340994e-06\n",
            "Epoch: 471, Loss: 4.339710358181037e-06\n",
            "Epoch: 472, Loss: 4.1266853259003256e-06\n",
            "Epoch: 473, Loss: 3.925279088434763e-06\n",
            "Epoch: 474, Loss: 3.7445311136252712e-06\n",
            "Epoch: 475, Loss: 3.5418349852989195e-06\n",
            "Epoch: 476, Loss: 3.387337528693024e-06\n",
            "Epoch: 477, Loss: 3.215625838492997e-06\n",
            "Epoch: 478, Loss: 3.468239128778805e-06\n",
            "Epoch: 479, Loss: 3.055963816223084e-06\n",
            "Epoch: 480, Loss: 2.818838311213767e-06\n",
            "Epoch: 481, Loss: 2.632494670251617e-06\n",
            "Epoch: 482, Loss: 2.4767059585428797e-06\n",
            "Epoch: 483, Loss: 2.344156655453844e-06\n",
            "Epoch: 484, Loss: 2.2180622636369662e-06\n",
            "Epoch: 485, Loss: 2.098853428833536e-06\n",
            "Epoch: 486, Loss: 1.9809358491329476e-06\n",
            "Epoch: 487, Loss: 1.8681824940358638e-06\n",
            "Epoch: 488, Loss: 1.7743647049428546e-06\n",
            "Epoch: 489, Loss: 1.6822681345729507e-06\n",
            "Epoch: 490, Loss: 1.5875897361183888e-06\n",
            "Epoch: 491, Loss: 1.5058220697028446e-06\n",
            "Epoch: 492, Loss: 1.4348132708619232e-06\n",
            "Epoch: 493, Loss: 1.3478809250955237e-06\n",
            "Epoch: 494, Loss: 1.282466655538883e-06\n",
            "Epoch: 495, Loss: 1.2204951644889661e-06\n",
            "Epoch: 496, Loss: 1.1598149285418913e-06\n",
            "Epoch: 497, Loss: 1.3156040949979797e-06\n",
            "Epoch: 498, Loss: 1.131841713686299e-06\n",
            "Epoch: 499, Loss: 1.0294166941093863e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "block_15_emd_128_relu"
      ],
      "metadata": {
        "id": "ofjCvMDl6wt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import re\n",
        "from urllib import request\n",
        "\n",
        "# Load the text data\n",
        "url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "text = response.read().decode('utf-8')\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ')\n",
        "words = text.split()\n",
        "words = [word.strip('.,!?\";:()[]') for word in words if len(word) > 1]  # Remove punctuation and short words\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "stoi = {s: i + 1 for i, s in enumerate(vocab)}  # word to index\n",
        "itos = {i: s for s, i in stoi.items()}  # index to word\n",
        "\n",
        "# Prepare the dataset\n",
        "block_size = 15  # Number of previous words to predict the next one\n",
        "X, Y = [], []\n",
        "\n",
        "for i in range(len(words) - block_size):\n",
        "    X.append([stoi[w] for w in words[i:i + block_size]])\n",
        "    Y.append(stoi[words[i + block_size]])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move data to GPU\n",
        "X = torch.tensor(X).to(device)\n",
        "Y = torch.tensor(Y).to(device)\n",
        "\n",
        "# Define the MLP model\n",
        "class NextWordMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        # Set the activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)  # Shape: (batch_size, block_size, emb_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, block_size * emb_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and train the model\n",
        "emb_dim = 128\n",
        "activation = 'relu'  # Change this to 'tanh' or 'sigmoid' as needed\n",
        "model = NextWordMLP(len(stoi) + 1, emb_dim, 128, activation).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "batch_size = 512\n",
        "for epoch in range(500):  # Reduce epoch for quicker training\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        x = X[i:i + batch_size]\n",
        "        y = Y[i:i + batch_size]\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model and vocabulary\n",
        "torch.save({\n",
        "    'model_state_dict_block_15_emd_128_relu': model.state_dict(),\n",
        "    'stoi': stoi,\n",
        "    'itos': itos,\n",
        "    'block_size': block_size,\n",
        "    'emb_dim': emb_dim,\n",
        "    'activation': activation,\n",
        "}, 'block_15_emd_128_relu.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGxezX5N70IX",
        "outputId": "b2e0a395-54f8-4a6a-9acc-89fef38bf682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 8.31226634979248\n",
            "Epoch: 1, Loss: 7.369995594024658\n",
            "Epoch: 2, Loss: 6.554446697235107\n",
            "Epoch: 3, Loss: 5.63350248336792\n",
            "Epoch: 4, Loss: 4.517008304595947\n",
            "Epoch: 5, Loss: 3.401571035385132\n",
            "Epoch: 6, Loss: 2.632340669631958\n",
            "Epoch: 7, Loss: 2.1618921756744385\n",
            "Epoch: 8, Loss: 1.8149000406265259\n",
            "Epoch: 9, Loss: 1.5309523344039917\n",
            "Epoch: 10, Loss: 1.2940138578414917\n",
            "Epoch: 11, Loss: 1.091055154800415\n",
            "Epoch: 12, Loss: 0.9078086614608765\n",
            "Epoch: 13, Loss: 0.7438458204269409\n",
            "Epoch: 14, Loss: 0.5942032933235168\n",
            "Epoch: 15, Loss: 0.4600483179092407\n",
            "Epoch: 16, Loss: 0.3434283137321472\n",
            "Epoch: 17, Loss: 0.2515428066253662\n",
            "Epoch: 18, Loss: 0.19867022335529327\n",
            "Epoch: 19, Loss: 0.1611953228712082\n",
            "Epoch: 20, Loss: 0.13159997761249542\n",
            "Epoch: 21, Loss: 0.1091817319393158\n",
            "Epoch: 22, Loss: 0.09031365811824799\n",
            "Epoch: 23, Loss: 0.07223691791296005\n",
            "Epoch: 24, Loss: 0.06094672903418541\n",
            "Epoch: 25, Loss: 0.04877366125583649\n",
            "Epoch: 26, Loss: 0.03902611881494522\n",
            "Epoch: 27, Loss: 0.030834970995783806\n",
            "Epoch: 28, Loss: 0.025082331150770187\n",
            "Epoch: 29, Loss: 0.020806370303034782\n",
            "Epoch: 30, Loss: 0.014882034622132778\n",
            "Epoch: 31, Loss: 0.0117809372022748\n",
            "Epoch: 32, Loss: 0.009304639883339405\n",
            "Epoch: 33, Loss: 0.0074194250628352165\n",
            "Epoch: 34, Loss: 0.0063386294059455395\n",
            "Epoch: 35, Loss: 0.005497364327311516\n",
            "Epoch: 36, Loss: 0.004816998261958361\n",
            "Epoch: 37, Loss: 0.004237903282046318\n",
            "Epoch: 38, Loss: 0.003752769436687231\n",
            "Epoch: 39, Loss: 0.00331798754632473\n",
            "Epoch: 40, Loss: 0.0029422850348055363\n",
            "Epoch: 41, Loss: 0.0026210746727883816\n",
            "Epoch: 42, Loss: 0.0023300719913095236\n",
            "Epoch: 43, Loss: 0.00207297271117568\n",
            "Epoch: 44, Loss: 0.0018531301757320762\n",
            "Epoch: 45, Loss: 0.0016508371336385608\n",
            "Epoch: 46, Loss: 0.0014896352076902986\n",
            "Epoch: 47, Loss: 0.001324286451563239\n",
            "Epoch: 48, Loss: 0.0011939750984311104\n",
            "Epoch: 49, Loss: 0.001072424347512424\n",
            "Epoch: 50, Loss: 0.0011697299778461456\n",
            "Epoch: 51, Loss: 0.0014140720013529062\n",
            "Epoch: 52, Loss: 0.0010480602504685521\n",
            "Epoch: 53, Loss: 0.19704322516918182\n",
            "Epoch: 54, Loss: 0.05937635898590088\n",
            "Epoch: 55, Loss: 0.006007887423038483\n",
            "Epoch: 56, Loss: 0.0018557803705334663\n",
            "Epoch: 57, Loss: 0.0012571524130180478\n",
            "Epoch: 58, Loss: 0.0010557528585195541\n",
            "Epoch: 59, Loss: 0.0009366885642521083\n",
            "Epoch: 60, Loss: 0.0008496229420416057\n",
            "Epoch: 61, Loss: 0.0007791878306306899\n",
            "Epoch: 62, Loss: 0.0007216762751340866\n",
            "Epoch: 63, Loss: 0.0006716775824315846\n",
            "Epoch: 64, Loss: 0.0006293069454841316\n",
            "Epoch: 65, Loss: 0.0005906736478209496\n",
            "Epoch: 66, Loss: 0.0005565114552155137\n",
            "Epoch: 67, Loss: 0.0005242933984845877\n",
            "Epoch: 68, Loss: 0.0004955602926202118\n",
            "Epoch: 69, Loss: 0.0004672577779274434\n",
            "Epoch: 70, Loss: 0.00044257115223445\n",
            "Epoch: 71, Loss: 0.0004178440722171217\n",
            "Epoch: 72, Loss: 0.0003950150276068598\n",
            "Epoch: 73, Loss: 0.00037304681609384716\n",
            "Epoch: 74, Loss: 0.0003522549814078957\n",
            "Epoch: 75, Loss: 0.00033178634475916624\n",
            "Epoch: 76, Loss: 0.00031291780760511756\n",
            "Epoch: 77, Loss: 0.00029416396864689887\n",
            "Epoch: 78, Loss: 0.00027637844323180616\n",
            "Epoch: 79, Loss: 0.00025918797473423183\n",
            "Epoch: 80, Loss: 0.0002425543061690405\n",
            "Epoch: 81, Loss: 0.0002265884686494246\n",
            "Epoch: 82, Loss: 0.00021151331020519137\n",
            "Epoch: 83, Loss: 0.0001972304016817361\n",
            "Epoch: 84, Loss: 0.00018317322246730328\n",
            "Epoch: 85, Loss: 0.00017033217591233552\n",
            "Epoch: 86, Loss: 0.0001581262331455946\n",
            "Epoch: 87, Loss: 0.00014662707690149546\n",
            "Epoch: 88, Loss: 0.00013588389265350997\n",
            "Epoch: 89, Loss: 0.00012553097622003406\n",
            "Epoch: 90, Loss: 0.00011594243551371619\n",
            "Epoch: 91, Loss: 0.00010713114170357585\n",
            "Epoch: 92, Loss: 9.836817480390891e-05\n",
            "Epoch: 93, Loss: 9.058736031875014e-05\n",
            "Epoch: 94, Loss: 8.31824290798977e-05\n",
            "Epoch: 95, Loss: 7.627812738064677e-05\n",
            "Epoch: 96, Loss: 6.98949079378508e-05\n",
            "Epoch: 97, Loss: 6.41918377368711e-05\n",
            "Epoch: 98, Loss: 5.8533449191600084e-05\n",
            "Epoch: 99, Loss: 5.357947520678863e-05\n",
            "Epoch: 100, Loss: 4.88730838696938e-05\n",
            "Epoch: 101, Loss: 4.4645457819569856e-05\n",
            "Epoch: 102, Loss: 4.0605111280456185e-05\n",
            "Epoch: 103, Loss: 3.728165029315278e-05\n",
            "Epoch: 104, Loss: 3.3809828892117366e-05\n",
            "Epoch: 105, Loss: 3.2776086300145835e-05\n",
            "Epoch: 106, Loss: 0.11211858689785004\n",
            "Epoch: 107, Loss: 0.026850266382098198\n",
            "Epoch: 108, Loss: 0.0018973101396113634\n",
            "Epoch: 109, Loss: 0.0006863093585707247\n",
            "Epoch: 110, Loss: 0.0005104703013785183\n",
            "Epoch: 111, Loss: 0.00044682889711111784\n",
            "Epoch: 112, Loss: 0.0004037337494082749\n",
            "Epoch: 113, Loss: 0.00037096274900250137\n",
            "Epoch: 114, Loss: 0.0003438052081037313\n",
            "Epoch: 115, Loss: 0.0003210050635971129\n",
            "Epoch: 116, Loss: 0.00030088797211647034\n",
            "Epoch: 117, Loss: 0.00028335582464933395\n",
            "Epoch: 118, Loss: 0.0002670918474905193\n",
            "Epoch: 119, Loss: 0.0002525407762732357\n",
            "Epoch: 120, Loss: 0.00023886778217274696\n",
            "Epoch: 121, Loss: 0.00022633759363088757\n",
            "Epoch: 122, Loss: 0.00021439911506604403\n",
            "Epoch: 123, Loss: 0.0002031964686466381\n",
            "Epoch: 124, Loss: 0.00019242143025621772\n",
            "Epoch: 125, Loss: 0.00018233370792586356\n",
            "Epoch: 126, Loss: 0.00017248187214136124\n",
            "Epoch: 127, Loss: 0.00016314037202391773\n",
            "Epoch: 128, Loss: 0.00015416547830682248\n",
            "Epoch: 129, Loss: 0.00014555729285348207\n",
            "Epoch: 130, Loss: 0.00013733287050854415\n",
            "Epoch: 131, Loss: 0.00012950175732839853\n",
            "Epoch: 132, Loss: 0.00012191286077722907\n",
            "Epoch: 133, Loss: 0.00011488930613268167\n",
            "Epoch: 134, Loss: 0.00010811894753715023\n",
            "Epoch: 135, Loss: 0.00010174031194765121\n",
            "Epoch: 136, Loss: 9.556005534250289e-05\n",
            "Epoch: 137, Loss: 8.972975047072396e-05\n",
            "Epoch: 138, Loss: 8.422974497079849e-05\n",
            "Epoch: 139, Loss: 7.901277422206476e-05\n",
            "Epoch: 140, Loss: 7.401705079246312e-05\n",
            "Epoch: 141, Loss: 6.93199472152628e-05\n",
            "Epoch: 142, Loss: 6.48282584734261e-05\n",
            "Epoch: 143, Loss: 6.057691280147992e-05\n",
            "Epoch: 144, Loss: 5.6576602219138294e-05\n",
            "Epoch: 145, Loss: 5.281896665110253e-05\n",
            "Epoch: 146, Loss: 4.920617357129231e-05\n",
            "Epoch: 147, Loss: 4.5776323531754315e-05\n",
            "Epoch: 148, Loss: 4.26111146225594e-05\n",
            "Epoch: 149, Loss: 3.961760376114398e-05\n",
            "Epoch: 150, Loss: 3.679059591377154e-05\n",
            "Epoch: 151, Loss: 3.4411212254781276e-05\n",
            "Epoch: 152, Loss: 3.252680107834749e-05\n",
            "Epoch: 153, Loss: 2.9686198104172945e-05\n",
            "Epoch: 154, Loss: 2.7233762011746876e-05\n",
            "Epoch: 155, Loss: 2.5180193915730342e-05\n",
            "Epoch: 156, Loss: 2.3162112483987585e-05\n",
            "Epoch: 157, Loss: 2.1385601939982735e-05\n",
            "Epoch: 158, Loss: 1.9687022358994e-05\n",
            "Epoch: 159, Loss: 1.8167671441915445e-05\n",
            "Epoch: 160, Loss: 1.6698246326996014e-05\n",
            "Epoch: 161, Loss: 1.5373136193375103e-05\n",
            "Epoch: 162, Loss: 1.557628820592072e-05\n",
            "Epoch: 163, Loss: 0.05750088021159172\n",
            "Epoch: 164, Loss: 0.018992166966199875\n",
            "Epoch: 165, Loss: 0.0008747471729293466\n",
            "Epoch: 166, Loss: 0.00041533552575856447\n",
            "Epoch: 167, Loss: 0.0003468490904197097\n",
            "Epoch: 168, Loss: 0.0003109994577243924\n",
            "Epoch: 169, Loss: 0.0002842259418684989\n",
            "Epoch: 170, Loss: 0.0002624766493681818\n",
            "Epoch: 171, Loss: 0.0002444835554342717\n",
            "Epoch: 172, Loss: 0.00022858873126097023\n",
            "Epoch: 173, Loss: 0.00021399842808023095\n",
            "Epoch: 174, Loss: 0.00020120711997151375\n",
            "Epoch: 175, Loss: 0.0001889675622805953\n",
            "Epoch: 176, Loss: 0.00017782006761990488\n",
            "Epoch: 177, Loss: 0.0001686879259068519\n",
            "Epoch: 178, Loss: 0.0001597335358383134\n",
            "Epoch: 179, Loss: 0.00015027125482447445\n",
            "Epoch: 180, Loss: 0.00014227154315449297\n",
            "Epoch: 181, Loss: 0.00013443805801216513\n",
            "Epoch: 182, Loss: 0.00012728087313007563\n",
            "Epoch: 183, Loss: 0.00012025071191601455\n",
            "Epoch: 184, Loss: 0.00011387662380002439\n",
            "Epoch: 185, Loss: 0.00010735459363786504\n",
            "Epoch: 186, Loss: 0.00010108898277394474\n",
            "Epoch: 187, Loss: 9.613397560315207e-05\n",
            "Epoch: 188, Loss: 9.097775910049677e-05\n",
            "Epoch: 189, Loss: 8.579419227316976e-05\n",
            "Epoch: 190, Loss: 8.102880383376032e-05\n",
            "Epoch: 191, Loss: 7.64399446779862e-05\n",
            "Epoch: 192, Loss: 7.210727198980749e-05\n",
            "Epoch: 193, Loss: 6.814621156081557e-05\n",
            "Epoch: 194, Loss: 6.521072646137327e-05\n",
            "Epoch: 195, Loss: 6.0953178035560995e-05\n",
            "Epoch: 196, Loss: 5.712874190066941e-05\n",
            "Epoch: 197, Loss: 5.3629781177733094e-05\n",
            "Epoch: 198, Loss: 5.029934618505649e-05\n",
            "Epoch: 199, Loss: 4.71524108434096e-05\n",
            "Epoch: 200, Loss: 4.414166323840618e-05\n",
            "Epoch: 201, Loss: 4.134042319492437e-05\n",
            "Epoch: 202, Loss: 3.8686343032168224e-05\n",
            "Epoch: 203, Loss: 3.6171943065710366e-05\n",
            "Epoch: 204, Loss: 3.378508336027153e-05\n",
            "Epoch: 205, Loss: 3.1536455935565755e-05\n",
            "Epoch: 206, Loss: 2.940730337286368e-05\n",
            "Epoch: 207, Loss: 2.7426491215010174e-05\n",
            "Epoch: 208, Loss: 2.5812218154896982e-05\n",
            "Epoch: 209, Loss: 2.3988684915821068e-05\n",
            "Epoch: 210, Loss: 2.2204394554137252e-05\n",
            "Epoch: 211, Loss: 2.053959724435117e-05\n",
            "Epoch: 212, Loss: 1.905056160467211e-05\n",
            "Epoch: 213, Loss: 1.7711321561364457e-05\n",
            "Epoch: 214, Loss: 1.6464724467368796e-05\n",
            "Epoch: 215, Loss: 1.521004469395848e-05\n",
            "Epoch: 216, Loss: 1.4081489098316524e-05\n",
            "Epoch: 217, Loss: 1.3007483175897505e-05\n",
            "Epoch: 218, Loss: 1.2007943041680846e-05\n",
            "Epoch: 219, Loss: 1.1105958947155159e-05\n",
            "Epoch: 220, Loss: 1.0234858564217575e-05\n",
            "Epoch: 221, Loss: 1.0002210728998762e-05\n",
            "Epoch: 222, Loss: 0.0818614810705185\n",
            "Epoch: 223, Loss: 0.004733085632324219\n",
            "Epoch: 224, Loss: 0.0005635066190734506\n",
            "Epoch: 225, Loss: 0.0003522450278978795\n",
            "Epoch: 226, Loss: 0.0003027858620043844\n",
            "Epoch: 227, Loss: 0.00027021620189771056\n",
            "Epoch: 228, Loss: 0.0002453541674185544\n",
            "Epoch: 229, Loss: 0.00022589821310248226\n",
            "Epoch: 230, Loss: 0.0002093700459226966\n",
            "Epoch: 231, Loss: 0.00019550806609913707\n",
            "Epoch: 232, Loss: 0.00018307106802240014\n",
            "Epoch: 233, Loss: 0.0001720534055493772\n",
            "Epoch: 234, Loss: 0.0001618587994016707\n",
            "Epoch: 235, Loss: 0.00015260980580933392\n",
            "Epoch: 236, Loss: 0.00014393679157365113\n",
            "Epoch: 237, Loss: 0.0001359845045953989\n",
            "Epoch: 238, Loss: 0.00012846279423683882\n",
            "Epoch: 239, Loss: 0.00012153360148658976\n",
            "Epoch: 240, Loss: 0.00011494744830997661\n",
            "Epoch: 241, Loss: 0.00010875659063458443\n",
            "Epoch: 242, Loss: 0.00010284197196597233\n",
            "Epoch: 243, Loss: 9.723070252221078e-05\n",
            "Epoch: 244, Loss: 9.188820695271716e-05\n",
            "Epoch: 245, Loss: 8.68540519149974e-05\n",
            "Epoch: 246, Loss: 8.205119956983253e-05\n",
            "Epoch: 247, Loss: 7.744881440885365e-05\n",
            "Epoch: 248, Loss: 7.30922183720395e-05\n",
            "Epoch: 249, Loss: 6.894561374792829e-05\n",
            "Epoch: 250, Loss: 6.497903086710721e-05\n",
            "Epoch: 251, Loss: 6.11742798355408e-05\n",
            "Epoch: 252, Loss: 5.756485916208476e-05\n",
            "Epoch: 253, Loss: 5.410746962297708e-05\n",
            "Epoch: 254, Loss: 5.082321149529889e-05\n",
            "Epoch: 255, Loss: 4.777961294166744e-05\n",
            "Epoch: 256, Loss: 4.489990169531666e-05\n",
            "Epoch: 257, Loss: 4.213704232824966e-05\n",
            "Epoch: 258, Loss: 3.95427159674e-05\n",
            "Epoch: 259, Loss: 3.706294592120685e-05\n",
            "Epoch: 260, Loss: 3.4720822441158816e-05\n",
            "Epoch: 261, Loss: 3.249702785979025e-05\n",
            "Epoch: 262, Loss: 3.0398756280192174e-05\n",
            "Epoch: 263, Loss: 2.8439879315556027e-05\n",
            "Epoch: 264, Loss: 2.659413257788401e-05\n",
            "Epoch: 265, Loss: 2.4886052415240556e-05\n",
            "Epoch: 266, Loss: 2.344435233680997e-05\n",
            "Epoch: 267, Loss: 2.1798319721710868e-05\n",
            "Epoch: 268, Loss: 2.023107481363695e-05\n",
            "Epoch: 269, Loss: 1.8826602172339335e-05\n",
            "Epoch: 270, Loss: 1.746426278259605e-05\n",
            "Epoch: 271, Loss: 1.6238736861851066e-05\n",
            "Epoch: 272, Loss: 1.5111050743144006e-05\n",
            "Epoch: 273, Loss: 1.4030981219548266e-05\n",
            "Epoch: 274, Loss: 1.3046455933363177e-05\n",
            "Epoch: 275, Loss: 1.209684614877915e-05\n",
            "Epoch: 276, Loss: 1.1130498023703694e-05\n",
            "Epoch: 277, Loss: 1.0283062692906242e-05\n",
            "Epoch: 278, Loss: 9.516734280623496e-06\n",
            "Epoch: 279, Loss: 8.772626642894465e-06\n",
            "Epoch: 280, Loss: 8.27212988951942e-06\n",
            "Epoch: 281, Loss: 7.657333298993763e-06\n",
            "Epoch: 282, Loss: 8.373996934096795e-06\n",
            "Epoch: 283, Loss: 9.811063137021847e-06\n",
            "Epoch: 284, Loss: 0.05547971650958061\n",
            "Epoch: 285, Loss: 0.007897879928350449\n",
            "Epoch: 286, Loss: 0.0005732393474318087\n",
            "Epoch: 287, Loss: 0.0002487464516889304\n",
            "Epoch: 288, Loss: 0.0002050654002232477\n",
            "Epoch: 289, Loss: 0.00018448733317200094\n",
            "Epoch: 290, Loss: 0.0001702593290247023\n",
            "Epoch: 291, Loss: 0.00015867034380789846\n",
            "Epoch: 292, Loss: 0.00014877562352921814\n",
            "Epoch: 293, Loss: 0.0001401800400344655\n",
            "Epoch: 294, Loss: 0.00013252614007797092\n",
            "Epoch: 295, Loss: 0.00012558417802210897\n",
            "Epoch: 296, Loss: 0.00011923284182557836\n",
            "Epoch: 297, Loss: 0.00011332538997521624\n",
            "Epoch: 298, Loss: 0.00010779529839055613\n",
            "Epoch: 299, Loss: 0.00010255115921609104\n",
            "Epoch: 300, Loss: 9.757890074979514e-05\n",
            "Epoch: 301, Loss: 9.276683704229072e-05\n",
            "Epoch: 302, Loss: 8.822092058835551e-05\n",
            "Epoch: 303, Loss: 8.387427806155756e-05\n",
            "Epoch: 304, Loss: 7.968160207383335e-05\n",
            "Epoch: 305, Loss: 7.566512795165181e-05\n",
            "Epoch: 306, Loss: 7.183901470853016e-05\n",
            "Epoch: 307, Loss: 6.812450737925246e-05\n",
            "Epoch: 308, Loss: 6.455393304349855e-05\n",
            "Epoch: 309, Loss: 6.114981806604192e-05\n",
            "Epoch: 310, Loss: 5.788878843304701e-05\n",
            "Epoch: 311, Loss: 5.4774314776295796e-05\n",
            "Epoch: 312, Loss: 5.181189044378698e-05\n",
            "Epoch: 313, Loss: 4.894380981568247e-05\n",
            "Epoch: 314, Loss: 4.6222306991694495e-05\n",
            "Epoch: 315, Loss: 4.361274841357954e-05\n",
            "Epoch: 316, Loss: 4.111544694751501e-05\n",
            "Epoch: 317, Loss: 3.868737258017063e-05\n",
            "Epoch: 318, Loss: 3.641600414994173e-05\n",
            "Epoch: 319, Loss: 3.4253713238285854e-05\n",
            "Epoch: 320, Loss: 3.2196167012443766e-05\n",
            "Epoch: 321, Loss: 3.0268776754382998e-05\n",
            "Epoch: 322, Loss: 2.8476448278524913e-05\n",
            "Epoch: 323, Loss: 2.666362161107827e-05\n",
            "Epoch: 324, Loss: 2.495266562618781e-05\n",
            "Epoch: 325, Loss: 2.338341437280178e-05\n",
            "Epoch: 326, Loss: 2.1863514120923355e-05\n",
            "Epoch: 327, Loss: 2.0434525140444748e-05\n",
            "Epoch: 328, Loss: 1.9071332644671202e-05\n",
            "Epoch: 329, Loss: 1.7801366993808188e-05\n",
            "Epoch: 330, Loss: 1.6693311408744194e-05\n",
            "Epoch: 331, Loss: 1.5921219528536312e-05\n",
            "Epoch: 332, Loss: 1.462700038246112e-05\n",
            "Epoch: 333, Loss: 1.3541452972276602e-05\n",
            "Epoch: 334, Loss: 1.2540182979137171e-05\n",
            "Epoch: 335, Loss: 1.164917011919897e-05\n",
            "Epoch: 336, Loss: 1.0794521585921757e-05\n",
            "Epoch: 337, Loss: 1.004176101560006e-05\n",
            "Epoch: 338, Loss: 9.295637028117198e-06\n",
            "Epoch: 339, Loss: 8.662656000524294e-06\n",
            "Epoch: 340, Loss: 8.091154086287133e-06\n",
            "Epoch: 341, Loss: 7.85446900408715e-06\n",
            "Epoch: 342, Loss: 6.8439494498306885e-06\n",
            "Epoch: 343, Loss: 6.537416993523948e-06\n",
            "Epoch: 344, Loss: 0.037156302481889725\n",
            "Epoch: 345, Loss: 0.0021508154459297657\n",
            "Epoch: 346, Loss: 0.000286294671241194\n",
            "Epoch: 347, Loss: 0.00020970696641597897\n",
            "Epoch: 348, Loss: 0.00018412075587548316\n",
            "Epoch: 349, Loss: 0.00016673438949510455\n",
            "Epoch: 350, Loss: 0.00015352352056652308\n",
            "Epoch: 351, Loss: 0.00014286207442637533\n",
            "Epoch: 352, Loss: 0.00013377271534409374\n",
            "Epoch: 353, Loss: 0.00012584449723362923\n",
            "Epoch: 354, Loss: 0.00011864028783747926\n",
            "Epoch: 355, Loss: 0.00011207265924895182\n",
            "Epoch: 356, Loss: 0.00010602150723570958\n",
            "Epoch: 357, Loss: 0.00010040475171990693\n",
            "Epoch: 358, Loss: 9.503064211457968e-05\n",
            "Epoch: 359, Loss: 9.006046457216144e-05\n",
            "Epoch: 360, Loss: 8.522943971911445e-05\n",
            "Epoch: 361, Loss: 8.077736129052937e-05\n",
            "Epoch: 362, Loss: 7.642095442861319e-05\n",
            "Epoch: 363, Loss: 7.231429481180385e-05\n",
            "Epoch: 364, Loss: 6.845164898550138e-05\n",
            "Epoch: 365, Loss: 6.469045183621347e-05\n",
            "Epoch: 366, Loss: 6.151241541374475e-05\n",
            "Epoch: 367, Loss: 5.7937806559493765e-05\n",
            "Epoch: 368, Loss: 5.465226786327548e-05\n",
            "Epoch: 369, Loss: 5.146944022271782e-05\n",
            "Epoch: 370, Loss: 4.850730692851357e-05\n",
            "Epoch: 371, Loss: 4.5657689042855054e-05\n",
            "Epoch: 372, Loss: 4.295523467590101e-05\n",
            "Epoch: 373, Loss: 4.0404844185104594e-05\n",
            "Epoch: 374, Loss: 3.7975929444655776e-05\n",
            "Epoch: 375, Loss: 3.567310704966076e-05\n",
            "Epoch: 376, Loss: 3.3553234970895573e-05\n",
            "Epoch: 377, Loss: 3.1516192393610254e-05\n",
            "Epoch: 378, Loss: 2.9533679480664432e-05\n",
            "Epoch: 379, Loss: 2.7719415811588988e-05\n",
            "Epoch: 380, Loss: 2.6025491024483927e-05\n",
            "Epoch: 381, Loss: 2.444008714519441e-05\n",
            "Epoch: 382, Loss: 2.2899117539054714e-05\n",
            "Epoch: 383, Loss: 2.1460025891428813e-05\n",
            "Epoch: 384, Loss: 2.013580706261564e-05\n",
            "Epoch: 385, Loss: 1.8913462554337457e-05\n",
            "Epoch: 386, Loss: 1.7679571101325564e-05\n",
            "Epoch: 387, Loss: 1.6569498257013038e-05\n",
            "Epoch: 388, Loss: 1.5471543520106934e-05\n",
            "Epoch: 389, Loss: 1.4412838936550543e-05\n",
            "Epoch: 390, Loss: 1.341446113656275e-05\n",
            "Epoch: 391, Loss: 1.2632549442059826e-05\n",
            "Epoch: 392, Loss: 1.1736338819900993e-05\n",
            "Epoch: 393, Loss: 1.0915459824900609e-05\n",
            "Epoch: 394, Loss: 1.0147979082830716e-05\n",
            "Epoch: 395, Loss: 9.429276360606309e-06\n",
            "Epoch: 396, Loss: 8.765988241066225e-06\n",
            "Epoch: 397, Loss: 8.14282066130545e-06\n",
            "Epoch: 398, Loss: 7.556598120572744e-06\n",
            "Epoch: 399, Loss: 7.033298061287496e-06\n",
            "Epoch: 400, Loss: 6.53106735626352e-06\n",
            "Epoch: 401, Loss: 6.0507722992042545e-06\n",
            "Epoch: 402, Loss: 5.6082890296238475e-06\n",
            "Epoch: 403, Loss: 5.191494892642368e-06\n",
            "Epoch: 404, Loss: 4.799810540134786e-06\n",
            "Epoch: 405, Loss: 4.455753241927596e-06\n",
            "Epoch: 406, Loss: 4.149216692894697e-06\n",
            "Epoch: 407, Loss: 3.978918812208576e-06\n",
            "Epoch: 408, Loss: 0.031739242374897\n",
            "Epoch: 409, Loss: 0.003587695537135005\n",
            "Epoch: 410, Loss: 0.0003127193485852331\n",
            "Epoch: 411, Loss: 0.0002092284121317789\n",
            "Epoch: 412, Loss: 0.000175394510733895\n",
            "Epoch: 413, Loss: 0.0001540157536510378\n",
            "Epoch: 414, Loss: 0.0001384382339892909\n",
            "Epoch: 415, Loss: 0.0001262490259250626\n",
            "Epoch: 416, Loss: 0.00011623465979937464\n",
            "Epoch: 417, Loss: 0.0001077809210983105\n",
            "Epoch: 418, Loss: 0.00010073384328279644\n",
            "Epoch: 419, Loss: 9.421940194442868e-05\n",
            "Epoch: 420, Loss: 8.841336966725066e-05\n",
            "Epoch: 421, Loss: 8.330863056471571e-05\n",
            "Epoch: 422, Loss: 7.859435572754592e-05\n",
            "Epoch: 423, Loss: 7.431840640492737e-05\n",
            "Epoch: 424, Loss: 7.031497079879045e-05\n",
            "Epoch: 425, Loss: 6.659705832134932e-05\n",
            "Epoch: 426, Loss: 6.310553726507351e-05\n",
            "Epoch: 427, Loss: 5.984591189189814e-05\n",
            "Epoch: 428, Loss: 5.67082897759974e-05\n",
            "Epoch: 429, Loss: 5.3729301725979894e-05\n",
            "Epoch: 430, Loss: 5.096785389469005e-05\n",
            "Epoch: 431, Loss: 4.827879092772491e-05\n",
            "Epoch: 432, Loss: 4.577525396598503e-05\n",
            "Epoch: 433, Loss: 4.33567984146066e-05\n",
            "Epoch: 434, Loss: 4.105665720999241e-05\n",
            "Epoch: 435, Loss: 3.8826907257316634e-05\n",
            "Epoch: 436, Loss: 3.672959792311303e-05\n",
            "Epoch: 437, Loss: 3.472291064099409e-05\n",
            "Epoch: 438, Loss: 3.271271998528391e-05\n",
            "Epoch: 439, Loss: 3.0813633202342317e-05\n",
            "Epoch: 440, Loss: 2.903055792558007e-05\n",
            "Epoch: 441, Loss: 2.7338097424944863e-05\n",
            "Epoch: 442, Loss: 2.5681123588583432e-05\n",
            "Epoch: 443, Loss: 2.4112170649459586e-05\n",
            "Epoch: 444, Loss: 2.262575799250044e-05\n",
            "Epoch: 445, Loss: 2.1306159396772273e-05\n",
            "Epoch: 446, Loss: 1.98869856831152e-05\n",
            "Epoch: 447, Loss: 1.863492070697248e-05\n",
            "Epoch: 448, Loss: 1.7453270629630424e-05\n",
            "Epoch: 449, Loss: 1.6332804079866037e-05\n",
            "Epoch: 450, Loss: 1.5257656741596293e-05\n",
            "Epoch: 451, Loss: 1.4248600564314984e-05\n",
            "Epoch: 452, Loss: 1.3288608897710219e-05\n",
            "Epoch: 453, Loss: 1.24062562463223e-05\n",
            "Epoch: 454, Loss: 1.1566905413928907e-05\n",
            "Epoch: 455, Loss: 1.0781530363601632e-05\n",
            "Epoch: 456, Loss: 1.0034254046331625e-05\n",
            "Epoch: 457, Loss: 9.346145816380158e-06\n",
            "Epoch: 458, Loss: 8.688342859386466e-06\n",
            "Epoch: 459, Loss: 8.070082913036458e-06\n",
            "Epoch: 460, Loss: 7.474912308680359e-06\n",
            "Epoch: 461, Loss: 6.997217496973462e-06\n",
            "Epoch: 462, Loss: 6.454288723034551e-06\n",
            "Epoch: 463, Loss: 5.971108294033911e-06\n",
            "Epoch: 464, Loss: 5.511019026016584e-06\n",
            "Epoch: 465, Loss: 5.100861471873941e-06\n",
            "Epoch: 466, Loss: 4.772678494191496e-06\n",
            "Epoch: 467, Loss: 4.6393261072807945e-06\n",
            "Epoch: 468, Loss: 4.288629952498013e-06\n",
            "Epoch: 469, Loss: 3.974012543039862e-06\n",
            "Epoch: 470, Loss: 3.93735308534815e-06\n",
            "Epoch: 471, Loss: 3.4305028293601936e-06\n",
            "Epoch: 472, Loss: 3.1155959732132033e-06\n",
            "Epoch: 473, Loss: 2.8373467557685217e-06\n",
            "Epoch: 474, Loss: 2.5764147721929476e-06\n",
            "Epoch: 475, Loss: 2.3743668862152845e-06\n",
            "Epoch: 476, Loss: 2.184729510190664e-06\n",
            "Epoch: 477, Loss: 2.0170286916254554e-06\n",
            "Epoch: 478, Loss: 1.863760417109006e-06\n",
            "Epoch: 479, Loss: 1.745994723023614e-06\n",
            "Epoch: 480, Loss: 1.605426518835884e-06\n",
            "Epoch: 481, Loss: 1.4876608247504919e-06\n",
            "Epoch: 482, Loss: 1.402800307914731e-06\n",
            "Epoch: 483, Loss: 2.589680889286683e-06\n",
            "Epoch: 484, Loss: 1.8473043610356399e-06\n",
            "Epoch: 485, Loss: 1.9018511920876335e-06\n",
            "Epoch: 486, Loss: 1.2053689033564297e-06\n",
            "Epoch: 487, Loss: 1.056430050994095e-06\n",
            "Epoch: 488, Loss: 9.519418995296292e-07\n",
            "Epoch: 489, Loss: 8.745857940084534e-07\n",
            "Epoch: 490, Loss: 8.237848305725493e-07\n",
            "Epoch: 491, Loss: 1.99201258510584e-06\n",
            "Epoch: 492, Loss: 0.017461327835917473\n",
            "Epoch: 493, Loss: 0.00583782559260726\n",
            "Epoch: 494, Loss: 0.00026112821069546044\n",
            "Epoch: 495, Loss: 7.207867747638375e-05\n",
            "Epoch: 496, Loss: 6.41992301098071e-05\n",
            "Epoch: 497, Loss: 5.9156958741368726e-05\n",
            "Epoch: 498, Loss: 5.537525794352405e-05\n",
            "Epoch: 499, Loss: 5.2312992920633405e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define the MLP model class\n",
        "class NextWordMLP(nn.Module):\n",
        "    def _init_(self, vocab_size, emb_dim, hidden_size, activation='relu', block_size=15):\n",
        "        super()._init_()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc1 = nn.Linear(emb_dim * block_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'tanh', or 'sigmoid'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Map model configurations to their corresponding filenames and state_dict names\n",
        "model_files = {\n",
        "    (5, 64, 'tanh'): ('block_5_emd_64_tanh.pth', 'model_state_dict_block_5_emd_64_tanh'),\n",
        "    (5, 128, 'relu'): ('block_5_emd_128_relu.pth', 'model_state_dict_block_5_emd_64_relu'),\n",
        "    (10, 64, 'tanh'): ('block_10_emd_64_tanh.pth', 'model_state_dict_block_10_emd_64_tanh'),\n",
        "    (10, 128, 'relu'): ('block_10_emd_128_relu.pth', 'model_state_dict_block_10_emd_128_relu'),\n",
        "    (15, 64, 'tanh'): ('block_15_emd_64_tanh.pth', 'model_state_dict_block_15_emd_64_tanh'),\n",
        "    (15, 128, 'relu'): ('block_15_emd_128_relu.pth', 'model_state_dict_block_15_emd_128_relu'),\n",
        "\n",
        "    # Additional files from the latest image\n",
        "    (5, 64, 'relu'): ('trained_model_1.pth', 'model_state_dict_1'),\n",
        "    (10, 64, 'relu'): ('trained_model_2.pth', 'model_state_dict_2'),\n",
        "    (15, 64, 'relu'): ('trained_model_3.pth', 'model_state_dict_3'),\n",
        "    (5, 128, 'tanh'): ('trained_model_7.pth', 'model_state_dict_4'),\n",
        "    (10, 128, 'tanh'): ('trained_model_8.pth', 'model_state_dict_8'),\n",
        "    (15, 128, 'tanh'): ('trained_model_9.pth', 'model_state_dict_6')\n",
        "}\n",
        "\n",
        "# Function to load the model based on user input\n",
        "def load_model(context_length, emb_dim, activation):\n",
        "    model_file, state_dict_key = model_files[(context_length, emb_dim, activation)]\n",
        "    checkpoint = torch.load(model_file, map_location=torch.device('cpu'))\n",
        "\n",
        "    model = NextWordMLP(\n",
        "        vocab_size=len(checkpoint['stoi']) + 1,\n",
        "        emb_dim=checkpoint['emb_dim'],\n",
        "        hidden_size=128,\n",
        "        activation=checkpoint['activation'],\n",
        "        block_size=checkpoint['block_size']\n",
        "    )\n",
        "    model.load_state_dict(checkpoint[state_dict_key])\n",
        "    model.eval()\n",
        "\n",
        "    return model, checkpoint['stoi'], checkpoint['itos']\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Next Word Prediction App\")\n",
        "\n",
        "# Dropdowns for selecting model parameters\n",
        "context_length = st.selectbox(\"Context Length\", [5, 10, 15])\n",
        "embedding_size = st.selectbox(\"Embedding Dimension\", [64, 128])\n",
        "activation_function = st.selectbox(\"Activation Function\", [\"relu\", \"tanh\"])\n",
        "\n",
        "# Load the model based on selections\n",
        "if st.button(\"Load Model\"):\n",
        "    try:\n",
        "        model, stoi, itos = load_model(context_length, embedding_size, activation_function)\n",
        "        st.success(\"Model loaded successfully!\")\n",
        "    except KeyError:\n",
        "        st.error(\"The selected model configuration is unavailable. Please try a different combination.\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading model: {e}\")\n",
        "\n",
        "# Prediction function\n",
        "def predict_next_word(model, input_text, stoi, itos):\n",
        "    words = input_text.lower().split()\n",
        "    if len(words) < context_length:\n",
        "        st.warning(f\"Please enter at least {context_length} words for context.\")\n",
        "        return None\n",
        "\n",
        "    # Convert words to indices\n",
        "    words = words[-context_length:]  # Get the last 'context_length' words\n",
        "    x = torch.tensor([[stoi.get(w, 0) for w in words]], dtype=torch.long)  # Using 0 for unknown words\n",
        "\n",
        "    # Check if x is empty or contains only zeros\n",
        "    if x.sum().item() == 0:\n",
        "        st.warning(\"Input words are not in vocabulary. Please check your input.\")\n",
        "        return None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs = torch.softmax(logits, dim=-1).squeeze()\n",
        "\n",
        "    # Debugging: print logits and probabilities\n",
        "    st.write(\"Logits:\", logits.numpy())\n",
        "    st.write(\"Probabilities:\", probs.numpy())\n",
        "\n",
        "    # Select the index of the maximum probability\n",
        "    next_word_index = torch.argmax(probs).item()\n",
        "\n",
        "    # Check if the index is valid\n",
        "    if next_word_index < len(itos):\n",
        "        return itos[next_word_index]\n",
        "    else:\n",
        "        st.warning(\"Predicted index is out of bounds.\")\n",
        "        return None\n",
        "\n",
        "# Text input for user to test prediction\n",
        "input_text = st.text_input(\"Enter a sentence to predict the next word:\")\n",
        "if st.button(\"Predict\"):\n",
        "    if 'model' in locals():\n",
        "        next_word = predict_next_word(model, input_text, stoi, itos)\n",
        "        if next_word is not None:\n",
        "            st.write(f\"Predicted next word: {next_word}\")\n",
        "    else:\n",
        "        st.warning(\"Please load a model first.\")"
      ],
      "metadata": {
        "id": "RsTjc1gsqc08"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}